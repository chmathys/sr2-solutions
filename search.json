[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"project companion Richard McElreath’s Statistical Rethinking (2nd Edition) (McElreath, 2020), 2022 version accompanying course. 10 weeks, course (materials provided ), work exercises chapter covered week assigned homework problems. Like Solomon Kurz, take {tidyverse} (Wickham et al., 2019; Wickham, 2021) {brms} (Bürkner, 2017, 2018, 2021) approach solving problems. translation actual book text {tidyverse} {brms} style code, please check project, Statistical rethinking brms, ggplot2, tidyverse: Second edition.can purchase Statistical Rethinking: Bayesian Course R Stan (McElreath, 2020) CRC Press.","code":""},{"path":"index.html","id":"disclaimer","chapter":"Welcome","heading":"Disclaimer","text":"project work progress. ’d like follow along, can find GitHub repository . solutions checked anybody, undoubtedly errors. find , please contribute let know!several ways contribute. simple edits suggestions, can use “Edit page” link sidebar right screen. can also create fork repository submit pull request open issue.Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"project licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"index.html","id":"colophon","chapter":"Welcome","heading":"Colophon","text":"project built :","code":"\ndesc <- desc::description$new()\ndeps <- desc$get_deps() %>% \n  filter(package != \"R\") %>% \n  pull(package)\n\nsessioninfo::session_info(deps)\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.1.2 (2021-11-01)\n#>  os       Ubuntu 20.04.3 LTS\n#>  system   x86_64, linux-gnu\n#>  ui       X11\n#>  language (EN)\n#>  collate  C.UTF-8\n#>  ctype    C.UTF-8\n#>  tz       UTC\n#>  date     2022-01-21\n#>  pandoc   2.14.2 @ /usr/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package              * version    date (UTC) lib source\n#>  abind                  1.4-5      2016-07-21 [1] CRAN (R 4.1.2)\n#>  arrayhelpers           1.1-0      2020-02-04 [1] CRAN (R 4.1.2)\n#>  askpass                1.1        2019-01-13 [1] CRAN (R 4.1.2)\n#>  assertthat             0.2.1      2019-03-21 [1] CRAN (R 4.1.2)\n#>  backports              1.4.1      2021-12-13 [1] CRAN (R 4.1.2)\n#>  base64enc              0.1-3      2015-07-28 [1] CRAN (R 4.1.2)\n#>  bayesplot              1.8.1      2021-06-14 [1] CRAN (R 4.1.2)\n#>  BH                     1.78.0-0   2021-12-15 [1] CRAN (R 4.1.2)\n#>  bit                    4.0.4      2020-08-04 [1] CRAN (R 4.1.2)\n#>  bit64                  4.0.5      2020-08-30 [1] CRAN (R 4.1.2)\n#>  blob                   1.2.2      2021-07-23 [1] CRAN (R 4.1.2)\n#>  bookdown               0.24       2021-09-02 [1] CRAN (R 4.1.2)\n#>  boot                   1.3-28     2021-05-03 [2] CRAN (R 4.1.2)\n#>  bridgesampling         1.1-2      2021-04-16 [1] CRAN (R 4.1.2)\n#>  brio                   1.1.3      2021-11-30 [1] CRAN (R 4.1.2)\n#>  brms                 * 2.16.3     2021-11-22 [1] CRAN (R 4.1.2)\n#>  Brobdingnag            1.2-6      2018-08-13 [1] CRAN (R 4.1.2)\n#>  broom                  0.7.11     2022-01-03 [1] CRAN (R 4.1.2)\n#>  bslib                  0.3.1      2021-10-06 [1] CRAN (R 4.1.2)\n#>  cachem                 1.0.6      2021-08-19 [1] CRAN (R 4.1.2)\n#>  callr                  3.7.0      2021-04-20 [1] CRAN (R 4.1.2)\n#>  cellranger             1.1.0      2016-07-27 [1] CRAN (R 4.1.2)\n#>  checkmate              2.0.0      2020-02-06 [1] CRAN (R 4.1.2)\n#>  class                  7.3-19     2021-05-03 [2] CRAN (R 4.1.2)\n#>  classInt               0.4-3      2020-04-07 [1] CRAN (R 4.1.2)\n#>  cli                    3.1.1      2022-01-20 [1] CRAN (R 4.1.2)\n#>  clipr                  0.7.1      2020-10-08 [1] CRAN (R 4.1.2)\n#>  cmdstanr             * 0.4.0.9001 2022-01-21 [1] Github (stan-dev/cmdstanr@a2a97d9)\n#>  coda                   0.19-4     2020-09-30 [1] CRAN (R 4.1.2)\n#>  codetools              0.2-18     2020-11-04 [2] CRAN (R 4.1.2)\n#>  colorblindr            0.1.0      2022-01-06 [1] Github (clauswilke/colorblindr@e6730be)\n#>  colorspace             2.0-2      2021-06-24 [1] CRAN (R 4.1.2)\n#>  colourpicker           1.1.1      2021-10-04 [1] CRAN (R 4.1.2)\n#>  commonmark             1.7        2018-12-01 [1] CRAN (R 4.1.2)\n#>  cowplot                1.1.1      2020-12-30 [1] CRAN (R 4.1.2)\n#>  cpp11                  0.4.2      2021-11-30 [1] CRAN (R 4.1.2)\n#>  crayon                 1.4.2      2021-10-29 [1] CRAN (R 4.1.2)\n#>  crosstalk              1.2.0      2021-11-04 [1] CRAN (R 4.1.2)\n#>  curl                   4.3.2      2021-06-23 [1] CRAN (R 4.1.2)\n#>  dagitty                0.3-1      2021-01-21 [1] CRAN (R 4.1.2)\n#>  data.table             1.14.2     2021-09-27 [1] CRAN (R 4.1.2)\n#>  DBI                    1.1.2      2021-12-20 [1] CRAN (R 4.1.2)\n#>  dbplyr                 2.1.1      2021-04-06 [1] CRAN (R 4.1.2)\n#>  desc                   1.4.0      2021-09-28 [1] CRAN (R 4.1.2)\n#>  digest                 0.6.29     2021-12-01 [1] CRAN (R 4.1.2)\n#>  distributional         0.3.0      2022-01-05 [1] CRAN (R 4.1.2)\n#>  downlit                0.4.0      2021-10-29 [1] CRAN (R 4.1.2)\n#>  dplyr                * 1.0.7      2021-06-18 [1] CRAN (R 4.1.2)\n#>  DT                     0.20       2021-11-15 [1] CRAN (R 4.1.2)\n#>  dtplyr                 1.2.1      2022-01-19 [1] CRAN (R 4.1.2)\n#>  dygraphs               1.1.1.6    2018-07-11 [1] CRAN (R 4.1.2)\n#>  e1071                  1.7-9      2021-09-16 [1] CRAN (R 4.1.2)\n#>  ellipsis               0.3.2      2021-04-29 [1] CRAN (R 4.1.2)\n#>  emo                    0.0.0.9000 2022-01-06 [1] Github (hadley/emo@3f03b11)\n#>  evaluate               0.14       2019-05-28 [1] CRAN (R 4.1.2)\n#>  extrafont              0.17       2014-12-08 [1] CRAN (R 4.1.2)\n#>  extrafontdb            1.0        2012-06-11 [1] CRAN (R 4.1.2)\n#>  fansi                  1.0.2      2022-01-14 [1] CRAN (R 4.1.2)\n#>  farver                 2.1.0      2021-02-28 [1] CRAN (R 4.1.2)\n#>  fastmap                1.1.0      2021-01-25 [1] CRAN (R 4.1.2)\n#>  fontawesome            0.2.2      2021-07-02 [1] CRAN (R 4.1.2)\n#>  forcats              * 0.5.1      2021-01-27 [1] CRAN (R 4.1.2)\n#>  fs                     1.5.2      2021-12-08 [1] CRAN (R 4.1.2)\n#>  future                 1.23.0     2021-10-31 [1] CRAN (R 4.1.2)\n#>  gargle                 1.2.0      2021-07-02 [1] CRAN (R 4.1.2)\n#>  gdtools                0.2.3      2021-01-06 [1] CRAN (R 4.1.2)\n#>  generics               0.1.1      2021-10-25 [1] CRAN (R 4.1.2)\n#>  geomtextpath         * 0.1.0      2022-01-21 [1] Github (AllanCameron/geomtextpath@efc9b55)\n#>  gganimate              1.0.7      2020-10-15 [1] CRAN (R 4.1.2)\n#>  ggdag                  0.2.4      2021-10-10 [1] CRAN (R 4.1.2)\n#>  ggdist               * 3.0.1      2021-11-30 [1] CRAN (R 4.1.2)\n#>  ggforce                0.3.3      2021-03-05 [1] CRAN (R 4.1.2)\n#>  gghighlight            0.3.2      2021-06-05 [1] CRAN (R 4.1.2)\n#>  ggplot2              * 3.3.5      2021-06-25 [1] CRAN (R 4.1.2)\n#>  ggraph                 2.0.5      2021-02-23 [1] CRAN (R 4.1.2)\n#>  ggrepel                0.9.1      2021-01-15 [1] CRAN (R 4.1.2)\n#>  ggridges               0.5.3      2021-01-08 [1] CRAN (R 4.1.2)\n#>  gifski                 1.4.3-1    2021-05-02 [1] CRAN (R 4.1.2)\n#>  globals                0.14.0     2020-11-22 [1] CRAN (R 4.1.2)\n#>  glue                 * 1.6.0      2021-12-17 [1] CRAN (R 4.1.2)\n#>  googledrive            2.0.0      2021-07-08 [1] CRAN (R 4.1.2)\n#>  googlesheets4          1.0.0      2021-07-21 [1] CRAN (R 4.1.2)\n#>  graphlayouts           0.8.0      2022-01-03 [1] CRAN (R 4.1.2)\n#>  gridExtra              2.3        2017-09-09 [1] CRAN (R 4.1.2)\n#>  gtable                 0.3.0      2019-03-25 [1] CRAN (R 4.1.2)\n#>  gtools                 3.9.2      2021-06-06 [1] CRAN (R 4.1.2)\n#>  haven                  2.4.3      2021-08-04 [1] CRAN (R 4.1.2)\n#>  HDInterval             0.2.2      2020-05-23 [1] CRAN (R 4.1.2)\n#>  here                 * 1.0.1      2020-12-13 [1] CRAN (R 4.1.2)\n#>  highr                  0.9        2021-04-16 [1] CRAN (R 4.1.2)\n#>  hms                    1.1.1      2021-09-26 [1] CRAN (R 4.1.2)\n#>  hrbrthemes           * 0.8.0      2020-03-06 [1] CRAN (R 4.1.2)\n#>  htmltools              0.5.2      2021-08-25 [1] CRAN (R 4.1.2)\n#>  htmlwidgets            1.5.4      2021-09-08 [1] CRAN (R 4.1.2)\n#>  httpuv                 1.6.5      2022-01-05 [1] CRAN (R 4.1.2)\n#>  httr                   1.4.2      2020-07-20 [1] CRAN (R 4.1.2)\n#>  ids                    1.0.1      2017-05-31 [1] CRAN (R 4.1.2)\n#>  igraph                 1.2.11     2022-01-04 [1] CRAN (R 4.1.2)\n#>  inline                 0.3.19     2021-05-31 [1] CRAN (R 4.1.2)\n#>  isoband                0.2.5      2021-07-13 [1] CRAN (R 4.1.2)\n#>  jquerylib              0.1.4      2021-04-26 [1] CRAN (R 4.1.2)\n#>  jsonlite               1.7.3      2022-01-17 [1] CRAN (R 4.1.2)\n#>  kableExtra             1.3.4      2021-02-20 [1] CRAN (R 4.1.2)\n#>  KernSmooth             2.23-20    2021-05-03 [2] CRAN (R 4.1.2)\n#>  knitr                  1.37       2021-12-16 [1] CRAN (R 4.1.2)\n#>  labeling               0.4.2      2020-10-20 [1] CRAN (R 4.1.2)\n#>  later                  1.3.0      2021-08-18 [1] CRAN (R 4.1.2)\n#>  lattice                0.20-45    2021-09-22 [2] CRAN (R 4.1.2)\n#>  lazyeval               0.2.2      2019-03-15 [1] CRAN (R 4.1.2)\n#>  lifecycle              1.0.1      2021-09-24 [1] CRAN (R 4.1.2)\n#>  listenv                0.8.0      2019-12-05 [1] CRAN (R 4.1.2)\n#>  loo                  * 2.4.1      2020-12-09 [1] CRAN (R 4.1.2)\n#>  lpSolve                5.6.15     2020-01-24 [1] CRAN (R 4.1.2)\n#>  lubridate              1.8.0      2021-10-07 [1] CRAN (R 4.1.2)\n#>  magrittr               2.0.1      2020-11-17 [1] CRAN (R 4.1.2)\n#>  markdown               1.1        2019-08-07 [1] CRAN (R 4.1.2)\n#>  MASS                   7.3-54     2021-05-03 [2] CRAN (R 4.1.2)\n#>  Matrix                 1.3-4      2021-06-01 [2] CRAN (R 4.1.2)\n#>  matrixStats            0.61.0     2021-09-17 [1] CRAN (R 4.1.2)\n#>  memoise                2.0.1      2021-11-26 [1] CRAN (R 4.1.2)\n#>  mgcv                   1.8-38     2021-10-06 [2] CRAN (R 4.1.2)\n#>  mime                   0.12       2021-09-28 [1] CRAN (R 4.1.2)\n#>  miniUI                 0.1.1.1    2018-05-18 [1] CRAN (R 4.1.2)\n#>  modelr                 0.1.8      2020-05-19 [1] CRAN (R 4.1.2)\n#>  munsell                0.5.0      2018-06-12 [1] CRAN (R 4.1.2)\n#>  mvtnorm                1.1-3      2021-10-08 [1] CRAN (R 4.1.2)\n#>  nleqslv                3.3.2      2018-05-17 [1] CRAN (R 4.1.2)\n#>  nlme                   3.1-153    2021-09-07 [2] CRAN (R 4.1.2)\n#>  numDeriv               2016.8-1.1 2019-06-06 [1] CRAN (R 4.1.2)\n#>  officedown             0.2.3      2021-11-16 [1] CRAN (R 4.1.2)\n#>  officer                0.4.1      2021-11-14 [1] CRAN (R 4.1.2)\n#>  openssl                1.4.6      2021-12-19 [1] CRAN (R 4.1.2)\n#>  packrat                0.7.0      2021-08-20 [1] CRAN (R 4.1.2)\n#>  parallelly             1.30.0     2021-12-17 [1] CRAN (R 4.1.2)\n#>  patchwork              1.1.1      2020-12-17 [1] CRAN (R 4.1.2)\n#>  pillar                 1.6.4      2021-10-18 [1] CRAN (R 4.1.2)\n#>  pkgbuild               1.3.1      2021-12-20 [1] CRAN (R 4.1.2)\n#>  pkgconfig              2.0.3      2019-09-22 [1] CRAN (R 4.1.2)\n#>  plyr                   1.8.6      2020-03-03 [1] CRAN (R 4.1.2)\n#>  polyclip               1.10-0     2019-03-14 [1] CRAN (R 4.1.2)\n#>  posterior              1.2.0      2022-01-05 [1] CRAN (R 4.1.2)\n#>  prettyunits            1.1.1      2020-01-24 [1] CRAN (R 4.1.2)\n#>  processx               3.5.2      2021-04-30 [1] CRAN (R 4.1.2)\n#>  progress               1.2.2      2019-05-16 [1] CRAN (R 4.1.2)\n#>  promises               1.2.0.1    2021-02-11 [1] CRAN (R 4.1.2)\n#>  proxy                  0.4-26     2021-06-07 [1] CRAN (R 4.1.2)\n#>  ps                     1.6.0      2021-02-28 [1] CRAN (R 4.1.2)\n#>  purrr                * 0.3.4      2020-04-17 [1] CRAN (R 4.1.2)\n#>  R6                     2.5.1      2021-08-19 [1] CRAN (R 4.1.2)\n#>  ragg                   1.2.1      2021-12-06 [1] CRAN (R 4.1.2)\n#>  rappdirs               0.3.3      2021-01-31 [1] CRAN (R 4.1.2)\n#>  ratlas               * 0.0.0.9000 2022-01-06 [1] Github (atlas-aai/ratlas@267dd5c)\n#>  RColorBrewer           1.1-2      2014-12-07 [1] CRAN (R 4.1.2)\n#>  Rcpp                 * 1.0.8      2022-01-13 [1] CRAN (R 4.1.2)\n#>  RcppArmadillo          0.10.7.5.0 2021-12-17 [1] CRAN (R 4.1.2)\n#>  RcppEigen              0.3.3.9.1  2020-12-17 [1] CRAN (R 4.1.2)\n#>  RcppParallel           5.1.5      2022-01-05 [1] CRAN (R 4.1.2)\n#>  readr                * 2.1.1      2021-11-30 [1] CRAN (R 4.1.2)\n#>  readxl                 1.3.1      2019-03-13 [1] CRAN (R 4.1.2)\n#>  rematch                1.0.1      2016-04-21 [1] CRAN (R 4.1.2)\n#>  rematch2               2.1.2      2020-05-01 [1] CRAN (R 4.1.2)\n#>  reprex                 2.0.1      2021-08-05 [1] CRAN (R 4.1.2)\n#>  reshape2               1.4.4      2020-04-09 [1] CRAN (R 4.1.2)\n#>  rethinking           * 2.21       2022-01-06 [1] Github (rmcelreath/rethinking@783d111)\n#>  rlang                  0.4.12     2021-10-18 [1] CRAN (R 4.1.2)\n#>  rmarkdown              2.11       2021-09-14 [1] CRAN (R 4.1.2)\n#>  rprojroot              2.0.2      2020-11-15 [1] CRAN (R 4.1.2)\n#>  rsconnect              0.8.25     2021-11-19 [1] CRAN (R 4.1.2)\n#>  rstan                * 2.21.3     2021-12-19 [1] CRAN (R 4.1.2)\n#>  rstantools             2.1.1      2020-07-06 [1] CRAN (R 4.1.2)\n#>  rstudioapi             0.13       2020-11-12 [1] CRAN (R 4.1.2)\n#>  Rttf2pt1               1.3.9      2021-07-22 [1] CRAN (R 4.1.2)\n#>  rvest                  1.0.2      2021-10-16 [1] CRAN (R 4.1.2)\n#>  rvg                    0.2.5      2020-06-30 [1] CRAN (R 4.1.2)\n#>  s2                     1.0.7      2021-09-28 [1] CRAN (R 4.1.2)\n#>  sass                   0.4.0      2021-05-12 [1] CRAN (R 4.1.2)\n#>  scales                 1.1.1      2020-05-11 [1] CRAN (R 4.1.2)\n#>  selectr                0.4-2      2019-11-20 [1] CRAN (R 4.1.2)\n#>  servr                  0.24       2021-11-16 [1] CRAN (R 4.1.2)\n#>  sessioninfo            1.2.2      2021-12-06 [1] any (@1.2.2)\n#>  sf                     1.0-5      2021-12-17 [1] CRAN (R 4.1.2)\n#>  shape                  1.4.6      2021-05-19 [1] CRAN (R 4.1.2)\n#>  shiny                  1.7.1      2021-10-02 [1] CRAN (R 4.1.2)\n#>  shinyjs                2.1.0      2021-12-23 [1] CRAN (R 4.1.2)\n#>  shinystan              2.5.0      2018-05-01 [1] CRAN (R 4.1.2)\n#>  shinythemes            1.2.0      2021-01-25 [1] CRAN (R 4.1.2)\n#>  sourcetools            0.1.7      2018-04-25 [1] CRAN (R 4.1.2)\n#>  StanHeaders          * 2.21.0-7   2020-12-17 [1] CRAN (R 4.1.2)\n#>  stringi                1.7.6      2021-11-29 [1] CRAN (R 4.1.2)\n#>  stringr              * 1.4.0      2019-02-10 [1] CRAN (R 4.1.2)\n#>  svglite                2.0.0      2021-02-20 [1] CRAN (R 4.1.2)\n#>  svUnit                 1.0.6      2021-04-19 [1] CRAN (R 4.1.2)\n#>  sys                    3.4        2020-07-23 [1] CRAN (R 4.1.2)\n#>  systemfonts            1.0.3      2021-10-13 [1] CRAN (R 4.1.2)\n#>  tensorA                0.36.2     2020-11-19 [1] CRAN (R 4.1.2)\n#>  textshaping            0.3.6      2021-10-13 [1] CRAN (R 4.1.2)\n#>  threejs                0.3.3      2020-01-21 [1] CRAN (R 4.1.2)\n#>  tibble               * 3.1.6      2021-11-07 [1] CRAN (R 4.1.2)\n#>  tidybayes            * 3.0.2      2022-01-05 [1] CRAN (R 4.1.2)\n#>  tidybayes.rethinking * 3.0.0      2022-01-06 [1] Github (mjskay/tidybayes.rethinking@7da9946)\n#>  tidygraph              1.2.0      2020-05-12 [1] CRAN (R 4.1.2)\n#>  tidyr                * 1.1.4      2021-09-27 [1] CRAN (R 4.1.2)\n#>  tidyselect             1.1.1      2021-04-30 [1] CRAN (R 4.1.2)\n#>  tidyverse            * 1.3.1      2021-04-15 [1] CRAN (R 4.1.2)\n#>  tinytex                0.36       2021-12-19 [1] CRAN (R 4.1.2)\n#>  transformr             0.1.3      2020-07-05 [1] CRAN (R 4.1.2)\n#>  tweenr                 1.0.2      2021-03-23 [1] CRAN (R 4.1.2)\n#>  tzdb                   0.2.0      2021-10-27 [1] CRAN (R 4.1.2)\n#>  units                  0.7-2      2021-06-08 [1] CRAN (R 4.1.2)\n#>  utf8                   1.2.2      2021-07-24 [1] CRAN (R 4.1.2)\n#>  uuid                   1.0-3      2021-11-01 [1] CRAN (R 4.1.2)\n#>  V8                     4.0.0      2021-12-23 [1] CRAN (R 4.1.2)\n#>  vctrs                  0.3.8      2021-04-29 [1] CRAN (R 4.1.2)\n#>  viridis                0.6.2      2021-10-13 [1] CRAN (R 4.1.2)\n#>  viridisLite            0.4.0      2021-04-13 [1] CRAN (R 4.1.2)\n#>  vroom                  1.5.7      2021-11-30 [1] CRAN (R 4.1.2)\n#>  webshot                0.5.2      2019-11-22 [1] CRAN (R 4.1.2)\n#>  withr                  2.4.3      2021-11-30 [1] CRAN (R 4.1.2)\n#>  wk                     0.6.0      2022-01-03 [1] CRAN (R 4.1.2)\n#>  xaringan               0.22       2021-06-23 [1] CRAN (R 4.1.2)\n#>  xfun                   0.29       2021-12-14 [1] CRAN (R 4.1.2)\n#>  xml2                   1.3.3      2021-11-30 [1] CRAN (R 4.1.2)\n#>  xtable                 1.8-4      2019-04-21 [1] CRAN (R 4.1.2)\n#>  xts                    0.12.1     2020-09-09 [1] CRAN (R 4.1.2)\n#>  yaml                   2.2.1      2020-02-01 [1] CRAN (R 4.1.2)\n#>  zip                    2.2.0      2021-05-31 [1] CRAN (R 4.1.2)\n#>  zoo                    1.8-9      2021-03-09 [1] CRAN (R 4.1.2)\n#> \n#>  [1] /home/runner/work/_temp/Library\n#>  [2] /opt/R/4.1.2/lib/R/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────"},{"path":"bayesian-inference.html","id":"bayesian-inference","chapter":"Week 1: Bayesian Inference","heading":"Week 1: Bayesian Inference","text":"first week covers Chapter 1 (Golem Prague), Chapter 2 (Small Worlds Large Worlds), Chapter 3 (Sampling Imaginary).","code":""},{"path":"bayesian-inference.html","id":"lectures","chapter":"Week 1: Bayesian Inference","heading":"1.1 Lectures","text":"Lecture 1:Lecture 2:","code":""},{"path":"bayesian-inference.html","id":"exercises","chapter":"Week 1: Bayesian Inference","heading":"1.2 Exercises","text":"","code":""},{"path":"bayesian-inference.html","id":"chapter-1","chapter":"Week 1: Bayesian Inference","heading":"1.2.1 Chapter 1","text":"exercises Chapter 1.","code":""},{"path":"bayesian-inference.html","id":"chapter-2","chapter":"Week 1: Bayesian Inference","heading":"1.2.2 Chapter 2","text":"2E1. expressions correspond statement: probability rain Monday?\n(1) Pr(rain)\n(2) Pr(rain|Monday)\n(3) Pr(Monday|rain)\n(4) Pr(rain, Monday) / Pr(Monday)take question mean probability rain given Monday. means (2) (4) correct.2E2. following statements corresponds expression: Pr(Monday|rain)?\n(1) probability rain Monday.\n(2) probability rain, given Monday.\n(3) probability Monday, given raining.\n(4) probability Monday raining.answer (3) corresponds expression Pr(Monday|rain).2E3. following expressions correspond statement: probability Monday, given raining?\n(1) Pr(Monday|rain)\n(2) Pr(rain|Monday)\n(3) Pr(rain|Monday) Pr(Monday)\n(4) Pr(rain|Monday) Pr(Monday) / Pr(rain)\n(5) Pr(Monday|rain) Pr(rain) / Pr(Monday)two correct answers. Answer option (1) standard notation conditional probability. Answer option (4) equivalent, Bayes’ Theorem.2E4. Bayesian statistician Bruno de Finetti (1906–1985) began 1973 book probability theory dedication: “PROBABILITY EXIST.” capitals appeared original, imagine de Finetti wanted us shout statement. meant probability device describing uncertainty perspective observer limited knowledge; objective reality. Discuss globe tossing example chapter, light statement. mean say “probability water 0.7”?idea probability subjective perception likelihood something happen. globe tossing example, result always either “land” “water” (.e., 0 1). toss globe, don’t know result , know always “land” “water.” express uncertainty outcome, use probability. know water likely land, may say probability “water” 0.7; however, ’ll never actually observe result 0.7 waters, observe probability. ever observe two results “land” “water.”2M1. Recall globe tossing model chapter. Compute plot grid approximate posterior distribution following sets observations. case, assume uniform prior p.\n(1) W, W, W\n(2) W, W, W, L\n(3) L, W, W, L, W, W, W2M2. Now assume prior p equal zero p < 0.5 positive constant p ≥ 0.5. compute plot grid approximate posterior distribution sets observations problem just .problem can use code , just altering prior defined.2M3. Suppose two globes, one Earth one Mars. Earth globe 70% covered water. Mars globe 100% land. suppose one globes—don’t know —tossed air produced “land” observatiion. Assume globe equally likely tossed. Show posterior probability globe Earth, conditional seeing “land” (Pr(Earth|land)), 0.23.2M4. Suppose deck three cards. card two sides, side either black white. One card two black sides. second card one black one white side. third card two white sides. Now suppose three cards placed bag shuffled. Someone reaches bag pulls card places flat table. black side shown facing , don’t know color side facing . Show probability side also black 2/3. Use counting method (Section 2 chapter) approach problem. means counting ways card produce observed data (black side faceing table).2M5. Now suppose four cards: B/B, B/W, W/W, another B/B. suppose card drawn bag black side appears face . calculate probability side black.2M6. Imagine black ink heavy, cards black sides heavier cards white sides. result, ’s less likely card black sides pulled bag. assume three cards: B/B, B/W, W/W. experimenting number times, conclude every way pull B/B card bag, 2 ways pull B/W card 3 ways pull W/W card. suppose card pulled black side appears face . Show probability side black now 0.5. Use counting method, .2M7. Assume original card problem, single card showing black side face . looking side, draw another card bag lay face table. face shown new card white. Show probability first card, one showing black side, black side now 0.75. Use counting method, can. Hint: Treat like sequence globe tosses, countng ways see observation, possiible first card.2H1. Suppose two species panda bear. equally common wild live sample places. look exactly alike eat food, yet genetic assay capable telling apart. differ however family sizes. Species gives birth twins 10% time, otherwise birthing single infant. Species births twins 20% time, ottherwise birthing singleton infants. Assume numbers known certainty, many years field research.\nNow suppose managing captive panda breeding program. newe female panda unknown species, just given birth twins. probability next birth also twins?2H2. Recall facts problem . Now compute probability panda species , asssuming observed first birth twins.2H3. Continuing previous problem, suppose panda mother second birth twins, singleton infant. Compute posterior probability panda species .2H4. common boast Bayesian statisticians Bayesian inference makes easy use data, even data different types.\nsuppose now veterinarian comes along new genetic test claims can identify species mother panda. test, like tests, imperfect. information test:probability correctly identifies species panda 0.8.probability correctly identifies species B panda 0.65.vet administers test panda tells test positive species . First ignore previous information births compute posterior probability panda species . redo calculation, now using birth data well.","code":"\nlibrary(tidyverse)\n\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),\n               prior = rep(1, times = 20)) %>%\n  mutate(likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20)) %>%\n  mutate(prior = case_when(p_grid < 0.5 ~ 0L,\n                           TRUE ~ 1L),\n         likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n# probability of land, given Earth\np_le <- 0.3\n\n# probability of land, given Mars\np_lm <- 1.0\n\n# probability of Earth\np_e <- 0.5\n\n# probability of land\np_l <- (p_e * p_le) + ((1 - p_e) * p_lm)\n\n# probability of Earth, given land (using Bayes' Theorem)\np_el <- (p_le * p_e) / p_l\np_el\n#> [1] 0.231\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood)\nprior <- c(1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.667\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood,\n                card_bb_likelihood)\nprior <- c(1, 1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1] + posterior[4]\n#> [1] 0.8\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood)\nprior <- c(1, 2, 3)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.5\n# 2 choices for first card, with 3 options for second card: 2 W/W + 1 W/B\ncard_bb_likelihood <- 2 * 3 \ncard_wb_likelihood <- 1 * 2 \ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_wb_likelihood, card_ww_likelihood)\nprior <- c(1,1,1)\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.75\n# After first birth, likelihood of species A and B is equal to the rate the\n# species give birth to twins\na_likelihood <- 0.1\nb_likelihood <- 0.2\n\n# Next calculate the posterior probability that the panda belongs to each\n# species, assume species are equally likely\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(1, 1) \nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\nposterior\n#> [1] 0.333 0.667\n\n# The probability the next birth is twins is the probability the panda belongs\n# to each species times the likelihood each species gives birth to twins\n(posterior[1] * a_likelihood) + (posterior[2] * b_likelihood)\n#> [1] 0.167\n# probability of species A\np_a <- 0.5\n\n# probability of twins, given species A\np_ta <- 0.1\n\n# probability of twins, given species B\np_tb <- 0.2\n\n# probability of twins\np_t <- (p_a * p_ta) + ((1 - p_a) * p_tb)\n\n# probability of species A, given twins (using Bayes' Theorem)\n# (note this is equivalent to `posterior[1]` above)\np_at <- (p_ta * p_a) / p_t\np_at\n#> [1] 0.333\n# likelihood for each species is Pr(twins) * Pr(singleton)\na_likelihood <- 0.1 * (1 - 0.1)\nb_likelihood <- 0.2 * (1 - 0.2)\n\n# compute posterior probabilities\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.36\n# use Bayes' Theorem to determine the probability of species A, given a positive\n# test\np_ap <- (0.8 * 0.5) / ((0.5 * 0.8) + (0.5 * 0.35))\np_ap\n#> [1] 0.696\n\n\n# Now include test data with observed births\n# likelihood for each species is Pr(twins) * Pr(singleton)\na_likelihood <- 0.1 * (1 - 0.1)\nb_likelihood <- 0.2 * (1 - 0.2)\n\n# compute posterior probabilities, using test result as prior\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(p_ap, (1 - p_ap))\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.563"},{"path":"bayesian-inference.html","id":"chapter-3","chapter":"Week 1: Bayesian Inference","heading":"1.2.3 Chapter 3","text":"Easy. Easy problems use sample posterior distribution globe tossing example. code give specific set samples, can check answers exactly.Use values samples answer questions follow.3E1. much posterior probability lies p = 0.2?3E2. much posterior probability lies p = 0.8?3E3. much posterior probability lies p = 0.2 p = 0.8?3E4. 20% posterior probability lies value p?3E5. 20% posterior probability lies value p?3E6. values p contain narrowest interval equal 66% posterior probability?3E7. values p contain 66% posterior probability, assuming equal posterior probability interval?3M1. Suppose globe tossing data turned 8 water 15 tosses. Constructe posterior distribution, using grid approximation. Use flat prior .3M2. Draw 10,000 samples grid approximation . use sample calculate 90% HPDI p.3M3. Construct posterior predictive check model data. means simulate distribution samples, averaging posterior uncertainty p. probability observing 8 water 15 tosses?3M4. Using posterior distribution constructed new (8/15) data, now calculate probability observing 6 water 9 tosses.3M5. Start 3M1, now use prior zero p = 0.5 constant p = 0.5. corresponds prior information majority Earth’s surface water. Repeat problem compare inferences (using priors) true value p = 0.7.HPDI 3M2 much narrower new prior ([.501, .711] vs. [.334, 0.722]). Additionally, probabilities observing 8 15 6 9 increased, value p < 0.5 longer taking posterior density. Thus model new prior giving us better information.3M6. Suppose want estimate Earth’s proportion water precisely. Specifically, want 99% percentile interval posterior distribution p 0.05 wide. means distance upper lower bound interval 0.05. many times toss globe ?figure shows average interval width 100 simulations given number tosses. , true proportion p 0.7 toss globe 1,000 times, average interval width approximately 0.074. get interval width .05 smaller, need around 2,300 tosses.Hard. Hard problems use data . data indicate gender (male = 1, female = 0) officially reported first second born children 100 two-child families. example, first family data reported boy (1) girl (0). second family reported girl (0) boy (1). third family reported two girls. can load tow vectors R’s memory typing:Use vectors data. example compute total number boys born across births, use:3H1. Using grid approximation, compute posterior distribution probability birth boy. Assume uniform prior probability. parameter value maximizes posterior probability?3H2. Using sample function, draw 10,000 random parameter values posterior distribution calculated . Use sample estimate 50%, 89%, 97% highest posterior density intervals.3H3. Use rbinom simulate 10,000 replicates 200 births. end 10,000 numbers, one count boys 200 births. Compare distribution predicted numbers boys actual count data (111 boys 200 births). many good ways visualize simulations, dens command (part rethinking package) prbably easiest way case. look like model fits data well? , distribution predictions include actual observation central, likely outcome?Based posterior predictive distribution, model appears fit well, observed value 111 middle distribution.3H4. Now compare 10,000 counts boys 100 simulated first borns number boys first births, birth1. model look light?Based first births , model appears fit less well. Specifically, model appears overestimating number first births boys. However, appear large discrepancy, observed value still within middle 66% interval.3H5. model assumes sex first second births independent. check assumption, focus now second births followed female first borns. Compare 10,000 simulated conts boys second births followed girls. correctly, need count number first borns girls simulate many births, 10,000 times. Compare counts boys simulations actual observed count boys following girls. model look light? guesses going data?model severely estimating number second-born boys first born child girl. Thus, assumption births independent appears violated.","code":"\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\nlikelihood <- dbinom(6, size = 9, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nset.seed(100)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nmean(samples < 0.2)\n#> [1] 4e-04\nmean(samples > 0.8)\n#> [1] 0.112\nlibrary(tidyverse)\nmean(between(samples, 0.2, 0.8))\n#> [1] 0.888\nquantile(samples, probs = 0.2)\n#>   20% \n#> 0.519\nquantile(samples, probs = 0.8)\n#>   80% \n#> 0.756\nlibrary(rethinking)\nHPDI(samples, prob = 0.66)\n#> |0.66 0.66| \n#> 0.509 0.774\nPI(samples, prob = 0.66)\n#>   17%   83% \n#> 0.503 0.770\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %>%\n  ggplot(aes(x = p, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\nset.seed(101)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\n\nHPDI(samples, prob = 0.9)\n#>  |0.9  0.9| \n#> 0.334 0.722\nw <- rbinom(1e4, size = 15, prob = samples)\nmean(w == 8)\n#> [1] 0.15\nw <- rbinom(1e4, size = 9, prob = samples)\nmean(w == 6)\n#> [1] 0.171\n# 3M5.1\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- case_when(p_grid < 0.5 ~ 0L,\n                   TRUE ~ 1L)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %>%\n  ggplot(aes(x = p, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n# 3M5.2\nset.seed(101)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nHPDI(samples, prob = 0.9)\n#>  |0.9  0.9| \n#> 0.501 0.711\n\n# 3M5.3\nw <- rbinom(1e4, size = 15, prob = samples)\nmean(w == 8)\n#> [1] 0.161\n\n# 3M5.4\nw <- rbinom(1e4, size = 9, prob = samples)\nmean(w == 6)\n#> [1] 0.236\nsingle_sim <- function(tosses, prior_type = c(\"uniform\", \"step\")) {\n  prior_type <- match.arg(prior_type)\n  obs <- rbinom(1, size = tosses, prob = 0.7)\n  \n  p_grid <- seq(from = 0, to = 1, length.out = 1000)\n  prior <- rep(1, 1000)\n  if (prior_type == \"step\") prior[1:500] <- 0\n  \n  likelihood <- dbinom(obs, size = tosses, prob = p_grid)\n  posterior <- likelihood * prior\n  posterior <- posterior / sum(posterior)\n  \n  samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\n  interval <- PI(samples, prob = 0.99)\n  width <- interval[2] - interval[1]\n}\nsingle_cond <- function(tosses, prior_type, reps = 100) {\n  tibble(tosses = tosses,\n         prior_type = prior_type,\n         width = map_dbl(seq_len(reps), ~single_sim(tosses = tosses,\n                                                    prior_type = prior_type)))\n}\n\nsimulation <- crossing(tosses = seq(1000, 5000, by = 100),\n                       prior_type = c(\"uniform\", \"step\")) %>%\n  pmap_dfr(single_cond, reps = 100) %>%\n  group_by(tosses, prior_type) %>%\n  summarize(avg_width = mean(width), .groups = \"drop\") %>%\n  mutate(prior_type = case_when(prior_type == \"uniform\" ~ \"Uniform Prior\",\n                                prior_type == \"step\" ~ \"Step Prior\"),\n         prior_type = factor(prior_type, levels = c(\"Uniform Prior\",\n                                                    \"Step Prior\")))\n\nggplot(simulation, aes(x = tosses, y = avg_width)) +\n  facet_wrap(~prior_type, nrow = 1) +\n  geom_point() +\n  geom_line() +\n  scale_x_comma() +\n  labs(x = \"Tosses\", y = \"Average Interval Width\") +\n  theme(panel.spacing.x = unit(2, \"lines\"))\ndata(homeworkch3)\nbirth1\n#>   [1] 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1\n#>  [38] 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1\n#>  [75] 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\nbirth2\n#>   [1] 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0\n#>  [38] 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0\n#>  [75] 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0\nsum(birth1) + sum(birth2)\n#> [1] 111\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\n\nboys <- sum(birth1) + sum(birth2)\ntotal <- length(birth1) + length(birth2)\nlikelihood <- dbinom(boys, size = total, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %>%\n  ggplot(aes(x = p, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Boys (p)\", y = \"Posterior Density\")\n\np_grid[which.max(posterior)]\n#> [1] 0.555\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nHPDI(samples, prob = c(0.50, 0.89, 0.97))\n#> |0.97 |0.89  |0.5  0.5| 0.89| 0.97| \n#> 0.474 0.496 0.531 0.578 0.609 0.628\nlibrary(tidybayes)\nbreak_func <- function(x) {\n  length(seq(min(x), max(x), by = 1)) + 1\n}\n\nset.seed(300)\nb <- rbinom(1e4, size = 200, prob = samples)\n\nggplot() +\n  stat_histinterval(aes(x = b), .width = c(0.66, 0.89), breaks = break_func) +\n  geom_vline(aes(xintercept = boys), linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Number of Boys\", y = \"Density\")\nset.seed(301)\nb <- rbinom(1e4, size = 100, prob = samples)\n\n\nggplot() +\n  stat_histinterval(aes(x = b), .width = c(0.66, 0.89), breaks = break_func) +\n  geom_vline(aes(xintercept = sum(birth1)), linetype = \"dashed\",\n             color = \"red\") +\n  labs(x = \"Number of Boys\", y = \"Density\")\nfb_girls <- length(birth1) - sum(birth1)\n\nset.seed(302)\nb <- rbinom(1e4, size = fb_girls, prob = samples)\n\nobs_bfg <- sum(birth2[which(birth1 == 0)])\n\nggplot() +\n  stat_histinterval(aes(x = b), .width = c(0.66, 0.89), breaks = break_func) +\n  geom_vline(aes(xintercept = obs_bfg), linetype = \"dashed\",\n             color = \"red\") +\n  labs(x = \"Number of Boys\", y = \"Density\")"},{"path":"bayesian-inference.html","id":"homework","chapter":"Week 1: Bayesian Inference","heading":"1.3 Homework","text":"1. Suppose globe tossing data (Chapter 2) turned 4 water 11 land. Construct posterior distribution, using grid approximation. Use flat prior book.First, create grid create prior, constant values grid.Next, calculate likelihood 4 water 11 land value grid normalize values sum 1.Finally, can sample visualize posterior distribution. majority posterior 0.5, mean 0.30. surprising, given data indicates (.e., land observed water).2. Now suppose data 4 water 2 land. Compute posterior , time use prior zero p = 0.5 constant p = 0.5. corresponds prior information majority Earth’s surface water.follow process, now define prior 0 p < .5 1 p ≥ .5.now see posterior distribution truncated .5. mean around 0.69, expected given 4 6 trials (two-thirds) water.3. posterior distribution 2, compute 89% percentile HPDI intervals. Compare widths intervals. wider? ? information interval, might misunderstand shape posterior distribution?First, take 10,000 samples posterior calculated previous question. summarize PI() HPDI().89% percentile interval [0.525, 0.882], highest posterior density interval [0.503, 0.848]. percentile interval 0.357 wide, highest posterior density interval 0.345 wide. Thus, percentile interval wider. HPDI finds narrowest interval contains 89% data. Therefore, unless posterior perfectly symmetrical, central 89% , definition, wider HDPI.intervals, boundary information conveyed. information actual shape posterior conveyed. Without visualizing posterior, interval boundaries might tempt us (incorrectly) assume values interval equally likely values middle range plausible. However, know previous problem posterior asymmetrical, values closer low end interval plausible values high end interval.4. OPTIONAL CHALLENGE. Suppose bias sampling Land likely Water recorded. Specifically, assume 1--5 (20%) Water samples accidentally recorded instead “Land.” First, write generative simulation sampling process. Assuming true proportion Water 0.70, proportion simulation tend produce instead? Second, using simulated sample 20 tosses, compute unbiased posterior distribution true proportion water.’ll start writing function generate biased data. First, generate specified number tosses true_prop. Normally, use rbinom(n = 1, size = tosses), provide total number successes (tosses “water” result). want results individual toss. , example, rather 1 trial 100 tosses, 100 trials 1 toss.Next, generate random uniform numbers 0 1. indicate trials biased. every element bias_sim less bias, corresponding element true_trials changed 0 new vector called bias_trials. original results 0 (“land”), bias effect (.e., 0 changed 0). original result 1 (“water”), bias chance result changed 0.Finally, sum total number 1s (“water”) implementing bias return results.bias mean estimated proportion water? explore, ’ll run short simulation. ’ll use biased_globe() function 1,000 simulations 100 tosses. simulations, can see proportion tosses water, bias included.previous figure, true proportion, 0.7, indicated dashed red line. However, can see biased tosses greatly reducing number “waters” actually observe. average, observe 56 waters, estimated proportion 0.56.final step compute unbiased posterior distribution true proportion water, using 20 tosses biased data generation process. ’ll start generating 20 tosses.data, observed 11 waters. final step estimate unbiased posterior. can accomplish using biased sampling rate 0.8. , true water observation 80% chance actually recorded water observation. shown crtd_likelihood line . dbinom() function, rather setting prob = p_grid, set prob = p_grid * 0.8, reflect bias sampling. see biased posterior peaks around 0.55, whereas correcting bias results posterior peaks right around 0.70.","code":"\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),\n               prior = rep(1, times = 1000))\ndist\n#> # A tibble: 1,000 × 2\n#>     p_grid prior\n#>      <dbl> <dbl>\n#>  1 0           1\n#>  2 0.00100     1\n#>  3 0.00200     1\n#>  4 0.00300     1\n#>  5 0.00400     1\n#>  6 0.00501     1\n#>  7 0.00601     1\n#>  8 0.00701     1\n#>  9 0.00801     1\n#> 10 0.00901     1\n#> # … with 990 more rows\ndist <- dist %>% \n  mutate(likelihood = dbinom(4, size = 15, prob = p_grid),\n         posterior = likelihood * prior,\n         posterior = posterior / sum(posterior))\ndist\n#> # A tibble: 1,000 × 4\n#>     p_grid prior    likelihood posterior\n#>      <dbl> <dbl>         <dbl>     <dbl>\n#>  1 0           1 0              0       \n#>  2 0.00100     1 0.00000000136  2.17e-11\n#>  3 0.00200     1 0.0000000214   3.44e-10\n#>  4 0.00300     1 0.000000107    1.72e- 9\n#>  5 0.00400     1 0.000000336    5.38e- 9\n#>  6 0.00501     1 0.000000811    1.30e- 8\n#>  7 0.00601     1 0.00000166     2.66e- 8\n#>  8 0.00701     1 0.00000305     4.88e- 8\n#>  9 0.00801     1 0.00000514     8.23e- 8\n#> 10 0.00901     1 0.00000814     1.30e- 7\n#> # … with 990 more rows\nset.seed(123)\n\ndist %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE) %>% \n  ggplot(aes(x = p_grid)) +\n  stat_histinterval(.width = c(0.67, 0.89, 0.97), breaks = seq(0, 1, 0.02),\n                    point_interval = mean_hdci) +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),\n               prior = rep(c(0, 1), each = 500)) %>% \n  mutate(likelihood = dbinom(4, size = 6, prob = p_grid),\n         posterior = likelihood * prior,\n         posterior = posterior / sum(posterior))\ndist\n#> # A tibble: 1,000 × 4\n#>     p_grid prior likelihood posterior\n#>      <dbl> <dbl>      <dbl>     <dbl>\n#>  1 0           0   0                0\n#>  2 0.00100     0   1.50e-11         0\n#>  3 0.00200     0   2.40e-10         0\n#>  4 0.00300     0   1.21e- 9         0\n#>  5 0.00400     0   3.82e- 9         0\n#>  6 0.00501     0   9.32e- 9         0\n#>  7 0.00601     0   1.93e- 8         0\n#>  8 0.00701     0   3.57e- 8         0\n#>  9 0.00801     0   6.07e- 8         0\n#> 10 0.00901     0   9.70e- 8         0\n#> # … with 990 more rows\nset.seed(123)\n\ndist %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE) %>% \n  ggplot(aes(x = p_grid)) +\n  stat_histinterval(.width = c(0.67, 0.89, 0.97), breaks = seq(0, 1, 0.02),\n                    point_interval = mean_hdci) +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\nset.seed(123)\n\nintervals <- dist %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE) %>% \n  summarize(bound = c(\"lower\", \"upper\"),\n            pi = PI(p_grid, prob = 0.89),\n            hpdi = HPDI(p_grid, prob = 0.89))\nintervals\n#> # A tibble: 2 × 3\n#>   bound    pi  hpdi\n#>   <chr> <dbl> <dbl>\n#> 1 lower 0.525 0.503\n#> 2 upper 0.882 0.848\nbiased_globe <- function(tosses = 100, true_prop = 0.7, bias = 0.2) {\n  true_trials <- rbinom(n = tosses, size = 1, prob = true_prop)\n  bias_sim <- runif(n = tosses, min = 0, max = 1)\n\n  bias_trials <- true_trials\n  bias_trials[which(bias_sim < bias)] <- 0L\n  sum(bias_trials)\n}\nbias_prop <- map_int(seq_len(1000),\n                     ~biased_globe(tosses = 100, true_prop = 0.7, bias = 0.2))\n\nggplot() +\n  stat_histinterval(aes(x = bias_prop / 100), .width = c(0.67, 0.89, 0.97),\n                    breaks = seq(0, 1, by = 0.01)) +\n  geom_vline(aes(xintercept = 0.7), linetype = \"dashed\", color = \"red\") +\n  expand_limits(x = c(0, 1)) +\n  labs(x = \"Proportion Water (p)\", y = \"Simulations\")\nset.seed(123)\nbiased_dat <- biased_globe(tosses = 20)\nbiased_dat\n#> [1] 11\nlibrary(geomtextpath)\n\nposterior <- tibble(p_grid = seq(0, 1, length.out = 1000)) %>% \n  mutate(prior = dbeta(p_grid, shape1 = 1, shape2 = 1),\n         bias_likelihood = dbinom(biased_dat, size = 20, prob = p_grid),\n         crtd_likelihood = dbinom(biased_dat, size = 20, prob = p_grid * 0.8),\n         bias_posterior = bias_likelihood * prior,\n         crtd_posterior = crtd_likelihood * prior,\n         bias_posterior = bias_posterior / sum(bias_posterior),\n         crtd_posterior = crtd_posterior / sum(crtd_posterior))\n\nggplot(posterior, aes(x = p_grid)) +\n  geom_textline(aes(y = bias_posterior), linetype = \"dashed\", color = \"grey70\",\n                size = 6, linewidth = 1, label = \"Biased\", hjust = 0.45,\n                family = \"Source Sans Pro\") +\n  geom_textline(aes(y = crtd_posterior), linetype = \"solid\", color = \"#009FB7\",\n                size = 6, linewidth = 1, label = \"Corrected\", hjust = 0.4,\n                family = \"Source Sans Pro\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")"},{"path":"linear-models-causal-inference.html","id":"linear-models-causal-inference","chapter":"Week 2: Linear Models & Causal Inference","heading":"Week 2: Linear Models & Causal Inference","text":"second week covers Chapter 4 (Geocentric Models).","code":""},{"path":"linear-models-causal-inference.html","id":"lectures-1","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.1 Lectures","text":"Lecture 3:Lecture 4:","code":""},{"path":"linear-models-causal-inference.html","id":"exercises-1","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.2 Exercises","text":"","code":""},{"path":"linear-models-causal-inference.html","id":"chapter-4","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.2.1 Chapter 4","text":"4E1. model definition , line likelihood?\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu,\\sigma) \\\\\n\\mu &\\sim \\text{Normal}(0,10) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]likelihood given first line, \\(y_i \\sim \\text{Normal}(\\mu,\\sigma)\\). lines represent prior distributions \\(\\mu\\) \\(\\sigma\\).4E2. model definition just , many parameters posterior distribution?two parameters, \\(\\mu\\) \\(\\sigma\\).4E3. Using model definition , write appropriate form Bayes’ theorem includes proper likelihood priors.\\[\n\\text{Pr}(\\mu,\\sigma|y) = \\frac{\\text{Normal}(y|\\mu,\\sigma)\\text{Normal}(\\mu|0,10)\\text{Exponential}(\\sigma|1)}{\\int\\int\\text{Normal}(y|\\mu,\\sigma)\\text{Normal}(\\mu|0,10)\\text{Exponential}(\\sigma|1)d \\mu d \\sigma}\n\\]4E4. model definition , line linear model?\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu,\\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\alpha &\\sim \\text{Normal}(0,10) \\\\\n\\beta &\\sim \\text{Normal}(0,1) \\\\\n\\sigma &\\sim \\text{Exponential}(2)\n\\end{align}\\]linear model second line, \\(\\mu_i = \\alpha + \\beta x_i\\).4E5. model definition just , many parameters posterior distribution?now three model parameters: \\(\\alpha\\), \\(\\beta\\), \\(\\sigma\\). mean, \\(\\mu\\) longer parameter, defined deterministically, function parameters model.4M1. model definition , simulate observed y values prior (posterior).\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu,\\sigma) \\\\\n\\mu &\\sim \\text{Normal}(0,10) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]4M2. Trasnlate model just quap formula.4M3. Translate quap model formula mathematical model definition.\\[\\begin{align}\n  y_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta x_i \\\\\n  \\alpha &\\sim \\text{Normal}(0, 10) \\\\\n  \\beta &\\sim \\text{Uniform}(0, 1) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]4M4. sample students measured height year 3 years. third year, want fit linear regression predicting height using year predictor. Write mathematical model definition regression, using variable names priors choose. prepared defend choice priors.\\[\\begin{align}\n  h_{ij} &\\sim \\text{Normal}(\\mu_{ij}, \\sigma) \\\\\n  \\mu_{ij} &= \\alpha + \\beta(y_j - \\bar{y}) \\\\\n  \\alpha &\\sim \\text{Normal}(100, 10) \\\\\n  \\beta &\\sim \\text{Normal}(0, 10) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]height centered, \\(\\alpha\\) represents average height average year (.e., year 2). prior \\(\\text{Normal}(100, 10)\\) chosen assuming height measured centimeters sample children still growing.slope extremely vague. prior centered zero, standard deviation prior 10 represents wide range possible growth (shrinkage). growth spurts, height growth averages 6–13 cm/year. standard deviation 10 encompasses range might expect see growth occurring high rate.Finally, exponential prior \\(\\sigma\\) assumes average deviation 1.Prior predictive simulations also appear give reasonably plausible regression lines, given current assumptions.4M5. Now suppose remind every student got taller year. information lead change choice priors? ?Yes. know increase year always lead increased height, know \\(\\beta\\) positive. Therefore, prior reflect using, example, log-normal distribution.\\[\n\\beta \\sim \\text{Log-Normal}(1,0.5)\n\\]prior gives expectation 3cm per year, 89% highest density interval 0.87cm 5.18cm per year.Prior predictive simulations plausible lines using new log-normal prior indicate priors still represent plausible values. lines positive, due prior constraint. However, variation around mean, lines show decrease height. truly impossible students shrink, data like might arise measurement error.4M6. Now suppose tell variance among heights students age never 64cm. lead revise priors?variance 64cm corresponds standard deviation 8cm. current prior \\(\\sigma \\sim \\text{Exponential}(1)\\) gives little probability mass values greater 8. However, still theoretically possible. want truly constrain variance way, use \\(\\text{Uniform}(0,8)\\) prior. eliminate values result variance greater 64cm.4M7. Refit model m4.3 chapter, omit mean weight xbar time. Compare new model’s posterior original model. particular, look covariance among parameters. different? compare posterior predictions models.First, ’ll reproduce m4.3 using quap(), using brm(). covariance matrix , expect.Now, let’s using non-centered parameterization. time, ’ll use brm().now see non-zero covariances parameters. Lets compare posterior predictions. ’ll generate hypothetical outcome plots animated show uncertainty estimates (see Hullman et al., 2015; Kale et al., 2019). ’ll just animate estimated regression line using {gganimate} (Pedersen & Robinson, 2020). can see predictions two models nearly identical.4M8. chapter, used 15 knots cherry blossom spline. Increase number knots observe happens resulting spline. adjust also width prior weights—change standard deviation prior watch happens. think combination know number prior weights controls?First lets duplicate 15-knot spline model chapter. ’ll double number knots play prior.Visualizing model, see looks similar fitted model chapter.Now ’ll fit two additional models. first uses 30 knots, second uses tighter prior.expected, visualize models see increasing number knots increases “wiggly-ness” spline. can also see tightening prior weights takes away “wiggly-ness”.4H1. weights listed recored !Kung census, heights recorded individuals. Provide predicted heights 89% intervals individuals. , fill table, , using model-based predictions.key function tidybayes::add_predicted_draws(), samples posterior predictive distribution. use model b4.3 make predictions, estimated back question 4M7.4H2. Select rows Howell1 data ages 18 years age. right, end new data frame 192 rows .Fit linear regression data, using quap. Present interpret estimates. every 10 units increase weight, much taller model predict child gets?’ll use brms::brm() fitting model. ’ll use priors model adults, except weight \\(\\beta\\). children shorter adults, ’ll use prior \\(\\text{Normal}(138,20)\\), based data reported Fryar et al. (2016). Based estimates, increase 10 units weights corresponds average increase height 27.2 centimeters.Plot raw data, height vertical axis weight horizontal axis. Superimpose MAP regression line 89% interval mean. Also superimpose 89% interval predicted heights.aspects model fit concern ? Describe kinds assumptions change, , improve model. don’t write new code. Just explain model appears bad job , hypothesize better model.model consistently -estimates height individuals weight less ~13 great ~35. model also consistently underestimating height individuals weight ~13-35. Thus, data appears curve assumption straight line violating. wanted improve model, relax assumption straight line.4H3. Suppose colleauge , works allometry, glances practice problems just . colleague exclaims, “’s silly. Everyone knows ’s logarithm body weight scales height!” Let’s take colleague’s advice see happens.Model relationship height (cm) natural logarithm weight (log-kg). Use entire Howell1 data frame, 544 rows, adults non-adults. Can interpret resulting estimates?Conditional data model, intercept estimate sprintf(\"%0.1f\", summary(b4h3)[[\"fixed\"]][\"Intercept\", \"Estimate\"]) represents predicted average height individual average log-weight (log-kg). \\(\\beta\\) estimate sprintf(\"%0.1f\", summary(b4h3)[[\"fixed\"]][\"log_weight_c\", \"Estimate\"]) represents average expected increase height associated one-unit increase weight (log-kg).Begin plot: plot( height ~ weight , data = Howell1 ). use samples quadratic approximate posterior model () superimpose plot: (1) predicted mean height function weight, (2) 97% interval mean, (3) 97% interval predicted heights.4H4. Plot prior predictive distribution parabolic polynomial regression model chapter. can modify code plots linear regression prior predictive distribution. Can modify prior distributions \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\) prior predictions stay within biologically reasonable outcome space? say: try fit data hand. try keep curves consistent know height weight, seeing exact data.polynomial model chapter defined :\\[\\begin{align}\n  h_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_1x_i + \\beta_2x_i^2 \\\\\n  \\alpha &\\sim \\text{Normal}(178,20) \\\\\n  \\beta_1 &\\sim \\text{Log-Normal}(0,1) \\\\\n  \\beta_2 &\\sim \\text{Normal}(0,1) \\\\\n  \\sigma &\\sim \\text{Uniform}(0,50)\n\\end{align}\\]First, let’s generate prior predictive checks original priors.Clearly room improvement . However, ’s intuitive exactly parameter effects parabolic curve, finding good prior distribution really hard! much trial error playing parabola calculators online, ended :\\[\\begin{align}\n  h_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_1x_i + \\beta_2x_i^2 \\\\\n  \\alpha &\\sim \\text{Normal}(-190,5) \\\\\n  \\beta_1 &\\sim \\text{Normal}(13,0.2) \\\\\n  \\beta_2 &\\sim \\text{Uniform}(-0.13,-0.10) \\\\\n  \\sigma &\\sim \\text{Uniform}(0,50)\n\\end{align}\\]following prior predictive distribution.4H5. Return data(cherry_blossoms) model association blossom date (doy) March temperature (temp). Note many missing values variables. may consider linear model, polynomial, spline temperature. well temperature trend predict blossom trend?’ll try type model: linear, polynomial, spline. , ’ll fit model, visualize predictions observed data.Now let’s visualize predictions model. Overall predictions model remarkably similar. Therefore, go linear model, simplest models.4H6. Simulate prior predictive distribution cherry blossom spline chapter. Adjust prior weights observe happens. think prior weights ?reminder, cherry blossom spline model chapter:\\[\\begin{align}\n  D_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\sum_{k=1}^Kw_kB_{k,} \\\\\n  \\alpha &\\sim \\text{Normal}(100, 10) \\\\\n  w_k &\\sim \\text{Normal}(0,10) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]’ll also need recreate basis functions model uses:Finally, can generate data priors, combine parameters basis functions get prior predictive distributions.Now let’s tighten prior w \\(\\text{Normal}(0,2)\\), used exercise 4M8. Now lines much less wiggly, consistent found previous exercise, used observed data.4H8. (sic; 4H7 text, ’ve kept labeled 4H8 consistent book) cherry blossom spline chapter used intercept \\(\\alpha\\), technically doesn’t require one. first basis functions substitute intercept. Try refitting cherry blossom spline without intercept. else model need change make work?can remove intercept removing parameter model. {brms} formula, means replace doy ~ 1 + B question 4M8 doy ~ 0 + B. 0 means “don’t estimate intercept.”looks lot like original model, except left hand side spline pulled . likely due prior w. prior centered 0, assumes intercept present (.e., curves spline average deviation 0 mean). However, without intercept, prior drags line actual zero first basis function non-zero. changing prior w prior originally used intercept, get almost original model back.","code":"\nlibrary(tidyverse)\n\nsim <- tibble(mu = rnorm(n = 10000, mean = 0, sd = 10),\n              sigma = rexp(n = 10000, rate = 1)) %>%\n  mutate(y = rnorm(n = 10000, mean = mu, sd = sigma))\n\nggplot(sim, aes(x = y)) +\n  geom_density() +\n  labs(x = \"y\", y = \"Density\")\nflist <- alist(\n  y ~ dnorm(mu, sigma),\n  mu ~ dnorm(0, 10),\n  sigma ~ dexp(1)\n)y ~ dnorm( mu , sigma ),\nmu <- a + b*x,\na ~ dnorm( 0 , 10 ),\nb ~ dunif( 0 , 1 ),\nsigma ~ dexp( 1 )\nn <- 50\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       beta = rnorm(n, 0, 10),\n       sigma = rexp(n, 1)) %>%\n  expand(nesting(group, alpha, beta, sigma), year = c(1, 2, 3)) %>%\n  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma)) %>%\n  ggplot(aes(x = year, y = height, group = group)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Height\")\nlibrary(tidybayes)\n\nset.seed(123)\nsamples <- rlnorm(1e8, 1, 0.5)\nbounds <- mean_hdi(samples, .width = 0.89)\n\nggplot() +\n  stat_function(data = tibble(x = c(0, 10)), mapping = aes(x = x),\n                geom = \"line\", fun = dlnorm,\n                args = list(meanlog = 1, sdlog = 0.5)) +\n  geom_ribbon(data = tibble(x = seq(bounds$ymin, bounds$ymax, 0.01)),\n              aes(x = x, ymin = 0, ymax = dlnorm(x, 1, 0.5)),\n              alpha = 0.8) +\n  scale_x_continuous(breaks = seq(0, 10, 2)) +\n  labs(x = expression(beta), y = \"Density\")\nn <- 50\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       beta = rlnorm(n, 1, 0.5),\n       sigma = rexp(n, 1)) %>%\n  expand(nesting(group, alpha, beta, sigma), year = c(1, 2, 3)) %>%\n  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma)) %>% \n  ggplot(aes(x = year, y = height, group = group)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Height\")\nlibrary(rethinking)\nlibrary(brms)\n\ndata(Howell1)\nhow_dat <- Howell1 %>%\n  filter(age >= 18) %>%\n  mutate(weight_c = weight - mean(weight))\n\n# first, duplicate model with `quap`\nm4.3 <- quap(alist(height ~ dnorm(mu, sigma),\n                   mu <- a + b * (weight_c),\n                   a ~ dnorm(178, 20),\n                   b ~ dlnorm(0, 1),\n                   sigma ~ dunif(0, 50)),\n             data = how_dat)\n\nround(vcov(m4.3), 3)\n#>           a     b sigma\n#> a     0.073 0.000 0.000\n#> b     0.000 0.002 0.000\n#> sigma 0.000 0.000 0.037\n\n# and then with brms\nb4.3 <- brm(height ~ 1 + weight_c, data = how_dat, family = gaussian,\n            prior = c(prior(normal(178, 20), class = Intercept),\n                      prior(lognormal(0, 1), class = b, lb = 0),\n                      prior(uniform(0, 50), class = sigma)),\n            iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4.3-0\"))\n\nas_draws_df(b4.3) %>%\n  as_tibble() %>% \n  select(b_Intercept, b_weight_c, sigma) %>%\n  cov() %>%\n  round(digits = 3)\n#>             b_Intercept b_weight_c sigma\n#> b_Intercept       0.074      0.000 0.000\n#> b_weight_c        0.000      0.002 0.000\n#> sigma             0.000      0.000 0.038\nb4.3_nc <- brm(height ~ 1 + weight, data = how_dat, family = gaussian,\n               prior = c(prior(normal(178, 20), class = Intercept),\n                         prior(lognormal(0, 1), class = b, lb = 0),\n                         prior(uniform(0, 50), class = sigma)),\n               iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4.3_nc\"))\n\nas_draws_df(b4.3_nc) %>%\n  as_tibble() %>% \n  select(b_Intercept, b_weight, sigma) %>%\n  cov() %>%\n  round(digits = 3)\n#>             b_Intercept b_weight sigma\n#> b_Intercept       3.653   -0.079 0.010\n#> b_weight         -0.079    0.002 0.000\n#> sigma             0.010    0.000 0.035\nlibrary(gganimate)\n\nweight_seq <- tibble(weight = seq(25, 70, length.out = 100)) %>%\n  mutate(weight_c = weight - mean(how_dat$weight))\n\npredictions <- bind_rows(\n  predict(b4.3, newdata = weight_seq) %>%\n    as_tibble() %>%\n    bind_cols(weight_seq) %>%\n    mutate(type = \"Centered\"),\n  predict(b4.3_nc, newdata = weight_seq) %>%\n    as_tibble() %>%\n    bind_cols(weight_seq) %>%\n    mutate(type = \"Non-centered\")\n)\n\nfits <- bind_rows(\n  weight_seq %>%\n    add_epred_draws(b4.3) %>%\n    mutate(type = \"Centered\"),\n  weight_seq %>%\n    add_epred_draws(b4.3_nc) %>%\n    mutate(type = \"Non-centered\")\n) %>%\n  ungroup()\n\nbands <- fits %>%\n  group_by(type, weight) %>%\n  median_qi(.epred, .width = c(.67, .89, .97))\n\nlines <- fits %>%\n  filter(.draw %in% sample(unique(.data$.draw), size = 50))\n\nggplot(lines, aes(x = weight)) +\n  facet_wrap(~type, nrow = 1) +\n  geom_ribbon(data = predictions, aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3) +\n  geom_lineribbon(data = bands, aes(y = .epred, ymin = .lower, ymax = .upper),\n                  color = NA) +\n  scale_fill_brewer(palette = \"Blues\", breaks = c(.67, .89, .97)) +\n  geom_line(aes(y = .epred, group = .draw)) +\n  geom_point(data = how_dat, aes(y = height), shape = 1, alpha = 0.7) +\n  labs(x = \"Weight\", y = \"Height\", fill = \"Interval\") +\n  theme(legend.position = \"bottom\") +\n  transition_states(.draw, 0, 1)\nlibrary(splines)\n\ndata(cherry_blossoms)\ncb_dat <- cherry_blossoms %>%\n  drop_na(doy)\n\n# original m4.7 model\nknots_15 <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15))\nB_15 <- bs(cb_dat$year, knots = knots_15[-c(1, 15)],\n           degree = 3, intercept = TRUE)\n\ncb_dat_15 <- cb_dat %>% \n  mutate(B = B_15)\n\nb4.7 <- brm(doy ~ 1 + B, data = cb_dat_15, family = gaussian,\n            prior = c(prior(normal(100, 10), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4.7\"))\noriginal_draws <- cb_dat_15 %>% \n  add_epred_draws(b4.7) %>% \n  summarize(mean_hdi(.epred, .width = 0.89),\n            .groups = \"drop\")\n\nggplot(original_draws, aes(x = year, y = doy)) +\n  geom_vline(xintercept = knots_15, alpha = 0.5) +\n  geom_hline(yintercept = fixef(b4.7)[1, 1], linetype = \"dashed\") +\n  geom_point(alpha = 0.5) +\n  geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = \"#009FB7\", alpha = 0.8) +\n  labs(x = \"Year\", y = \"Day in Year\")\n# double the number of knots\nknots_30 <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 30))\nB_30 <- bs(cb_dat$year, knots = knots_30[-c(1, 30)],\n           degree = 3, intercept = TRUE)\n\ncb_dat_30 <- cb_dat %>% \n  mutate(B = B_30)\n\nb4.7_30 <- brm(doy ~ 1 + B, data = cb_dat_30, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4.7_30\"))\n\n# and modify the prior\nb4.7_30p <- brm(doy ~ 1 + B, data = cb_dat_30, family = gaussian,\n                prior = c(prior(normal(100, 10), class = Intercept),\n                          prior(normal(0, 2), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp4\", \"b4.7_30p\"))\n# create plot data\nspline_15 <- original_draws %>%\n  select(-B) %>% \n  mutate(knots = \"15 knots (original model)\")\n\nspline_30 <- cb_dat_30 %>% \n  add_epred_draws(b4.7_30) %>% \n  summarize(mean_hdi(.epred, .width = 0.89),\n            .groups = \"drop\") %>% \n  select(-B) %>% \n  mutate(knots = \"30 knots\")\n\nspline_30p <- cb_dat_30 %>% \n  add_epred_draws(b4.7_30p) %>% \n  summarize(mean_hdi(.epred, .width = 0.89),\n            .groups = \"drop\") %>% \n  select(-B) %>% \n  mutate(knots = \"30 knots; Tight prior\")\n\nall_splines <- bind_rows(spline_15, spline_30, spline_30p)\n\n# make plot\nggplot(all_splines, aes(x = year, y = doy)) +\n  geom_point(alpha = 0.5) +\n  geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = \"#009FB7\", alpha = 0.8) +\n  facet_wrap(~knots, ncol = 1) +\n  theme(strip.text = element_text(size = 12, hjust = 0)) +\n  labs(x = \"Year\", y = \"Day in Year\")\ntibble(individual = 1:5,\n       weight = c(46.95, 43.72, 64.78, 32.59, 54.63)) %>%\n  mutate(weight_c = weight - mean(how_dat$weight)) %>%\n  add_predicted_draws(b4.3) %>%\n  group_by(individual, weight) %>%\n  mean_qi(.prediction, .width = 0.89) %>%\n  mutate(range = glue(\"[{sprintf('%0.1f', .lower)}--\",\n                      \"{sprintf('%0.1f', .upper)}]\"),\n         .prediction = sprintf(\"%0.1f\", .prediction)) %>%\n  select(individual, weight, exp = .prediction, range) %>%\n  kbl(align = \"c\", booktabs = TRUE,\n      col.names = c(\"Individual\", \"weight\", \"expected height\", \"89% interval\"))\nyoung_how <- Howell1 %>%\n  filter(age < 18) %>%\n  mutate(weight_c = weight - mean(weight))\nnrow(young_how)\n#> [1] 192\nb4h2 <- brm(height ~ 1 + weight_c, data = young_how, family = gaussian,\n            prior = c(prior(normal(138, 20), class = Intercept),\n                      prior(lognormal(0, 1), class = b, lb = 0),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h2.rds\"))\n\nsummary(b4h2)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: young_how (Number of observations: 192) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   108.36      0.60   107.17   109.55 1.00     6976     5427\n#> weight_c      2.72      0.07     2.58     2.85 1.00     7770     5680\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     8.36      0.42     7.59     9.22 1.00     7618     5811\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nlibrary(modelr)\n\nmod_fits <- young_how %>%\n  data_grid(weight = seq_range(weight, 100)) %>%\n  mutate(weight_c = weight - mean(young_how$weight)) %>%\n  add_epred_draws(b4h2) %>%\n  group_by(weight) %>%\n  mean_qi(.epred, .width = 0.89)\n\nmod_preds <- young_how %>%\n  data_grid(weight = seq_range(weight, 100)) %>%\n  mutate(weight_c = weight - mean(young_how$weight)) %>%\n  add_predicted_draws(b4h2) %>%\n  group_by(weight) %>%\n  mean_qi(.prediction, .width = 0.89)\n\nggplot(young_how, aes(x = weight)) +\n  geom_point(aes(y = height), alpha = 0.4) +\n  geom_ribbon(data = mod_preds, aes(ymin = .lower, ymax = .upper),\n              alpha = 0.2) +\n  geom_lineribbon(data = mod_fits,\n                  aes(y = .epred, ymin = .lower, ymax = .upper),\n                  fill = \"grey60\", size = 1) +\n  labs(x = \"Weight\", y = \"Height\")\nfull_how <- Howell1 %>%\n  mutate(log_weight = log(weight),\n         log_weight_c = log_weight - mean(log_weight))\n\nb4h3 <- brm(height ~ 1 + log_weight_c, data = full_how, family = gaussian,\n            prior = c(prior(normal(158, 20), class = Intercept),\n                      prior(lognormal(0, 1), class = b, lb = 0),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h3\"))\nsummary(b4h3)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + log_weight_c \n#>    Data: full_how (Number of observations: 544) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept      138.27      0.22   137.83   138.70 1.00     8706     6181\n#> log_weight_c    47.08      0.38    46.33    47.81 1.00     8601     6066\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.13      0.15     4.85     5.44 1.00     8332     5723\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nhow_fits <- full_how %>%\n  data_grid(weight = seq_range(weight, 100)) %>%\n  mutate(log_weight = log(weight),\n         log_weight_c = log_weight - mean(full_how$log_weight)) %>%\n  add_epred_draws(b4h3) %>%\n  group_by(weight) %>%\n  mean_qi(.epred, .width = 0.97)\n\nhow_preds <- full_how %>%\n  data_grid(weight = seq_range(weight, 100)) %>%\n  mutate(log_weight = log(weight),\n         log_weight_c = log_weight - mean(full_how$log_weight)) %>%\n  add_predicted_draws(b4h3) %>%\n  group_by(weight) %>%\n  mean_qi(.prediction, .width = 0.97)\n\nggplot(full_how, aes(x = weight)) +\n  geom_point(aes(y = height), alpha = 0.4) +\n  geom_ribbon(data = how_preds, aes(ymin = .lower, ymax = .upper),\n              alpha = 0.2) +\n  geom_lineribbon(data = how_fits,\n                  aes(y = .epred, ymin = .lower, ymax = .upper),\n                  fill = \"grey60\", size = 1) +\n  labs(x = \"Weight\", y = \"Height\")\nn <- 1000\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 178, 20),\n       beta1 = rlnorm(n, 0, 1),\n       beta2 = rnorm(n, 0, 1)) %>%\n  expand(nesting(group, alpha, beta1, beta2),\n         weight = seq(25, 70, length.out = 100)) %>%\n  mutate(height = alpha + (beta1 * weight) + (beta2 * (weight ^ 2))) %>%\n  ggplot(aes(x = weight, y = height, group = group)) +\n  geom_line(alpha = 1 / 10) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, color = \"red\") +\n  annotate(geom = \"text\", x = 25, y = 0, hjust = 0, vjust = 1,\n           label = \"Embryo\") +\n  annotate(geom = \"text\", x = 25, y = 272, hjust = 0, vjust = 0,\n           label = \"World's tallest person (272cm)\") +\n  coord_cartesian(ylim = c(-25, 300)) +\n  labs(x = \"Weight\", y = \"Height\")\nn <- 1000\ntibble(group = seq_len(n),\n       alpha = rnorm(n, -190, 5),\n       beta1 = rnorm(n, 13, 0.2),\n       beta2 = runif(n, -0.13, -0.1)) %>%\n  expand(nesting(group, alpha, beta1, beta2),\n         weight = seq(25, 70, length.out = 100)) %>%\n  mutate(height = alpha + (beta1 * weight) + (beta2 * (weight ^ 2))) %>%\n  ggplot(aes(x = weight, y = height, group = group)) +\n  geom_line(alpha = 1 / 10) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, color = \"red\") +\n  annotate(geom = \"text\", x = 25, y = -3, hjust = 0, vjust = 1,\n           label = \"Embryo\") +\n  annotate(geom = \"text\", x = 25, y = 275, hjust = 0, vjust = 0,\n           label = \"World's tallest person (272cm)\") +\n  coord_cartesian(ylim = c(-25, 300)) +\n  labs(x = \"Weight\", y = \"Height\")\ndata(cherry_blossoms)\n\ncb_temp <- cherry_blossoms %>%\n  drop_na(doy, temp) %>%\n  mutate(temp_c = temp - mean(temp),\n         temp_s = temp_c / sd(temp),\n         temp_s2 = temp_s ^ 2,\n         temp_s3 = temp_s ^ 3)\n\n# linear model\nlin_mod <- brm(doy ~ 1 + temp_c, data = cb_temp, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-linear\"))\n\n# quadratic model\nqad_mod <- brm(doy ~ 1 + temp_s + temp_s2, data = cb_temp, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b, coef = \"temp_s\"),\n                         prior(normal(0, 1), class = b, coef = \"temp_s2\"),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-quadratic\"))\n\n# cubic model\ncub_mod <- brm(doy ~ 1 + temp_s + temp_s2 + temp_s3, data = cb_temp,\n               family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b, coef = \"temp_s\"),\n                         prior(normal(0, 1), class = b, coef = \"temp_s2\"),\n                         prior(normal(0, 1), class = b, coef = \"temp_s3\"),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-cubic\"))\n\n# spline model\nknots_30 <- quantile(cb_temp$temp, probs = seq(0, 1, length.out = 30))\nB_30 <- bs(cb_temp$temp, knots = knots_30[-c(1, 30)],\n           degree = 3, intercept = TRUE)\n\ncb_temp_30 <- cb_temp %>% \n  mutate(B = B_30)\n\nspl_mod <- brm(doy ~ 1 + B, data = cb_temp_30, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-spline\"))\ngrid <- cb_temp_30 %>%\n  data_grid(temp = seq_range(temp, 100)) %>%\n  mutate(temp_c = temp - mean(cb_temp$temp),\n         temp_s = temp_c / sd(cb_temp$temp),\n         temp_s2 = temp_s ^ 2,\n         temp_s3 = temp_s ^ 3)\n\nknots_30 <- quantile(grid$temp, probs = seq(0, 1, length.out = 30))\nB_30 <- bs(grid$temp, knots = knots_30[-c(1, 30)],\n           degree = 3, intercept = TRUE)\n\ngrid <- grid %>% \n  mutate(B = B_30)\n\n\nfits <- bind_rows(\n  add_epred_draws(grid, lin_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Linear\"),\n  add_epred_draws(grid, qad_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Quadratic\"),\n  add_epred_draws(grid, cub_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Cubic\"),\n  add_epred_draws(grid, spl_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Spline\")\n) %>%\n  ungroup() %>%\n  mutate(model = factor(model, levels = c(\"Linear\", \"Quadratic\", \"Cubic\",\n                                          \"Spline\")))\n\npreds <- bind_rows(\n  add_predicted_draws(grid, lin_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Linear\"),\n  add_predicted_draws(grid, qad_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Quadratic\"),\n  add_predicted_draws(grid, cub_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Cubic\"),\n  add_predicted_draws(grid, spl_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Spline\")\n) %>%\n  ungroup() %>%\n  mutate(model = factor(model, levels = c(\"Linear\", \"Quadratic\", \"Cubic\",\n                                          \"Spline\")))\n\nggplot(cb_temp, aes(x = temp)) +\n  facet_wrap(~model, nrow = 2) +\n  geom_point(aes(y = doy), alpha = 0.2) +\n  geom_ribbon(data = preds, aes(ymin = .lower, ymax = .upper),\n              alpha = 0.2) +\n  geom_lineribbon(data = fits, aes(y = .epred, ymin = .lower, ymax = .upper),\n                  size = .6) +\n  scale_fill_brewer(palette = \"Blues\", breaks = c(0.67, 0.89, 0.97)) +\n  labs(x = \"March Temperature\", y = \"Day in Year\") +\n  theme(legend.position = \"bottom\",\n        strip.text = element_text(size = 12))\ncb_dat <- cherry_blossoms %>%\n  drop_na(doy)\n\nknot_list <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15))\n\nB <- bs(cb_dat$year,\n        knots = knot_list[-c(1, 15)],\n        degree = 3, intercept = TRUE)\nn <- 50\ntibble(.draw = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       w = purrr::map(seq_len(n),\n                      function(x, knots) {\n                        w <- rnorm(n = knots + 2, 0, 10)\n                        return(w)\n                      },\n                      knots = 15)) %>%\n  mutate(mu = map2(alpha, w,\n                   function(alpha, w, b) {\n                     res <- b %*% w\n                     res <- res + alpha\n                     res <- res %>%\n                       as_tibble(.name_repair = ~\".value\") %>%\n                       mutate(year = cb_dat$year, .before = 1)\n                     return(res)\n                   },\n                   b = B)) %>%\n  unnest(cols = mu) %>%\n  ggplot(aes(x = year, y = .value)) +\n  geom_vline(xintercept = knot_list, alpha = 0.5) +\n  geom_line(aes(group = .draw)) +\n  expand_limits(y = c(60, 140)) +\n  labs(x = \"Year\", y = \"Day in Year\")\nn <- 50\ntibble(.draw = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       w = purrr::map(seq_len(n),\n                      function(x, knots) {\n                        w <- rnorm(n = knots + 2, 0, 1)\n                        return(w)\n                      },\n                      knots = 15)) %>%\n  mutate(mu = map2(alpha, w,\n                   function(alpha, w, b) {\n                     res <- b %*% w\n                     res <- res + alpha\n                     res <- res %>%\n                       as_tibble(.name_repair = ~\".value\") %>%\n                       mutate(year = cb_dat$year, .before = 1)\n                     return(res)\n                   },\n                   b = B)) %>%\n  unnest(cols = mu) %>%\n  ggplot(aes(x = year, y = .value)) +\n  geom_vline(xintercept = knot_list, alpha = 0.5) +\n  geom_line(aes(group = .draw)) +\n  expand_limits(y = c(60, 140)) +\n  labs(x = \"Year\", y = \"Day in Year\")\ncb_dat <- cherry_blossoms %>%\n  drop_na(doy)\n\nknots_15 <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15))\nB_15 <- bs(cb_dat$year, knots = knots_15[-c(1, 15)],\n           degree = 3, intercept = TRUE)\n\ncb_dat_15 <- cb_dat %>% \n  mutate(B = B_15)\n\nb4h8 <- brm(doy ~ 0 + B, data = cb_dat_15, family = gaussian,\n            prior = c(prior(normal(0, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h8\"))\n\nepred_draws(b4h8, cb_dat_15) %>%\n  summarize(mean_qi(.epred, .width = 0.89), .groups = \"drop\") %>%\n  ggplot(aes(x = year, y = doy)) +\n  geom_point(alpha = 0.2) +\n  geom_hline(yintercept = mean(cb_dat$doy), linetype = \"dashed\") +\n  geom_lineribbon(aes(y = y, ymin = ymin, ymax = ymax),\n                  alpha = 0.8, fill = \"#009FB7\") +\n  labs(x = \"Year\", y = \"Day in Year\")\nb4h8_2 <- brm(doy ~ 0 + B, data = cb_dat_15, family = gaussian,\n            prior = c(prior(normal(100, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h8-2\"))\n\nepred_draws(b4h8_2, cb_dat_15) %>%\n  summarize(mean_qi(.epred, .width = 0.89), .groups = \"drop\") %>%\n  ggplot(aes(x = year, y = doy)) +\n  geom_point(alpha = 0.2) +\n  geom_hline(yintercept = mean(cb_dat$doy), linetype = \"dashed\") +\n  geom_lineribbon(aes(y = y, ymin = ymin, ymax = ymax),\n                  alpha = 0.8, fill = \"#009FB7\") +\n  labs(x = \"Year\", y = \"Day in Year\")"},{"path":"linear-models-causal-inference.html","id":"homework-1","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.3 Homework","text":"1. Construct linear regression weight predicted height, using adults (age 18 greater) Howell1 dataset. heights listed recorded !Kung census, weights recorded individuals. Provide predicted weights 89% compatibility intervals individuals. , fill table , using model-based predictions.similar question 4H1 . whereas problem knew weight wanted predict height, know height want predict weight. ’ll start estimating linear model.Using model, can use tidybayes::add_predicted_draws() get model based predictions fill table.2. Howell1 dataset, consider people younger 13 years old. Estimate causal association age weight. Assume age influences weight two paths. First, age influences height, height influences weight. Second, age directly influences weight age-related changes muscle growth body proportions. implies causal model (DAG):Use linear regression estimate total (just direct) causal effect year growth weight. sure carefully consider priors. Try using prior predictive simulation assess imply.example, \\(H\\) pipe. Including \\(H\\) model close pipe, removing indirect effect age weight. Therefore, include age predictor weight model.prior distributions, can assume positive relationship age weight, ’ll use lognormal prior slope. intercept represents weight age 0 (.e., birth weight). typically around 3-4 kg, ’ll use normal prior mean 4 standard deviation 1. results wide range plausible regression lines:priors, can now estimate model.Visualizing posterior, see 89% compatibility interval causal effect age weight (b_age) 1.25 1.42 kg/year.3. Now suppose causal association age weight might different boys girls. Use single linear regression, categorical variable sex, estimate total causal effect age weight separately boys girls. girls boys differ? Provide one posterior contrasts summary.can create separate regression lines sex sex (factor) model formula. ’re now estimating intercept sex, use ~ 0 code indicate estimating global intercept, treating sex variable index variable. also use non-linear syntax {brms}. details approach, see Solomon Kurz’s section indicator variables.comparing two intercepts, see posterior distribution girls (b_a_sex1) slightly lower average distribution boys (b_a_sex2); however, lot overlap distributions. Similarly, posterior distributions slopes also show boys (b_b_sex2) slightly higher slope average girls (b_b_sex1).Let visualize regression lines look like. Overall, boys slightly higher intercept appear increase slightly higher rate. , distributions pretty similar overall.However, interested mean difference boys girls, difference means. estimate contrast need calculate posterior simulations sex. represents distribution expected weights individuals sex. can take difference two distributions. posterior distribution difference, shown figure . Overall, difference relatively small, appear increase age.4 - OPTIONAL CHALLENGE. data data(Oxboys) (rethinking package) growth records 26 boys measured 9 periods. want model growth. Specifically, model increments growth one period (Occasion data table) next. increment simply difference height one occasion height previous occasion. Since none boys shrunk study, growth increments greater zero. Estimate posterior distribution increments. Constrain distribution always positive—possible model think boys can shrink year year. Finally compute posterior distribution total growth 9 occasions.Let’s start looking data.first thing need convert individual height measures increments height. straightforward using dplyr::lag().Now want model increments always positive. easy setting prior, increments outcome, parameter model. can {brms} using lognormal family instead gaussian family used point. tell {brms} outcome normally distributed (.e., gaussian), rather lognormally distributed (.e., constrained positive). Specifically, model defined :\\[\\begin{align}\n  y_i &\\sim \\text{Lognormal}(\\alpha,\\sigma) \\\\\n  \\alpha &\\sim \\text{Normal}(0,0.3) \\\\\n  \\sigma &\\sim \\text{Exponential}(4)\n\\end{align}\\]priors? lot trial error prior predictive simulations. lognormal distribution super intuitive , played different priors found looked reasonable . code prior predictive simulation. prior expects growth increments around 1cm. 89% interval .41cm 1.7cm per occasion. seems reasonable, ’ll move forward.estimate model {brms} specifying family = lognormal.Using model, can examine posterior distribution incremental growth. average, expect boys grow 1.5cm per occasion.cumulative growth across occasions, need simulation 8 incremental changes sum . 9 occasions, expect boys grow 13cm, 89% highest density compatibility interval 8.6cm 18.5cm.","code":"\ndata(Howell1)\nhow_dat <- Howell1 %>%\n  filter(age >= 18) %>%\n  mutate(height_c = height - mean(height))\n\nw2h1 <- brm(weight ~ 1 + height_c, data = how_dat, family = gaussian,\n            prior = c(prior(normal(178, 20), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw2\", \"w2h1\"))\ntibble(individual = 1:3,\n       height = c(140, 160, 175)) %>%\n  mutate(height_c = height - mean(how_dat$height)) %>%\n  add_predicted_draws(w2h1) %>%\n  mean_qi(.prediction, .width = 0.89) %>%\n  mutate(range = glue(\"[{sprintf('%0.1f', .lower)}--\",\n                      \"{sprintf('%0.1f', .upper)}]\"),\n         .prediction = sprintf(\"%0.1f\", .prediction)) %>%\n  select(individual, height, exp = .prediction, range) %>%\n  kbl(align = \"c\", booktabs = TRUE,\n      col.names = c(\"Individual\", \"height\", \"expected weight\", \"89% interval\"))\nset.seed(123)\nn <- 50\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 4, 1),\n       beta = rlnorm(n, 0, 1)) %>%\n  expand(nesting(group, alpha, beta), age = 0:12) %>%\n  mutate(weight = alpha + beta * age) %>%\n  ggplot(aes(x = age, y = weight, group = group)) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 12, 2)) +\n  coord_cartesian(xlim = c(0, 12), ylim = c(0, 30)) +\n  labs(x = \"Age\", y = \"Weight\")\ndata(Howell1)\nkid_dat <- Howell1 %>%\n  filter(age < 13)\n\nw2h2 <- brm(weight ~ 1 + age, data = kid_dat, family = gaussian,\n            prior = c(prior(normal(4, 1), class = Intercept),\n                      prior(normal(0, 1), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw2\", \"w2h2\"))\ndraws <- gather_draws(w2h2, b_Intercept, b_age, sigma)\n\nmean_qi(draws, .width = 0.89)\n#> # A tibble: 3 × 7\n#>   .variable   .value .lower .upper .width .point .interval\n#>   <chr>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 b_age         1.34   1.25   1.42   0.89 mean   qi       \n#> 2 b_Intercept   7.00   6.42   7.59   0.89 mean   qi       \n#> 3 sigma         2.58   2.34   2.85   0.89 mean   qi\n\nggplot(draws, aes(x = .value, y = .variable)) +\n  stat_halfeye(.width = 0.89) +\n  labs(x = \"Parameter value\", y = \"Parameter\")\nkid_dat <- kid_dat %>% \n  mutate(sex = male + 1,\n         sex = factor(sex))\n\nw2h3 <- brm(\n  bf(weight ~ 0 + a + b * age,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = kid_dat, family = gaussian,\n  prior = c(prior(normal(4, 1), class = b, coef = sex1, nlpar = a),\n            prior(normal(4, 1), class = b, coef = sex2, nlpar = a),\n            prior(normal(0, 1), class = b, coef = sex1, nlpar = b),\n            prior(normal(0, 1), class = b, coef = sex2, nlpar = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n  file = here(\"fits\", \"hw2\", \"w2h3\")\n)\ndraws <- gather_draws(w2h3, b_a_sex1, b_a_sex2, b_b_sex1, b_b_sex2, sigma)\n\nmean_qi(draws, .width = 0.89)\n#> # A tibble: 5 × 7\n#>   .variable .value .lower .upper .width .point .interval\n#>   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 b_a_sex1    6.50   5.79   7.21   0.89 mean   qi       \n#> 2 b_a_sex2    7.11   6.34   7.86   0.89 mean   qi       \n#> 3 b_b_sex1    1.36   1.25   1.47   0.89 mean   qi       \n#> 4 b_b_sex2    1.48   1.37   1.60   0.89 mean   qi       \n#> 5 sigma       2.49   2.26   2.74   0.89 mean   qi\n\nggplot(draws, aes(x = .value, y = .variable)) +\n  stat_halfeye(.width = 0.89) +\n  labs(x = \"Parameter value\", y = \"Parameter\")\nnew_age <- expand_grid(age = 0:12, sex = factor(c(1, 2)))\n\nall_lines <- new_age %>% \n  add_epred_draws(w2h3) %>% \n  ungroup() %>% \n  mutate(group = paste0(sex, \"_\", .draw))\n\nplot_lines <- all_lines %>%\n  filter(.draw %in% sample(unique(.data$.draw), size = 1000)) %>% \n  select(-.draw)\n\nanimate_lines <- all_lines %>%\n  filter(.draw %in% sample(unique(.data$.draw), size = 50))\n\nggplot(animate_lines, aes(x = age, y = .epred, color = sex, group = group)) +\n  geom_line(data = plot_lines, alpha = 0.01, show.legend = FALSE) + \n  geom_point(data = kid_dat, aes(x = age, y = weight, color = sex),\n             inherit.aes = FALSE) +\n  geom_line(alpha = 1, show.legend = FALSE, color = \"black\") +\n  scale_color_okabeito(labels = c(\"Girls\", \"Boys\")) +\n  scale_x_continuous(breaks = seq(0, 12, 2)) +\n  labs(x = \"Age\", y = \"Weight (kg)\", color = NULL) +\n  guides(color = guide_legend(override.aes = list(size = 3))) +\n  theme(legend.position = \"bottom\") +\n  transition_states(.draw, 0, 1)\ncontrast <- new_age %>% \n  add_predicted_draws(w2h3) %>% \n  ungroup() %>% \n  select(age, sex, .draw, .prediction) %>% \n  mutate(sex = fct_recode(sex,\n                          \"Girls\" = \"1\",\n                          \"Boys\" = \"2\")) %>% \n  pivot_wider(names_from = sex, values_from = .prediction) %>% \n  mutate(diff = Boys - Girls)\n\nggplot(contrast, aes(x = age, y = diff)) +\n  stat_lineribbon(aes(fill_ramp = stat(.width)), .width = ppoints(50),\n                  fill = \"#009FB7\", show.legend = FALSE) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_ramp_continuous(from = \"transparent\", range = c(1, 0)) +\n  scale_x_continuous(breaks = seq(0, 12, 2)) +\n  labs(x = \"Age\", y = \"Weight difference (kg; Boys-Girls)\")\ndata(Oxboys)\nhead(Oxboys)\n#>   Subject     age height Occasion\n#> 1       1 -1.0000    140        1\n#> 2       1 -0.7479    143        2\n#> 3       1 -0.4630    145        3\n#> 4       1 -0.1643    147        4\n#> 5       1 -0.0027    148        5\n#> 6       1  0.2466    150        6\nincrements <- Oxboys %>% \n  group_by(Subject) %>% \n  mutate(previous_height = lag(height),\n         change = height - previous_height) %>% \n  filter(!is.na(change)) %>% \n  ungroup()\n\nincrements\n#> # A tibble: 208 × 6\n#>    Subject     age height Occasion previous_height change\n#>      <int>   <dbl>  <dbl>    <int>           <dbl>  <dbl>\n#>  1       1 -0.748    143.        2            140.  2.90 \n#>  2       1 -0.463    145.        3            143.  1.40 \n#>  3       1 -0.164    147.        4            145.  2.30 \n#>  4       1 -0.0027   148.        5            147.  0.600\n#>  5       1  0.247    150.        6            148.  2.5  \n#>  6       1  0.556    152.        7            150.  1.5  \n#>  7       1  0.778    153.        8            152.  1.60 \n#>  8       1  0.994    156.        9            153.  2.5  \n#>  9       2 -0.748    139.        2            137.  2.20 \n#> 10       2 -0.463    140.        3            139.  1    \n#> # … with 198 more rows\nset.seed(123)\n\nn <- 1000\ntibble(alpha = rnorm(n, mean = 0, sd = 0.3),\n       sigma = rexp(n, rate = 4)) %>% \n  mutate(sim_change = rlnorm(n, meanlog = alpha, sdlog = sigma)) %>% \n  ggplot(aes(x = sim_change)) +\n  geom_density() +\n  labs(x = \"Prior Expectation for Incremental Growth (cm)\", y = \"Density\")\nw2h4 <- brm(change ~ 1, data = increments, family = lognormal,\n            prior = c(prior(normal(0, 0.3), class = Intercept),\n                      prior(exponential(4), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw2\", \"w2h4\"))\nset.seed(234)\n\nas_draws_df(w2h4) %>% \n  mutate(post_change = rlnorm(n(), meanlog = b_Intercept, sdlog = sigma)) %>% \n  ggplot(aes(x = post_change)) +\n  geom_density() +\n  labs(x = \"Posterior Distribution for Incremental Growth (cm)\",\n       y = \"Density\")\nset.seed(345)\n\nas_draws_df(w2h4) %>% \n  mutate(all_change = map2_dbl(b_Intercept, sigma, ~sum(rlnorm(8, .x, .y)))) %>% \n  ggplot(aes(x = all_change)) +\n  stat_halfeye(aes(fill = stat(between(x, 8.6, 18.5))), color = NA) +\n  scale_fill_manual(values = c(\"#009FB7\", NA),\n                    labels = \"89% Compatibility Interval\",\n                    breaks = \"TRUE\", na.value = \"#F0F0F0\") +\n  labs(x = \"Posterior Distribution for Cumulative Growth (cm)\",\n       y = \"Density\", fill = NULL) +\n  theme(legend.position = \"bottom\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
