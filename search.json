[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"project companion Richard McElreath’s Statistical Rethinking (2nd Edition) (McElreath, 2020), 2022 version accompanying course. 10 weeks, course (materials provided ), work exercises chapter covered week assigned homework problems. Like Solomon Kurz, take {tidyverse} (Wickham et al., 2019; Wickham, 2021) {brms} (Bürkner, 2017, 2018, 2021) approach solving problems. translation actual book text {tidyverse} {brms} style code, please check project, Statistical rethinking brms, ggplot2, tidyverse: Second edition.can purchase Statistical Rethinking: Bayesian Course R Stan (McElreath, 2020) CRC Press.","code":""},{"path":"index.html","id":"disclaimer","chapter":"Welcome","heading":"Disclaimer","text":"project work progress. ’d like follow along, can find GitHub repository . solutions checked anybody, undoubtedly errors. find , please contribute let know!several ways contribute. simple edits suggestions, can use “Edit page” link sidebar right screen. can also create fork repository submit pull request open issue.Please note project released Contributor Code Conduct. participating project agree abide terms.","code":""},{"path":"index.html","id":"license","chapter":"Welcome","heading":"License","text":"project licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.","code":""},{"path":"index.html","id":"colophon","chapter":"Welcome","heading":"Colophon","text":"project built :","code":"\ndesc <- desc::description$new()\ndeps <- desc$get_deps() %>% \n  filter(package != \"R\") %>% \n  pull(package)\n\nsessioninfo::session_info(deps)\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value\n#>  version  R version 4.1.2 (2021-11-01)\n#>  os       Ubuntu 20.04.3 LTS\n#>  system   x86_64, linux-gnu\n#>  ui       X11\n#>  language (EN)\n#>  collate  C.UTF-8\n#>  ctype    C.UTF-8\n#>  tz       UTC\n#>  date     2022-01-30\n#>  pandoc   2.14.2 @ /usr/bin/ (via rmarkdown)\n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package        * version    date (UTC) lib source\n#>  abind            1.4-5      2016-07-21 [1] CRAN (R 4.1.2)\n#>  arrayhelpers     1.1-0      2020-02-04 [1] CRAN (R 4.1.2)\n#>  askpass          1.1        2019-01-13 [1] CRAN (R 4.1.2)\n#>  assertthat       0.2.1      2019-03-21 [1] CRAN (R 4.1.2)\n#>  backports        1.4.1      2021-12-13 [1] CRAN (R 4.1.2)\n#>  base64enc        0.1-3      2015-07-28 [1] CRAN (R 4.1.2)\n#>  bayesplot        1.8.1      2021-06-14 [1] CRAN (R 4.1.2)\n#>  BH               1.78.0-0   2021-12-15 [1] CRAN (R 4.1.2)\n#>  bit              4.0.4      2020-08-04 [1] CRAN (R 4.1.2)\n#>  bit64            4.0.5      2020-08-30 [1] CRAN (R 4.1.2)\n#>  bitops           1.0-7      2021-04-24 [1] CRAN (R 4.1.2)\n#>  blob             1.2.2      2021-07-23 [1] CRAN (R 4.1.2)\n#>  bookdown         0.24       2021-09-02 [1] CRAN (R 4.1.2)\n#>  boot             1.3-28     2021-05-03 [2] CRAN (R 4.1.2)\n#>  bridgesampling   1.1-2      2021-04-16 [1] CRAN (R 4.1.2)\n#>  brio             1.1.3      2021-11-30 [1] CRAN (R 4.1.2)\n#>  brms           * 2.16.3     2021-11-22 [1] CRAN (R 4.1.2)\n#>  Brobdingnag      1.2-6      2018-08-13 [1] CRAN (R 4.1.2)\n#>  broom            0.7.12     2022-01-28 [1] CRAN (R 4.1.2)\n#>  bslib            0.3.1      2021-10-06 [1] CRAN (R 4.1.2)\n#>  cachem           1.0.6      2021-08-19 [1] CRAN (R 4.1.2)\n#>  callr            3.7.0      2021-04-20 [1] CRAN (R 4.1.2)\n#>  cellranger       1.1.0      2016-07-27 [1] CRAN (R 4.1.2)\n#>  checkmate        2.0.0      2020-02-06 [1] CRAN (R 4.1.2)\n#>  class            7.3-19     2021-05-03 [2] CRAN (R 4.1.2)\n#>  classInt         0.4-3      2020-04-07 [1] CRAN (R 4.1.2)\n#>  cli              3.1.1      2022-01-20 [1] CRAN (R 4.1.2)\n#>  clipr            0.7.1      2020-10-08 [1] CRAN (R 4.1.2)\n#>  cmdstanr       * 0.4.0.9001 2022-01-30 [1] Github (stan-dev/cmdstanr@a2a97d9)\n#>  coda             0.19-4     2020-09-30 [1] CRAN (R 4.1.2)\n#>  codetools        0.2-18     2020-11-04 [2] CRAN (R 4.1.2)\n#>  colorspace       2.0-2      2021-06-24 [1] CRAN (R 4.1.2)\n#>  colourpicker     1.1.1      2021-10-04 [1] CRAN (R 4.1.2)\n#>  commonmark       1.7        2018-12-01 [1] CRAN (R 4.1.2)\n#>  cpp11            0.4.2      2021-11-30 [1] CRAN (R 4.1.2)\n#>  crayon           1.4.2      2021-10-29 [1] CRAN (R 4.1.2)\n#>  crosstalk        1.2.0      2021-11-04 [1] CRAN (R 4.1.2)\n#>  curl             4.3.2      2021-06-23 [1] CRAN (R 4.1.2)\n#>  dagitty        * 0.3-1      2021-01-21 [1] CRAN (R 4.1.2)\n#>  data.table       1.14.2     2021-09-27 [1] CRAN (R 4.1.2)\n#>  DBI              1.1.2      2021-12-20 [1] CRAN (R 4.1.2)\n#>  dbplyr           2.1.1      2021-04-06 [1] CRAN (R 4.1.2)\n#>  desc             1.4.0      2021-09-28 [1] CRAN (R 4.1.2)\n#>  digest           0.6.29     2021-12-01 [1] CRAN (R 4.1.2)\n#>  distributional   0.3.0      2022-01-05 [1] CRAN (R 4.1.2)\n#>  downlit          0.4.0      2021-10-29 [1] CRAN (R 4.1.2)\n#>  dplyr          * 1.0.7      2021-06-18 [1] CRAN (R 4.1.2)\n#>  DT               0.20       2021-11-15 [1] CRAN (R 4.1.2)\n#>  dtplyr           1.2.1      2022-01-19 [1] CRAN (R 4.1.2)\n#>  dygraphs         1.1.1.6    2018-07-11 [1] CRAN (R 4.1.2)\n#>  e1071            1.7-9      2021-09-16 [1] CRAN (R 4.1.2)\n#>  ellipsis         0.3.2      2021-04-29 [1] CRAN (R 4.1.2)\n#>  emo              0.0.0.9000 2022-01-06 [1] Github (hadley/emo@3f03b11)\n#>  evaluate         0.14       2019-05-28 [1] CRAN (R 4.1.2)\n#>  extrafont        0.17       2014-12-08 [1] CRAN (R 4.1.2)\n#>  extrafontdb      1.0        2012-06-11 [1] CRAN (R 4.1.2)\n#>  fansi            1.0.2      2022-01-14 [1] CRAN (R 4.1.2)\n#>  farver           2.1.0      2021-02-28 [1] CRAN (R 4.1.2)\n#>  fastmap          1.1.0      2021-01-25 [1] CRAN (R 4.1.2)\n#>  fontawesome      0.2.2      2021-07-02 [1] CRAN (R 4.1.2)\n#>  forcats        * 0.5.1      2021-01-27 [1] CRAN (R 4.1.2)\n#>  fs               1.5.2      2021-12-08 [1] CRAN (R 4.1.2)\n#>  future           1.23.0     2021-10-31 [1] CRAN (R 4.1.2)\n#>  gargle           1.2.0      2021-07-02 [1] CRAN (R 4.1.2)\n#>  gdtools          0.2.3      2021-01-06 [1] CRAN (R 4.1.2)\n#>  generics         0.1.1      2021-10-25 [1] CRAN (R 4.1.2)\n#>  geomtextpath   * 0.1.0.9000 2022-01-30 [1] Github (AllanCameron/geomtextpath@0612ba8)\n#>  gganimate        1.0.7      2020-10-15 [1] CRAN (R 4.1.2)\n#>  ggdag            0.2.4      2021-10-10 [1] CRAN (R 4.1.2)\n#>  ggdist         * 3.0.1      2021-11-30 [1] CRAN (R 4.1.2)\n#>  ggforce          0.3.3      2021-03-05 [1] CRAN (R 4.1.2)\n#>  gghighlight      0.3.2      2021-06-05 [1] CRAN (R 4.1.2)\n#>  ggplot2        * 3.3.5      2021-06-25 [1] CRAN (R 4.1.2)\n#>  ggraph           2.0.5      2021-02-23 [1] CRAN (R 4.1.2)\n#>  ggrepel          0.9.1      2021-01-15 [1] CRAN (R 4.1.2)\n#>  ggridges         0.5.3      2021-01-08 [1] CRAN (R 4.1.2)\n#>  ggtext           0.1.1      2020-12-17 [1] CRAN (R 4.1.2)\n#>  gifski           1.4.3-1    2021-05-02 [1] CRAN (R 4.1.2)\n#>  globals          0.14.0     2020-11-22 [1] CRAN (R 4.1.2)\n#>  glue           * 1.6.1      2022-01-22 [1] CRAN (R 4.1.2)\n#>  googledrive      2.0.0      2021-07-08 [1] CRAN (R 4.1.2)\n#>  googlesheets4    1.0.0      2021-07-21 [1] CRAN (R 4.1.2)\n#>  graphlayouts     0.8.0      2022-01-03 [1] CRAN (R 4.1.2)\n#>  gridExtra        2.3        2017-09-09 [1] CRAN (R 4.1.2)\n#>  gridtext         0.1.4      2020-12-10 [1] CRAN (R 4.1.2)\n#>  gt               0.3.1      2021-08-07 [1] CRAN (R 4.1.2)\n#>  gtable           0.3.0      2019-03-25 [1] CRAN (R 4.1.2)\n#>  gtools           3.9.2      2021-06-06 [1] CRAN (R 4.1.2)\n#>  haven            2.4.3      2021-08-04 [1] CRAN (R 4.1.2)\n#>  HDInterval       0.2.2      2020-05-23 [1] CRAN (R 4.1.2)\n#>  here           * 1.0.1      2020-12-13 [1] CRAN (R 4.1.2)\n#>  highr            0.9        2021-04-16 [1] CRAN (R 4.1.2)\n#>  hms              1.1.1      2021-09-26 [1] CRAN (R 4.1.2)\n#>  hrbrthemes       0.8.0      2020-03-06 [1] CRAN (R 4.1.2)\n#>  htmltools        0.5.2      2021-08-25 [1] CRAN (R 4.1.2)\n#>  htmlwidgets      1.5.4      2021-09-08 [1] CRAN (R 4.1.2)\n#>  httpuv           1.6.5      2022-01-05 [1] CRAN (R 4.1.2)\n#>  httr             1.4.2      2020-07-20 [1] CRAN (R 4.1.2)\n#>  ids              1.0.1      2017-05-31 [1] CRAN (R 4.1.2)\n#>  igraph           1.2.11     2022-01-04 [1] CRAN (R 4.1.2)\n#>  inline           0.3.19     2021-05-31 [1] CRAN (R 4.1.2)\n#>  isoband          0.2.5      2021-07-13 [1] CRAN (R 4.1.2)\n#>  jpeg             0.1-9      2021-07-24 [1] CRAN (R 4.1.2)\n#>  jquerylib        0.1.4      2021-04-26 [1] CRAN (R 4.1.2)\n#>  jsonlite         1.7.3      2022-01-17 [1] CRAN (R 4.1.2)\n#>  kableExtra       1.3.4      2021-02-20 [1] CRAN (R 4.1.2)\n#>  KernSmooth       2.23-20    2021-05-03 [2] CRAN (R 4.1.2)\n#>  knitr            1.37       2021-12-16 [1] CRAN (R 4.1.2)\n#>  labeling         0.4.2      2020-10-20 [1] CRAN (R 4.1.2)\n#>  later            1.3.0      2021-08-18 [1] CRAN (R 4.1.2)\n#>  lattice          0.20-45    2021-09-22 [2] CRAN (R 4.1.2)\n#>  lazyeval         0.2.2      2019-03-15 [1] CRAN (R 4.1.2)\n#>  lifecycle        1.0.1      2021-09-24 [1] CRAN (R 4.1.2)\n#>  listenv          0.8.0      2019-12-05 [1] CRAN (R 4.1.2)\n#>  loo            * 2.4.1      2020-12-09 [1] CRAN (R 4.1.2)\n#>  lpSolve          5.6.15     2020-01-24 [1] CRAN (R 4.1.2)\n#>  lubridate        1.8.0      2021-10-07 [1] CRAN (R 4.1.2)\n#>  magrittr         2.0.2      2022-01-26 [1] CRAN (R 4.1.2)\n#>  markdown         1.1        2019-08-07 [1] CRAN (R 4.1.2)\n#>  MASS             7.3-54     2021-05-03 [2] CRAN (R 4.1.2)\n#>  Matrix           1.3-4      2021-06-01 [2] CRAN (R 4.1.2)\n#>  matrixStats      0.61.0     2021-09-17 [1] CRAN (R 4.1.2)\n#>  memoise          2.0.1      2021-11-26 [1] CRAN (R 4.1.2)\n#>  mgcv             1.8-38     2021-10-06 [2] CRAN (R 4.1.2)\n#>  mime             0.12       2021-09-28 [1] CRAN (R 4.1.2)\n#>  miniUI           0.1.1.1    2018-05-18 [1] CRAN (R 4.1.2)\n#>  modelr           0.1.8      2020-05-19 [1] CRAN (R 4.1.2)\n#>  munsell          0.5.0      2018-06-12 [1] CRAN (R 4.1.2)\n#>  mvtnorm          1.1-3      2021-10-08 [1] CRAN (R 4.1.2)\n#>  nleqslv          3.3.2      2018-05-17 [1] CRAN (R 4.1.2)\n#>  nlme             3.1-153    2021-09-07 [2] CRAN (R 4.1.2)\n#>  numDeriv         2016.8-1.1 2019-06-06 [1] CRAN (R 4.1.2)\n#>  officedown       0.2.3      2021-11-16 [1] CRAN (R 4.1.2)\n#>  officer          0.4.1      2021-11-14 [1] CRAN (R 4.1.2)\n#>  openssl          1.4.6      2021-12-19 [1] CRAN (R 4.1.2)\n#>  packrat          0.7.0      2021-08-20 [1] CRAN (R 4.1.2)\n#>  parallelly       1.30.0     2021-12-17 [1] CRAN (R 4.1.2)\n#>  pillar           1.6.5      2022-01-25 [1] CRAN (R 4.1.2)\n#>  pkgbuild         1.3.1      2021-12-20 [1] CRAN (R 4.1.2)\n#>  pkgconfig        2.0.3      2019-09-22 [1] CRAN (R 4.1.2)\n#>  plyr             1.8.6      2020-03-03 [1] CRAN (R 4.1.2)\n#>  png              0.1-7      2013-12-03 [1] CRAN (R 4.1.2)\n#>  polyclip         1.10-0     2019-03-14 [1] CRAN (R 4.1.2)\n#>  posterior        1.2.0      2022-01-05 [1] CRAN (R 4.1.2)\n#>  prettyunits      1.1.1      2020-01-24 [1] CRAN (R 4.1.2)\n#>  processx         3.5.2      2021-04-30 [1] CRAN (R 4.1.2)\n#>  progress         1.2.2      2019-05-16 [1] CRAN (R 4.1.2)\n#>  promises         1.2.0.1    2021-02-11 [1] CRAN (R 4.1.2)\n#>  proxy            0.4-26     2021-06-07 [1] CRAN (R 4.1.2)\n#>  ps               1.6.0      2021-02-28 [1] CRAN (R 4.1.2)\n#>  purrr          * 0.3.4      2020-04-17 [1] CRAN (R 4.1.2)\n#>  R6               2.5.1      2021-08-19 [1] CRAN (R 4.1.2)\n#>  ragg             1.2.1      2021-12-06 [1] CRAN (R 4.1.2)\n#>  rappdirs         0.3.3      2021-01-31 [1] CRAN (R 4.1.2)\n#>  ratlas           0.0.0.9000 2022-01-30 [1] Github (atlas-aai/ratlas@ebb795b)\n#>  RColorBrewer     1.1-2      2014-12-07 [1] CRAN (R 4.1.2)\n#>  Rcpp           * 1.0.8      2022-01-13 [1] CRAN (R 4.1.2)\n#>  RcppArmadillo    0.10.8.1.0 2022-01-24 [1] CRAN (R 4.1.2)\n#>  RcppEigen        0.3.3.9.1  2020-12-17 [1] CRAN (R 4.1.2)\n#>  RcppParallel     5.1.5      2022-01-05 [1] CRAN (R 4.1.2)\n#>  RCurl            1.98-1.5   2021-09-17 [1] CRAN (R 4.1.2)\n#>  readr          * 2.1.1      2021-11-30 [1] CRAN (R 4.1.2)\n#>  readxl           1.3.1      2019-03-13 [1] CRAN (R 4.1.2)\n#>  rematch          1.0.1      2016-04-21 [1] CRAN (R 4.1.2)\n#>  rematch2         2.1.2      2020-05-01 [1] CRAN (R 4.1.2)\n#>  reprex           2.0.1      2021-08-05 [1] CRAN (R 4.1.2)\n#>  reshape2         1.4.4      2020-04-09 [1] CRAN (R 4.1.2)\n#>  rethinking     * 2.21       2022-01-06 [1] Github (rmcelreath/rethinking@783d111)\n#>  rJava            1.0-6      2021-12-10 [1] CRAN (R 4.1.2)\n#>  rlang            1.0.0      2022-01-26 [1] CRAN (R 4.1.2)\n#>  rmarkdown        2.11       2021-09-14 [1] CRAN (R 4.1.2)\n#>  rprojroot        2.0.2      2020-11-15 [1] CRAN (R 4.1.2)\n#>  rsconnect        0.8.25     2021-11-19 [1] CRAN (R 4.1.2)\n#>  rstan          * 2.21.3     2021-12-19 [1] CRAN (R 4.1.2)\n#>  rstantools       2.1.1      2020-07-06 [1] CRAN (R 4.1.2)\n#>  rstudioapi       0.13       2020-11-12 [1] CRAN (R 4.1.2)\n#>  Rttf2pt1         1.3.9      2021-07-22 [1] CRAN (R 4.1.2)\n#>  rvest            1.0.2      2021-10-16 [1] CRAN (R 4.1.2)\n#>  rvg              0.2.5      2020-06-30 [1] CRAN (R 4.1.2)\n#>  s2               1.0.7      2021-09-28 [1] CRAN (R 4.1.2)\n#>  sass             0.4.0      2021-05-12 [1] CRAN (R 4.1.2)\n#>  scales           1.1.1      2020-05-11 [1] CRAN (R 4.1.2)\n#>  selectr          0.4-2      2019-11-20 [1] CRAN (R 4.1.2)\n#>  servr            0.24       2021-11-16 [1] CRAN (R 4.1.2)\n#>  sessioninfo      1.2.2      2021-12-06 [1] any (@1.2.2)\n#>  sf               1.0-5      2021-12-17 [1] CRAN (R 4.1.2)\n#>  shape            1.4.6      2021-05-19 [1] CRAN (R 4.1.2)\n#>  shiny            1.7.1      2021-10-02 [1] CRAN (R 4.1.2)\n#>  shinyjs          2.1.0      2021-12-23 [1] CRAN (R 4.1.2)\n#>  shinystan        2.5.0      2018-05-01 [1] CRAN (R 4.1.2)\n#>  shinythemes      1.2.0      2021-01-25 [1] CRAN (R 4.1.2)\n#>  sourcetools      0.1.7      2018-04-25 [1] CRAN (R 4.1.2)\n#>  StanHeaders    * 2.21.0-7   2020-12-17 [1] CRAN (R 4.1.2)\n#>  staplr           3.1.1      2021-01-11 [1] CRAN (R 4.1.2)\n#>  stringi          1.7.6      2021-11-29 [1] CRAN (R 4.1.2)\n#>  stringr        * 1.4.0      2019-02-10 [1] CRAN (R 4.1.2)\n#>  svglite          2.0.0      2021-02-20 [1] CRAN (R 4.1.2)\n#>  svUnit           1.0.6      2021-04-19 [1] CRAN (R 4.1.2)\n#>  sys              3.4        2020-07-23 [1] CRAN (R 4.1.2)\n#>  systemfonts      1.0.3      2021-10-13 [1] CRAN (R 4.1.2)\n#>  tensorA          0.36.2     2020-11-19 [1] CRAN (R 4.1.2)\n#>  textshaping      0.3.6      2021-10-13 [1] CRAN (R 4.1.2)\n#>  threejs          0.3.3      2020-01-21 [1] CRAN (R 4.1.2)\n#>  tibble         * 3.1.6      2021-11-07 [1] CRAN (R 4.1.2)\n#>  tidybayes      * 3.0.2      2022-01-05 [1] CRAN (R 4.1.2)\n#>  tidygraph        1.2.0      2020-05-12 [1] CRAN (R 4.1.2)\n#>  tidyr          * 1.1.4      2021-09-27 [1] CRAN (R 4.1.2)\n#>  tidyselect       1.1.1      2021-04-30 [1] CRAN (R 4.1.2)\n#>  tidyverse      * 1.3.1      2021-04-15 [1] CRAN (R 4.1.2)\n#>  tinytex          0.36       2021-12-19 [1] CRAN (R 4.1.2)\n#>  transformr       0.1.3      2020-07-05 [1] CRAN (R 4.1.2)\n#>  tweenr           1.0.2      2021-03-23 [1] CRAN (R 4.1.2)\n#>  tzdb             0.2.0      2021-10-27 [1] CRAN (R 4.1.2)\n#>  units            0.7-2      2021-06-08 [1] CRAN (R 4.1.2)\n#>  utf8             1.2.2      2021-07-24 [1] CRAN (R 4.1.2)\n#>  uuid             1.0-3      2021-11-01 [1] CRAN (R 4.1.2)\n#>  V8               4.0.0      2021-12-23 [1] CRAN (R 4.1.2)\n#>  vctrs            0.3.8      2021-04-29 [1] CRAN (R 4.1.2)\n#>  viridis          0.6.2      2021-10-13 [1] CRAN (R 4.1.2)\n#>  viridisLite      0.4.0      2021-04-13 [1] CRAN (R 4.1.2)\n#>  vroom            1.5.7      2021-11-30 [1] CRAN (R 4.1.2)\n#>  webshot          0.5.2      2019-11-22 [1] CRAN (R 4.1.2)\n#>  withr            2.4.3      2021-11-30 [1] CRAN (R 4.1.2)\n#>  wjake          * 0.1.0      2022-01-30 [1] Github (wjakethompson/wjake@ce12925)\n#>  wk               0.6.0      2022-01-03 [1] CRAN (R 4.1.2)\n#>  xaringan         0.22       2021-06-23 [1] CRAN (R 4.1.2)\n#>  xfun             0.29       2021-12-14 [1] CRAN (R 4.1.2)\n#>  XML              3.99-0.8   2021-09-17 [1] CRAN (R 4.1.2)\n#>  xml2             1.3.3      2021-11-30 [1] CRAN (R 4.1.2)\n#>  xtable           1.8-4      2019-04-21 [1] CRAN (R 4.1.2)\n#>  xts              0.12.1     2020-09-09 [1] CRAN (R 4.1.2)\n#>  yaml             2.2.2      2022-01-25 [1] CRAN (R 4.1.2)\n#>  zip              2.2.0      2021-05-31 [1] CRAN (R 4.1.2)\n#>  zoo              1.8-9      2021-03-09 [1] CRAN (R 4.1.2)\n#> \n#>  [1] /home/runner/work/_temp/Library\n#>  [2] /opt/R/4.1.2/lib/R/library\n#> \n#> ──────────────────────────────────────────────────────────────────────────────"},{"path":"bayesian-inference.html","id":"bayesian-inference","chapter":"Week 1: Bayesian Inference","heading":"Week 1: Bayesian Inference","text":"first week covers Chapter 1 (Golem Prague), Chapter 2 (Small Worlds Large Worlds), Chapter 3 (Sampling Imaginary).","code":""},{"path":"bayesian-inference.html","id":"lectures","chapter":"Week 1: Bayesian Inference","heading":"1.1 Lectures","text":"Lecture 1:Lecture 2:","code":""},{"path":"bayesian-inference.html","id":"exercises","chapter":"Week 1: Bayesian Inference","heading":"1.2 Exercises","text":"","code":""},{"path":"bayesian-inference.html","id":"chapter-1","chapter":"Week 1: Bayesian Inference","heading":"1.2.1 Chapter 1","text":"exercises Chapter 1.","code":""},{"path":"bayesian-inference.html","id":"chapter-2","chapter":"Week 1: Bayesian Inference","heading":"1.2.2 Chapter 2","text":"2E1. expressions correspond statement: probability rain Monday?\n(1) Pr(rain)\n(2) Pr(rain|Monday)\n(3) Pr(Monday|rain)\n(4) Pr(rain, Monday) / Pr(Monday)take question mean probability rain given Monday. means (2) (4) correct.2E2. following statements corresponds expression: Pr(Monday|rain)?\n(1) probability rain Monday.\n(2) probability rain, given Monday.\n(3) probability Monday, given raining.\n(4) probability Monday raining.answer (3) corresponds expression Pr(Monday|rain).2E3. following expressions correspond statement: probability Monday, given raining?\n(1) Pr(Monday|rain)\n(2) Pr(rain|Monday)\n(3) Pr(rain|Monday) Pr(Monday)\n(4) Pr(rain|Monday) Pr(Monday) / Pr(rain)\n(5) Pr(Monday|rain) Pr(rain) / Pr(Monday)two correct answers. Answer option (1) standard notation conditional probability. Answer option (4) equivalent, Bayes’ Theorem.2E4. Bayesian statistician Bruno de Finetti (1906–1985) began 1973 book probability theory dedication: “PROBABILITY EXIST.” capitals appeared original, imagine de Finetti wanted us shout statement. meant probability device describing uncertainty perspective observer limited knowledge; objective reality. Discuss globe tossing example chapter, light statement. mean say “probability water 0.7”?idea probability subjective perception likelihood something happen. globe tossing example, result always either “land” “water” (.e., 0 1). toss globe, don’t know result , know always “land” “water.” express uncertainty outcome, use probability. know water likely land, may say probability “water” 0.7; however, ’ll never actually observe result 0.7 waters, observe probability. ever observe two results “land” “water.”2M1. Recall globe tossing model chapter. Compute plot grid approximate posterior distribution following sets observations. case, assume uniform prior p.\n(1) W, W, W\n(2) W, W, W, L\n(3) L, W, W, L, W, W, W2M2. Now assume prior p equal zero p < 0.5 positive constant p ≥ 0.5. compute plot grid approximate posterior distribution sets observations problem just .problem can use code , just altering prior defined.2M3. Suppose two globes, one Earth one Mars. Earth globe 70% covered water. Mars globe 100% land. suppose one globes—don’t know —tossed air produced “land” observatiion. Assume globe equally likely tossed. Show posterior probability globe Earth, conditional seeing “land” (Pr(Earth|land)), 0.23.2M4. Suppose deck three cards. card two sides, side either black white. One card two black sides. second card one black one white side. third card two white sides. Now suppose three cards placed bag shuffled. Someone reaches bag pulls card places flat table. black side shown facing , don’t know color side facing . Show probability side also black 2/3. Use counting method (Section 2 chapter) approach problem. means counting ways card produce observed data (black side faceing table).2M5. Now suppose four cards: B/B, B/W, W/W, another B/B. suppose card drawn bag black side appears face . calculate probability side black.2M6. Imagine black ink heavy, cards black sides heavier cards white sides. result, ’s less likely card black sides pulled bag. assume three cards: B/B, B/W, W/W. experimenting number times, conclude every way pull B/B card bag, 2 ways pull B/W card 3 ways pull W/W card. suppose card pulled black side appears face . Show probability side black now 0.5. Use counting method, .2M7. Assume original card problem, single card showing black side face . looking side, draw another card bag lay face table. face shown new card white. Show probability first card, one showing black side, black side now 0.75. Use counting method, can. Hint: Treat like sequence globe tosses, countng ways see observation, possiible first card.2H1. Suppose two species panda bear. equally common wild live sample places. look exactly alike eat food, yet genetic assay capable telling apart. differ however family sizes. Species gives birth twins 10% time, otherwise birthing single infant. Species births twins 20% time, ottherwise birthing singleton infants. Assume numbers known certainty, many years field research.\nNow suppose managing captive panda breeding program. newe female panda unknown species, just given birth twins. probability next birth also twins?2H2. Recall facts problem . Now compute probability panda species , asssuming observed first birth twins.2H3. Continuing previous problem, suppose panda mother second birth twins, singleton infant. Compute posterior probability panda species .2H4. common boast Bayesian statisticians Bayesian inference makes easy use data, even data different types.\nsuppose now veterinarian comes along new genetic test claims can identify species mother panda. test, like tests, imperfect. information test:probability correctly identifies species panda 0.8.probability correctly identifies species B panda 0.65.vet administers test panda tells test positive species . First ignore previous information births compute posterior probability panda species . redo calculation, now using birth data well.","code":"\nlibrary(tidyverse)\n\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20),\n               prior = rep(1, times = 20)) %>%\n  mutate(likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 20)) %>%\n  mutate(prior = case_when(p_grid < 0.5 ~ 0L,\n                           TRUE ~ 1L),\n         likelihood_1 = dbinom(3, size = 3, prob = p_grid),\n         likelihood_2 = dbinom(3, size = 4, prob = p_grid),\n         likelihood_3 = dbinom(5, size = 7, prob = p_grid),\n         across(starts_with(\"likelihood\"), ~ .x * prior),\n         across(starts_with(\"likelihood\"), ~ .x / sum(.x))) %>%\n  pivot_longer(cols = starts_with(\"likelihood\"), names_to = \"pattern\",\n               values_to = \"posterior\") %>%\n  separate(pattern, c(NA, \"pattern\"), sep = \"_\", convert = TRUE) %>%\n  mutate(obs = case_when(pattern == 1L ~ \"W, W, W\",\n                         pattern == 2L ~ \"W, W, W, L\",\n                         pattern == 3L ~ \"L, W, W, L, W, W, W\"))\n\nggplot(dist, aes(x = p_grid, y = posterior)) +\n  facet_wrap(vars(fct_inorder(obs)), nrow = 1) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n# probability of land, given Earth\np_le <- 0.3\n\n# probability of land, given Mars\np_lm <- 1.0\n\n# probability of Earth\np_e <- 0.5\n\n# probability of land\np_l <- (p_e * p_le) + ((1 - p_e) * p_lm)\n\n# probability of Earth, given land (using Bayes' Theorem)\np_el <- (p_le * p_e) / p_l\np_el\n#> [1] 0.231\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood)\nprior <- c(1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.667\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood,\n                card_bb_likelihood)\nprior <- c(1, 1, 1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1] + posterior[4]\n#> [1] 0.8\ncard_bb_likelihood <- 2\ncard_bw_likelihood <- 1\ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_bw_likelihood, card_ww_likelihood)\nprior <- c(1, 2, 3)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.5\n# 2 choices for first card, with 3 options for second card: 2 W/W + 1 W/B\ncard_bb_likelihood <- 2 * 3 \ncard_wb_likelihood <- 1 * 2 \ncard_ww_likelihood <- 0\n\nlikelihood <- c(card_bb_likelihood, card_wb_likelihood, card_ww_likelihood)\nprior <- c(1,1,1)\nposterior <- prior * likelihood\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.75\n# After first birth, likelihood of species A and B is equal to the rate the\n# species give birth to twins\na_likelihood <- 0.1\nb_likelihood <- 0.2\n\n# Next calculate the posterior probability that the panda belongs to each\n# species, assume species are equally likely\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(1, 1) \nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\nposterior\n#> [1] 0.333 0.667\n\n# The probability the next birth is twins is the probability the panda belongs\n# to each species times the likelihood each species gives birth to twins\n(posterior[1] * a_likelihood) + (posterior[2] * b_likelihood)\n#> [1] 0.167\n# probability of species A\np_a <- 0.5\n\n# probability of twins, given species A\np_ta <- 0.1\n\n# probability of twins, given species B\np_tb <- 0.2\n\n# probability of twins\np_t <- (p_a * p_ta) + ((1 - p_a) * p_tb)\n\n# probability of species A, given twins (using Bayes' Theorem)\n# (note this is equivalent to `posterior[1]` above)\np_at <- (p_ta * p_a) / p_t\np_at\n#> [1] 0.333\n# likelihood for each species is Pr(twins) * Pr(singleton)\na_likelihood <- 0.1 * (1 - 0.1)\nb_likelihood <- 0.2 * (1 - 0.2)\n\n# compute posterior probabilities\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(1, 1)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.36\n# use Bayes' Theorem to determine the probability of species A, given a positive\n# test\np_ap <- (0.8 * 0.5) / ((0.5 * 0.8) + (0.5 * 0.35))\np_ap\n#> [1] 0.696\n\n\n# Now include test data with observed births\n# likelihood for each species is Pr(twins) * Pr(singleton)\na_likelihood <- 0.1 * (1 - 0.1)\nb_likelihood <- 0.2 * (1 - 0.2)\n\n# compute posterior probabilities, using test result as prior\nlikelihood <- c(a_likelihood, b_likelihood)\nprior <- c(p_ap, (1 - p_ap))\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nposterior[1]\n#> [1] 0.563"},{"path":"bayesian-inference.html","id":"chapter-3","chapter":"Week 1: Bayesian Inference","heading":"1.2.3 Chapter 3","text":"Easy. Easy problems use sample posterior distribution globe tossing example. code give specific set samples, can check answers exactly.Use values samples answer questions follow.3E1. much posterior probability lies p = 0.2?3E2. much posterior probability lies p = 0.8?3E3. much posterior probability lies p = 0.2 p = 0.8?3E4. 20% posterior probability lies value p?3E5. 20% posterior probability lies value p?3E6. values p contain narrowest interval equal 66% posterior probability?3E7. values p contain 66% posterior probability, assuming equal posterior probability interval?3M1. Suppose globe tossing data turned 8 water 15 tosses. Constructe posterior distribution, using grid approximation. Use flat prior .3M2. Draw 10,000 samples grid approximation . use sample calculate 90% HPDI p.3M3. Construct posterior predictive check model data. means simulate distribution samples, averaging posterior uncertainty p. probability observing 8 water 15 tosses?3M4. Using posterior distribution constructed new (8/15) data, now calculate probability observing 6 water 9 tosses.3M5. Start 3M1, now use prior zero p = 0.5 constant p = 0.5. corresponds prior information majority Earth’s surface water. Repeat problem compare inferences (using priors) true value p = 0.7.HPDI 3M2 much narrower new prior ([.501, .711] vs. [.334, 0.722]). Additionally, probabilities observing 8 15 6 9 increased, value p < 0.5 longer taking posterior density. Thus model new prior giving us better information.3M6. Suppose want estimate Earth’s proportion water precisely. Specifically, want 99% percentile interval posterior distribution p 0.05 wide. means distance upper lower bound interval 0.05. many times toss globe ?figure shows average interval width 100 simulations given number tosses. , true proportion p 0.7 toss globe 1,000 times, average interval width approximately 0.074. get interval width .05 smaller, need around 2,300 tosses.Hard. Hard problems use data . data indicate gender (male = 1, female = 0) officially reported first second born children 100 two-child families. example, first family data reported boy (1) girl (0). second family reported girl (0) boy (1). third family reported two girls. can load tow vectors R’s memory typing:Use vectors data. example compute total number boys born across births, use:3H1. Using grid approximation, compute posterior distribution probability birth boy. Assume uniform prior probability. parameter value maximizes posterior probability?3H2. Using sample function, draw 10,000 random parameter values posterior distribution calculated . Use sample estimate 50%, 89%, 97% highest posterior density intervals.3H3. Use rbinom simulate 10,000 replicates 200 births. end 10,000 numbers, one count boys 200 births. Compare distribution predicted numbers boys actual count data (111 boys 200 births). many good ways visualize simulations, dens command (part rethinking package) prbably easiest way case. look like model fits data well? , distribution predictions include actual observation central, likely outcome?Based posterior predictive distribution, model appears fit well, observed value 111 middle distribution.3H4. Now compare 10,000 counts boys 100 simulated first borns number boys first births, birth1. model look light?Based first births , model appears fit less well. Specifically, model appears overestimating number first births boys. However, appear large discrepancy, observed value still within middle 66% interval.3H5. model assumes sex first second births independent. check assumption, focus now second births followed female first borns. Compare 10,000 simulated conts boys second births followed girls. correctly, need count number first borns girls simulate many births, 10,000 times. Compare counts boys simulations actual observed count boys following girls. model look light? guesses going data?model severely estimating number second-born boys first born child girl. Thus, assumption births independent appears violated.","code":"\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\nlikelihood <- dbinom(6, size = 9, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\nset.seed(100)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nmean(samples < 0.2)\n#> [1] 4e-04\nmean(samples > 0.8)\n#> [1] 0.112\nlibrary(tidyverse)\nmean(between(samples, 0.2, 0.8))\n#> [1] 0.888\nquantile(samples, probs = 0.2)\n#>   20% \n#> 0.519\nquantile(samples, probs = 0.8)\n#>   80% \n#> 0.756\nlibrary(rethinking)\nHPDI(samples, prob = 0.66)\n#> |0.66 0.66| \n#> 0.509 0.774\nPI(samples, prob = 0.66)\n#>   17%   83% \n#> 0.503 0.770\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %>%\n  ggplot(aes(x = p, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\nset.seed(101)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\n\nHPDI(samples, prob = 0.9)\n#>  |0.9  0.9| \n#> 0.334 0.722\nw <- rbinom(1e4, size = 15, prob = samples)\nmean(w == 8)\n#> [1] 0.15\nw <- rbinom(1e4, size = 9, prob = samples)\nmean(w == 6)\n#> [1] 0.171\n# 3M5.1\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- case_when(p_grid < 0.5 ~ 0L,\n                   TRUE ~ 1L)\nlikelihood <- dbinom(8, size = 15, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %>%\n  ggplot(aes(x = p, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\n# 3M5.2\nset.seed(101)\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nHPDI(samples, prob = 0.9)\n#>  |0.9  0.9| \n#> 0.501 0.711\n\n# 3M5.3\nw <- rbinom(1e4, size = 15, prob = samples)\nmean(w == 8)\n#> [1] 0.161\n\n# 3M5.4\nw <- rbinom(1e4, size = 9, prob = samples)\nmean(w == 6)\n#> [1] 0.236\nsingle_sim <- function(tosses, prior_type = c(\"uniform\", \"step\")) {\n  prior_type <- match.arg(prior_type)\n  obs <- rbinom(1, size = tosses, prob = 0.7)\n  \n  p_grid <- seq(from = 0, to = 1, length.out = 1000)\n  prior <- rep(1, 1000)\n  if (prior_type == \"step\") prior[1:500] <- 0\n  \n  likelihood <- dbinom(obs, size = tosses, prob = p_grid)\n  posterior <- likelihood * prior\n  posterior <- posterior / sum(posterior)\n  \n  samples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\n  interval <- PI(samples, prob = 0.99)\n  width <- interval[2] - interval[1]\n}\nsingle_cond <- function(tosses, prior_type, reps = 100) {\n  tibble(tosses = tosses,\n         prior_type = prior_type,\n         width = map_dbl(seq_len(reps), ~single_sim(tosses = tosses,\n                                                    prior_type = prior_type)))\n}\n\nsimulation <- crossing(tosses = seq(1000, 5000, by = 100),\n                       prior_type = c(\"uniform\", \"step\")) %>%\n  pmap_dfr(single_cond, reps = 100) %>%\n  group_by(tosses, prior_type) %>%\n  summarize(avg_width = mean(width), .groups = \"drop\") %>%\n  mutate(prior_type = case_when(prior_type == \"uniform\" ~ \"Uniform Prior\",\n                                prior_type == \"step\" ~ \"Step Prior\"),\n         prior_type = factor(prior_type, levels = c(\"Uniform Prior\",\n                                                    \"Step Prior\")))\n\nggplot(simulation, aes(x = tosses, y = avg_width)) +\n  facet_wrap(~prior_type, nrow = 1) +\n  geom_point() +\n  geom_line() +\n  scale_x_comma() +\n  labs(x = \"Tosses\", y = \"Average Interval Width\") +\n  theme(panel.spacing.x = unit(2, \"lines\"))\ndata(homeworkch3)\nbirth1\n#>   [1] 1 0 0 0 1 1 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1\n#>  [38] 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1\n#>  [75] 1 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 1 1 1 1\nbirth2\n#>   [1] 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 0 0\n#>  [38] 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 0 0 0\n#>  [75] 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 1 1 0 0 0 0\nsum(birth1) + sum(birth2)\n#> [1] 111\np_grid <- seq(from = 0, to = 1, length.out = 1000)\nprior <- rep(1, 1000)\n\nboys <- sum(birth1) + sum(birth2)\ntotal <- length(birth1) + length(birth2)\nlikelihood <- dbinom(boys, size = total, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n\ntibble(p = p_grid, posterior = posterior) %>%\n  ggplot(aes(x = p, y = posterior)) +\n  geom_point() +\n  geom_line() +\n  labs(x = \"Proportion Boys (p)\", y = \"Posterior Density\")\n\np_grid[which.max(posterior)]\n#> [1] 0.555\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\nHPDI(samples, prob = c(0.50, 0.89, 0.97))\n#> |0.97 |0.89  |0.5  0.5| 0.89| 0.97| \n#> 0.474 0.496 0.531 0.578 0.609 0.628\nlibrary(tidybayes)\nbreak_func <- function(x) {\n  length(seq(min(x), max(x), by = 1)) + 1\n}\n\nset.seed(300)\nb <- rbinom(1e4, size = 200, prob = samples)\n\nggplot() +\n  stat_histinterval(aes(x = b), .width = c(0.66, 0.89), breaks = break_func) +\n  geom_vline(aes(xintercept = boys), linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Number of Boys\", y = \"Density\")\nset.seed(301)\nb <- rbinom(1e4, size = 100, prob = samples)\n\n\nggplot() +\n  stat_histinterval(aes(x = b), .width = c(0.66, 0.89), breaks = break_func) +\n  geom_vline(aes(xintercept = sum(birth1)), linetype = \"dashed\",\n             color = \"red\") +\n  labs(x = \"Number of Boys\", y = \"Density\")\nfb_girls <- length(birth1) - sum(birth1)\n\nset.seed(302)\nb <- rbinom(1e4, size = fb_girls, prob = samples)\n\nobs_bfg <- sum(birth2[which(birth1 == 0)])\n\nggplot() +\n  stat_histinterval(aes(x = b), .width = c(0.66, 0.89), breaks = break_func) +\n  geom_vline(aes(xintercept = obs_bfg), linetype = \"dashed\",\n             color = \"red\") +\n  labs(x = \"Number of Boys\", y = \"Density\")"},{"path":"bayesian-inference.html","id":"homework","chapter":"Week 1: Bayesian Inference","heading":"1.3 Homework","text":"1. Suppose globe tossing data (Chapter 2) turned 4 water 11 land. Construct posterior distribution, using grid approximation. Use flat prior book.First, create grid create prior, constant values grid.Next, calculate likelihood 4 water 11 land value grid normalize values sum 1.Finally, can sample visualize posterior distribution. majority posterior 0.5, mean 0.30. surprising, given data indicates (.e., land observed water).2. Now suppose data 4 water 2 land. Compute posterior , time use prior zero p = 0.5 constant p = 0.5. corresponds prior information majority Earth’s surface water.follow process, now define prior 0 p < .5 1 p ≥ .5.now see posterior distribution truncated .5. mean around 0.69, expected given 4 6 trials (two-thirds) water.3. posterior distribution 2, compute 89% percentile HPDI intervals. Compare widths intervals. wider? ? information interval, might misunderstand shape posterior distribution?First, take 10,000 samples posterior calculated previous question. summarize PI() HPDI().89% percentile interval [0.525, 0.882], highest posterior density interval [0.503, 0.848]. percentile interval 0.357 wide, highest posterior density interval 0.345 wide. Thus, percentile interval wider. HPDI finds narrowest interval contains 89% data. Therefore, unless posterior perfectly symmetrical, central 89% , definition, wider HDPI.intervals, boundary information conveyed. information actual shape posterior conveyed. Without visualizing posterior, interval boundaries might tempt us (incorrectly) assume values interval equally likely values middle range plausible. However, know previous problem posterior asymmetrical, values closer low end interval plausible values high end interval.4. OPTIONAL CHALLENGE. Suppose bias sampling Land likely Water recorded. Specifically, assume 1--5 (20%) Water samples accidentally recorded instead “Land.” First, write generative simulation sampling process. Assuming true proportion Water 0.70, proportion simulation tend produce instead? Second, using simulated sample 20 tosses, compute unbiased posterior distribution true proportion water.’ll start writing function generate biased data. First, generate specified number tosses true_prop. Normally, use rbinom(n = 1, size = tosses), provide total number successes (tosses “water” result). want results individual toss. , example, rather 1 trial 100 tosses, 100 trials 1 toss.Next, generate random uniform numbers 0 1. indicate trials biased. every element bias_sim less bias, corresponding element true_trials changed 0 new vector called bias_trials. original results 0 (“land”), bias effect (.e., 0 changed 0). original result 1 (“water”), bias chance result changed 0.Finally, sum total number 1s (“water”) implementing bias return results.bias mean estimated proportion water? explore, ’ll run short simulation. ’ll use biased_globe() function 1,000 simulations 100 tosses. simulations, can see proportion tosses water, bias included.previous figure, true proportion, 0.7, indicated dashed red line. However, can see biased tosses greatly reducing number “waters” actually observe. average, observe 56 waters, estimated proportion 0.56.final step compute unbiased posterior distribution true proportion water, using 20 tosses biased data generation process. ’ll start generating 20 tosses.data, observed 11 waters. final step estimate unbiased posterior. can accomplish using biased sampling rate 0.8. , true water observation 80% chance actually recorded water observation. shown crtd_likelihood line . dbinom() function, rather setting prob = p_grid, set prob = p_grid * 0.8, reflect bias sampling. see biased posterior peaks around 0.55, whereas correcting bias results posterior peaks right around 0.70.","code":"\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),\n               prior = rep(1, times = 1000))\ndist\n#> # A tibble: 1,000 × 2\n#>     p_grid prior\n#>      <dbl> <dbl>\n#>  1 0           1\n#>  2 0.00100     1\n#>  3 0.00200     1\n#>  4 0.00300     1\n#>  5 0.00400     1\n#>  6 0.00501     1\n#>  7 0.00601     1\n#>  8 0.00701     1\n#>  9 0.00801     1\n#> 10 0.00901     1\n#> # … with 990 more rows\ndist <- dist %>% \n  mutate(likelihood = dbinom(4, size = 15, prob = p_grid),\n         posterior = likelihood * prior,\n         posterior = posterior / sum(posterior))\ndist\n#> # A tibble: 1,000 × 4\n#>     p_grid prior    likelihood posterior\n#>      <dbl> <dbl>         <dbl>     <dbl>\n#>  1 0           1 0              0       \n#>  2 0.00100     1 0.00000000136  2.17e-11\n#>  3 0.00200     1 0.0000000214   3.44e-10\n#>  4 0.00300     1 0.000000107    1.72e- 9\n#>  5 0.00400     1 0.000000336    5.38e- 9\n#>  6 0.00501     1 0.000000811    1.30e- 8\n#>  7 0.00601     1 0.00000166     2.66e- 8\n#>  8 0.00701     1 0.00000305     4.88e- 8\n#>  9 0.00801     1 0.00000514     8.23e- 8\n#> 10 0.00901     1 0.00000814     1.30e- 7\n#> # … with 990 more rows\nset.seed(123)\n\ndist %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE) %>% \n  ggplot(aes(x = p_grid)) +\n  stat_histinterval(.width = c(0.67, 0.89, 0.97), breaks = seq(0, 1, 0.02),\n                    point_interval = mean_hdci) +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\ndist <- tibble(p_grid = seq(from = 0, to = 1, length.out = 1000),\n               prior = rep(c(0, 1), each = 500)) %>% \n  mutate(likelihood = dbinom(4, size = 6, prob = p_grid),\n         posterior = likelihood * prior,\n         posterior = posterior / sum(posterior))\ndist\n#> # A tibble: 1,000 × 4\n#>     p_grid prior likelihood posterior\n#>      <dbl> <dbl>      <dbl>     <dbl>\n#>  1 0           0   0                0\n#>  2 0.00100     0   1.50e-11         0\n#>  3 0.00200     0   2.40e-10         0\n#>  4 0.00300     0   1.21e- 9         0\n#>  5 0.00400     0   3.82e- 9         0\n#>  6 0.00501     0   9.32e- 9         0\n#>  7 0.00601     0   1.93e- 8         0\n#>  8 0.00701     0   3.57e- 8         0\n#>  9 0.00801     0   6.07e- 8         0\n#> 10 0.00901     0   9.70e- 8         0\n#> # … with 990 more rows\nset.seed(123)\n\ndist %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE) %>% \n  ggplot(aes(x = p_grid)) +\n  stat_histinterval(.width = c(0.67, 0.89, 0.97), breaks = seq(0, 1, 0.02),\n                    point_interval = mean_hdci) +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")\nset.seed(123)\n\nintervals <- dist %>% \n  slice_sample(n = 10000, weight_by = posterior, replace = TRUE) %>% \n  summarize(bound = c(\"lower\", \"upper\"),\n            pi = PI(p_grid, prob = 0.89),\n            hpdi = HPDI(p_grid, prob = 0.89))\nintervals\n#> # A tibble: 2 × 3\n#>   bound    pi  hpdi\n#>   <chr> <dbl> <dbl>\n#> 1 lower 0.525 0.503\n#> 2 upper 0.882 0.848\nbiased_globe <- function(tosses = 100, true_prop = 0.7, bias = 0.2) {\n  true_trials <- rbinom(n = tosses, size = 1, prob = true_prop)\n  bias_sim <- runif(n = tosses, min = 0, max = 1)\n\n  bias_trials <- true_trials\n  bias_trials[which(bias_sim < bias)] <- 0L\n  sum(bias_trials)\n}\nbias_prop <- map_int(seq_len(1000),\n                     ~biased_globe(tosses = 100, true_prop = 0.7, bias = 0.2))\n\nggplot() +\n  stat_histinterval(aes(x = bias_prop / 100), .width = c(0.67, 0.89, 0.97),\n                    breaks = seq(0, 1, by = 0.01)) +\n  geom_vline(aes(xintercept = 0.7), linetype = \"dashed\", color = \"red\") +\n  expand_limits(x = c(0, 1)) +\n  labs(x = \"Proportion Water (p)\", y = \"Simulations\")\nset.seed(123)\nbiased_dat <- biased_globe(tosses = 20)\nbiased_dat\n#> [1] 11\nlibrary(geomtextpath)\n\nposterior <- tibble(p_grid = seq(0, 1, length.out = 1000)) %>% \n  mutate(prior = dbeta(p_grid, shape1 = 1, shape2 = 1),\n         bias_likelihood = dbinom(biased_dat, size = 20, prob = p_grid),\n         crtd_likelihood = dbinom(biased_dat, size = 20, prob = p_grid * 0.8),\n         bias_posterior = bias_likelihood * prior,\n         crtd_posterior = crtd_likelihood * prior,\n         bias_posterior = bias_posterior / sum(bias_posterior),\n         crtd_posterior = crtd_posterior / sum(crtd_posterior))\n\nggplot(posterior, aes(x = p_grid)) +\n  geom_textline(aes(y = bias_posterior), linetype = \"dashed\", color = \"grey70\",\n                size = 6, linewidth = 1, label = \"Biased\", hjust = 0.45,\n                family = \"Source Sans Pro\") +\n  geom_textline(aes(y = crtd_posterior), linetype = \"solid\", color = \"#009FB7\",\n                size = 6, linewidth = 1, label = \"Corrected\", hjust = 0.4,\n                family = \"Source Sans Pro\") +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  labs(x = \"Proportion Water (p)\", y = \"Posterior Density\")"},{"path":"linear-models-causal-inference.html","id":"linear-models-causal-inference","chapter":"Week 2: Linear Models & Causal Inference","heading":"Week 2: Linear Models & Causal Inference","text":"second week covers Chapter 4 (Geocentric Models).","code":""},{"path":"linear-models-causal-inference.html","id":"lectures-1","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.1 Lectures","text":"Lecture 3:Lecture 4:","code":""},{"path":"linear-models-causal-inference.html","id":"exercises-1","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.2 Exercises","text":"","code":""},{"path":"linear-models-causal-inference.html","id":"chapter-4","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.2.1 Chapter 4","text":"4E1. model definition , line likelihood?\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu,\\sigma) \\\\\n\\mu &\\sim \\text{Normal}(0,10) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]likelihood given first line, \\(y_i \\sim \\text{Normal}(\\mu,\\sigma)\\). lines represent prior distributions \\(\\mu\\) \\(\\sigma\\).4E2. model definition just , many parameters posterior distribution?two parameters, \\(\\mu\\) \\(\\sigma\\).4E3. Using model definition , write appropriate form Bayes’ theorem includes proper likelihood priors.\\[\n\\text{Pr}(\\mu,\\sigma|y) = \\frac{\\text{Normal}(y|\\mu,\\sigma)\\text{Normal}(\\mu|0,10)\\text{Exponential}(\\sigma|1)}{\\int\\int\\text{Normal}(y|\\mu,\\sigma)\\text{Normal}(\\mu|0,10)\\text{Exponential}(\\sigma|1)d \\mu d \\sigma}\n\\]4E4. model definition , line linear model?\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu,\\sigma) \\\\\n\\mu_i &= \\alpha + \\beta x_i \\\\\n\\alpha &\\sim \\text{Normal}(0,10) \\\\\n\\beta &\\sim \\text{Normal}(0,1) \\\\\n\\sigma &\\sim \\text{Exponential}(2)\n\\end{align}\\]linear model second line, \\(\\mu_i = \\alpha + \\beta x_i\\).4E5. model definition just , many parameters posterior distribution?now three model parameters: \\(\\alpha\\), \\(\\beta\\), \\(\\sigma\\). mean, \\(\\mu\\) longer parameter, defined deterministically, function parameters model.4M1. model definition , simulate observed y values prior (posterior).\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu,\\sigma) \\\\\n\\mu &\\sim \\text{Normal}(0,10) \\\\\n\\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]4M2. Trasnlate model just quap formula.4M3. Translate quap model formula mathematical model definition.\\[\\begin{align}\n  y_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta x_i \\\\\n  \\alpha &\\sim \\text{Normal}(0, 10) \\\\\n  \\beta &\\sim \\text{Uniform}(0, 1) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]4M4. sample students measured height year 3 years. third year, want fit linear regression predicting height using year predictor. Write mathematical model definition regression, using variable names priors choose. prepared defend choice priors.\\[\\begin{align}\n  h_{ij} &\\sim \\text{Normal}(\\mu_{ij}, \\sigma) \\\\\n  \\mu_{ij} &= \\alpha + \\beta(y_j - \\bar{y}) \\\\\n  \\alpha &\\sim \\text{Normal}(100, 10) \\\\\n  \\beta &\\sim \\text{Normal}(0, 10) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]height centered, \\(\\alpha\\) represents average height average year (.e., year 2). prior \\(\\text{Normal}(100, 10)\\) chosen assuming height measured centimeters sample children still growing.slope extremely vague. prior centered zero, standard deviation prior 10 represents wide range possible growth (shrinkage). growth spurts, height growth averages 6–13 cm/year. standard deviation 10 encompasses range might expect see growth occurring high rate.Finally, exponential prior \\(\\sigma\\) assumes average deviation 1.Prior predictive simulations also appear give reasonably plausible regression lines, given current assumptions.4M5. Now suppose remind every student got taller year. information lead change choice priors? ?Yes. know increase year always lead increased height, know \\(\\beta\\) positive. Therefore, prior reflect using, example, log-normal distribution.\\[\n\\beta \\sim \\text{Log-Normal}(1,0.5)\n\\]prior gives expectation 3cm per year, 89% highest density interval 0.87cm 5.18cm per year.Prior predictive simulations plausible lines using new log-normal prior indicate priors still represent plausible values. lines positive, due prior constraint. However, variation around mean, lines show decrease height. truly impossible students shrink, data like might arise measurement error.4M6. Now suppose tell variance among heights students age never 64cm. lead revise priors?variance 64cm corresponds standard deviation 8cm. current prior \\(\\sigma \\sim \\text{Exponential}(1)\\) gives little probability mass values greater 8. However, still theoretically possible. want truly constrain variance way, use \\(\\text{Uniform}(0,8)\\) prior. eliminate values result variance greater 64cm.4M7. Refit model m4.3 chapter, omit mean weight xbar time. Compare new model’s posterior original model. particular, look covariance among parameters. different? compare posterior predictions models.First, ’ll reproduce m4.3 using quap(), using brm(). covariance matrix , expect.Now, let’s using non-centered parameterization. time, ’ll use brm().now see non-zero covariances parameters. Lets compare posterior predictions. ’ll generate hypothetical outcome plots animated show uncertainty estimates (see Hullman et al., 2015; Kale et al., 2019). ’ll just animate estimated regression line using {gganimate} (Pedersen & Robinson, 2020). can see predictions two models nearly identical.4M8. chapter, used 15 knots cherry blossom spline. Increase number knots observe happens resulting spline. adjust also width prior weights—change standard deviation prior watch happens. think combination know number prior weights controls?First lets duplicate 15-knot spline model chapter. ’ll double number knots play prior.Visualizing model, see looks similar fitted model chapter.Now ’ll fit two additional models. first uses 30 knots, second uses tighter prior.expected, visualize models see increasing number knots increases “wiggly-ness” spline. can also see tightening prior weights takes away “wiggly-ness”.4H1. weights listed recored !Kung census, heights recorded individuals. Provide predicted heights 89% intervals individuals. , fill table, , using model-based predictions.key function tidybayes::add_predicted_draws(), samples posterior predictive distribution. use model b4.3 make predictions, estimated back question 4M7.4H2. Select rows Howell1 data ages 18 years age. right, end new data frame 192 rows .Fit linear regression data, using quap. Present interpret estimates. every 10 units increase weight, much taller model predict child gets?’ll use brms::brm() fitting model. ’ll use priors model adults, except weight \\(\\beta\\). children shorter adults, ’ll use prior \\(\\text{Normal}(138,20)\\), based data reported Fryar et al. (2016). Based estimates, increase 10 units weights corresponds average increase height 27.2 centimeters.Plot raw data, height vertical axis weight horizontal axis. Superimpose MAP regression line 89% interval mean. Also superimpose 89% interval predicted heights.aspects model fit concern ? Describe kinds assumptions change, , improve model. don’t write new code. Just explain model appears bad job , hypothesize better model.model consistently -estimates height individuals weight less ~13 great ~35. model also consistently underestimating height individuals weight ~13-35. Thus, data appears curve assumption straight line violating. wanted improve model, relax assumption straight line.4H3. Suppose colleauge , works allometry, glances practice problems just . colleague exclaims, “’s silly. Everyone knows ’s logarithm body weight scales height!” Let’s take colleague’s advice see happens.Model relationship height (cm) natural logarithm weight (log-kg). Use entire Howell1 data frame, 544 rows, adults non-adults. Can interpret resulting estimates?Conditional data model, intercept estimate sprintf(\"%0.1f\", summary(b4h3)[[\"fixed\"]][\"Intercept\", \"Estimate\"]) represents predicted average height individual average log-weight (log-kg). \\(\\beta\\) estimate sprintf(\"%0.1f\", summary(b4h3)[[\"fixed\"]][\"log_weight_c\", \"Estimate\"]) represents average expected increase height associated one-unit increase weight (log-kg).Begin plot: plot( height ~ weight , data = Howell1 ). use samples quadratic approximate posterior model () superimpose plot: (1) predicted mean height function weight, (2) 97% interval mean, (3) 97% interval predicted heights.4H4. Plot prior predictive distribution parabolic polynomial regression model chapter. can modify code plots linear regression prior predictive distribution. Can modify prior distributions \\(\\alpha\\), \\(\\beta_1\\), \\(\\beta_2\\) prior predictions stay within biologically reasonable outcome space? say: try fit data hand. try keep curves consistent know height weight, seeing exact data.polynomial model chapter defined :\\[\\begin{align}\n  h_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_1x_i + \\beta_2x_i^2 \\\\\n  \\alpha &\\sim \\text{Normal}(178,20) \\\\\n  \\beta_1 &\\sim \\text{Log-Normal}(0,1) \\\\\n  \\beta_2 &\\sim \\text{Normal}(0,1) \\\\\n  \\sigma &\\sim \\text{Uniform}(0,50)\n\\end{align}\\]First, let’s generate prior predictive checks original priors.Clearly room improvement . However, ’s intuitive exactly parameter effects parabolic curve, finding good prior distribution really hard! much trial error playing parabola calculators online, ended :\\[\\begin{align}\n  h_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_1x_i + \\beta_2x_i^2 \\\\\n  \\alpha &\\sim \\text{Normal}(-190,5) \\\\\n  \\beta_1 &\\sim \\text{Normal}(13,0.2) \\\\\n  \\beta_2 &\\sim \\text{Uniform}(-0.13,-0.10) \\\\\n  \\sigma &\\sim \\text{Uniform}(0,50)\n\\end{align}\\]following prior predictive distribution.4H5. Return data(cherry_blossoms) model association blossom date (doy) March temperature (temp). Note many missing values variables. may consider linear model, polynomial, spline temperature. well temperature trend predict blossom trend?’ll try type model: linear, polynomial, spline. , ’ll fit model, visualize predictions observed data.Now let’s visualize predictions model. Overall predictions model remarkably similar. Therefore, go linear model, simplest models.4H6. Simulate prior predictive distribution cherry blossom spline chapter. Adjust prior weights observe happens. think prior weights ?reminder, cherry blossom spline model chapter:\\[\\begin{align}\n  D_i &\\sim \\text{Normal}(\\mu_i,\\sigma) \\\\\n  \\mu_i &= \\alpha + \\sum_{k=1}^Kw_kB_{k,} \\\\\n  \\alpha &\\sim \\text{Normal}(100, 10) \\\\\n  w_k &\\sim \\text{Normal}(0,10) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]’ll also need recreate basis functions model uses:Finally, can generate data priors, combine parameters basis functions get prior predictive distributions.Now let’s tighten prior w \\(\\text{Normal}(0,2)\\), used exercise 4M8. Now lines much less wiggly, consistent found previous exercise, used observed data.4H8. (sic; 4H7 text, ’ve kept labeled 4H8 consistent book) cherry blossom spline chapter used intercept \\(\\alpha\\), technically doesn’t require one. first basis functions substitute intercept. Try refitting cherry blossom spline without intercept. else model need change make work?can remove intercept removing parameter model. {brms} formula, means replace doy ~ 1 + B question 4M8 doy ~ 0 + B. 0 means “don’t estimate intercept.”looks lot like original model, except left hand side spline pulled . likely due prior w. prior centered 0, assumes intercept present (.e., curves spline average deviation 0 mean). However, without intercept, prior drags line actual zero first basis function non-zero. changing prior w prior originally used intercept, get almost original model back.","code":"\nlibrary(tidyverse)\n\nsim <- tibble(mu = rnorm(n = 10000, mean = 0, sd = 10),\n              sigma = rexp(n = 10000, rate = 1)) %>%\n  mutate(y = rnorm(n = 10000, mean = mu, sd = sigma))\n\nggplot(sim, aes(x = y)) +\n  geom_density() +\n  labs(x = \"y\", y = \"Density\")\nflist <- alist(\n  y ~ dnorm(mu, sigma),\n  mu ~ dnorm(0, 10),\n  sigma ~ dexp(1)\n)y ~ dnorm( mu , sigma ),\nmu <- a + b*x,\na ~ dnorm( 0 , 10 ),\nb ~ dunif( 0 , 1 ),\nsigma ~ dexp( 1 )\nn <- 50\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       beta = rnorm(n, 0, 10),\n       sigma = rexp(n, 1)) %>%\n  expand(nesting(group, alpha, beta, sigma), year = c(1, 2, 3)) %>%\n  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma)) %>%\n  ggplot(aes(x = year, y = height, group = group)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Height\")\nlibrary(tidybayes)\n\nset.seed(123)\nsamples <- rlnorm(1e8, 1, 0.5)\nbounds <- mean_hdi(samples, .width = 0.89)\n\nggplot() +\n  stat_function(data = tibble(x = c(0, 10)), mapping = aes(x = x),\n                geom = \"line\", fun = dlnorm,\n                args = list(meanlog = 1, sdlog = 0.5)) +\n  geom_ribbon(data = tibble(x = seq(bounds$ymin, bounds$ymax, 0.01)),\n              aes(x = x, ymin = 0, ymax = dlnorm(x, 1, 0.5)),\n              alpha = 0.8) +\n  scale_x_continuous(breaks = seq(0, 10, 2)) +\n  labs(x = expression(beta), y = \"Density\")\nn <- 50\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       beta = rlnorm(n, 1, 0.5),\n       sigma = rexp(n, 1)) %>%\n  expand(nesting(group, alpha, beta, sigma), year = c(1, 2, 3)) %>%\n  mutate(height = rnorm(n(), alpha + beta * (year - mean(year)), sigma)) %>% \n  ggplot(aes(x = year, y = height, group = group)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Height\")\nlibrary(rethinking)\nlibrary(brms)\n\ndata(Howell1)\nhow_dat <- Howell1 %>%\n  filter(age >= 18) %>%\n  mutate(weight_c = weight - mean(weight))\n\n# first, duplicate model with `quap`\nm4.3 <- quap(alist(height ~ dnorm(mu, sigma),\n                   mu <- a + b * (weight_c),\n                   a ~ dnorm(178, 20),\n                   b ~ dlnorm(0, 1),\n                   sigma ~ dunif(0, 50)),\n             data = how_dat)\n\nround(vcov(m4.3), 3)\n#>           a     b sigma\n#> a     0.073 0.000 0.000\n#> b     0.000 0.002 0.000\n#> sigma 0.000 0.000 0.037\n\n# and then with brms\nb4.3 <- brm(height ~ 1 + weight_c, data = how_dat, family = gaussian,\n            prior = c(prior(normal(178, 20), class = Intercept),\n                      prior(lognormal(0, 1), class = b, lb = 0),\n                      prior(uniform(0, 50), class = sigma)),\n            iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4.3-0\"))\n\nas_draws_df(b4.3) %>%\n  as_tibble() %>% \n  select(b_Intercept, b_weight_c, sigma) %>%\n  cov() %>%\n  round(digits = 3)\n#>             b_Intercept b_weight_c sigma\n#> b_Intercept       0.074      0.000 0.000\n#> b_weight_c        0.000      0.002 0.000\n#> sigma             0.000      0.000 0.038\nb4.3_nc <- brm(height ~ 1 + weight, data = how_dat, family = gaussian,\n               prior = c(prior(normal(178, 20), class = Intercept),\n                         prior(lognormal(0, 1), class = b, lb = 0),\n                         prior(uniform(0, 50), class = sigma)),\n               iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4.3_nc\"))\n\nas_draws_df(b4.3_nc) %>%\n  as_tibble() %>% \n  select(b_Intercept, b_weight, sigma) %>%\n  cov() %>%\n  round(digits = 3)\n#>             b_Intercept b_weight sigma\n#> b_Intercept       3.653   -0.079 0.010\n#> b_weight         -0.079    0.002 0.000\n#> sigma             0.010    0.000 0.035\nlibrary(gganimate)\n\nweight_seq <- tibble(weight = seq(25, 70, length.out = 100)) %>%\n  mutate(weight_c = weight - mean(how_dat$weight))\n\npredictions <- bind_rows(\n  predict(b4.3, newdata = weight_seq) %>%\n    as_tibble() %>%\n    bind_cols(weight_seq) %>%\n    mutate(type = \"Centered\"),\n  predict(b4.3_nc, newdata = weight_seq) %>%\n    as_tibble() %>%\n    bind_cols(weight_seq) %>%\n    mutate(type = \"Non-centered\")\n)\n\nfits <- bind_rows(\n  weight_seq %>%\n    add_epred_draws(b4.3) %>%\n    mutate(type = \"Centered\"),\n  weight_seq %>%\n    add_epred_draws(b4.3_nc) %>%\n    mutate(type = \"Non-centered\")\n) %>%\n  ungroup()\n\nbands <- fits %>%\n  group_by(type, weight) %>%\n  median_qi(.epred, .width = c(.67, .89, .97))\n\nlines <- fits %>%\n  filter(.draw %in% sample(unique(.data$.draw), size = 50))\n\nggplot(lines, aes(x = weight)) +\n  facet_wrap(~type, nrow = 1) +\n  geom_ribbon(data = predictions, aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3) +\n  geom_lineribbon(data = bands, aes(y = .epred, ymin = .lower, ymax = .upper),\n                  color = NA) +\n  scale_fill_brewer(palette = \"Blues\", breaks = c(.67, .89, .97)) +\n  geom_line(aes(y = .epred, group = .draw)) +\n  geom_point(data = how_dat, aes(y = height), shape = 1, alpha = 0.7) +\n  labs(x = \"Weight\", y = \"Height\", fill = \"Interval\") +\n  transition_states(.draw, 0, 1)\nlibrary(splines)\n\ndata(cherry_blossoms)\ncb_dat <- cherry_blossoms %>%\n  drop_na(doy)\n\n# original m4.7 model\nknots_15 <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15))\nB_15 <- bs(cb_dat$year, knots = knots_15[-c(1, 15)],\n           degree = 3, intercept = TRUE)\n\ncb_dat_15 <- cb_dat %>% \n  mutate(B = B_15)\n\nb4.7 <- brm(doy ~ 1 + B, data = cb_dat_15, family = gaussian,\n            prior = c(prior(normal(100, 10), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4.7\"))\noriginal_draws <- cb_dat_15 %>% \n  add_epred_draws(b4.7) %>% \n  summarize(mean_hdi(.epred, .width = 0.89),\n            .groups = \"drop\")\n\nggplot(original_draws, aes(x = year, y = doy)) +\n  geom_vline(xintercept = knots_15, alpha = 0.5) +\n  geom_hline(yintercept = fixef(b4.7)[1, 1], linetype = \"dashed\") +\n  geom_point(alpha = 0.5) +\n  geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = \"#009FB7\", alpha = 0.8) +\n  labs(x = \"Year\", y = \"Day in Year\")\n# double the number of knots\nknots_30 <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 30))\nB_30 <- bs(cb_dat$year, knots = knots_30[-c(1, 30)],\n           degree = 3, intercept = TRUE)\n\ncb_dat_30 <- cb_dat %>% \n  mutate(B = B_30)\n\nb4.7_30 <- brm(doy ~ 1 + B, data = cb_dat_30, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4.7_30\"))\n\n# and modify the prior\nb4.7_30p <- brm(doy ~ 1 + B, data = cb_dat_30, family = gaussian,\n                prior = c(prior(normal(100, 10), class = Intercept),\n                          prior(normal(0, 2), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp4\", \"b4.7_30p\"))\n# create plot data\nspline_15 <- original_draws %>%\n  select(-B) %>% \n  mutate(knots = \"15 knots (original model)\")\n\nspline_30 <- cb_dat_30 %>% \n  add_epred_draws(b4.7_30) %>% \n  summarize(mean_hdi(.epred, .width = 0.89),\n            .groups = \"drop\") %>% \n  select(-B) %>% \n  mutate(knots = \"30 knots\")\n\nspline_30p <- cb_dat_30 %>% \n  add_epred_draws(b4.7_30p) %>% \n  summarize(mean_hdi(.epred, .width = 0.89),\n            .groups = \"drop\") %>% \n  select(-B) %>% \n  mutate(knots = \"30 knots; Tight prior\")\n\nall_splines <- bind_rows(spline_15, spline_30, spline_30p)\n\n# make plot\nggplot(all_splines, aes(x = year, y = doy)) +\n  geom_point(alpha = 0.5) +\n  geom_ribbon(aes(ymin = ymin, ymax = ymax), fill = \"#009FB7\", alpha = 0.8) +\n  facet_wrap(~knots, ncol = 1) +\n  labs(x = \"Year\", y = \"Day in Year\")\ntibble(individual = 1:5,\n       weight = c(46.95, 43.72, 64.78, 32.59, 54.63)) %>%\n  mutate(weight_c = weight - mean(how_dat$weight)) %>%\n  add_predicted_draws(b4.3) %>%\n  group_by(individual, weight) %>%\n  mean_qi(.prediction, .width = 0.89) %>%\n  mutate(range = glue(\"[{sprintf('%0.1f', .lower)}--\",\n                      \"{sprintf('%0.1f', .upper)}]\"),\n         .prediction = sprintf(\"%0.1f\", .prediction)) %>%\n  select(individual, weight, exp = .prediction, range) %>%\n  kbl(align = \"c\", booktabs = TRUE,\n      col.names = c(\"Individual\", \"weight\", \"expected height\", \"89% interval\"))\nyoung_how <- Howell1 %>%\n  filter(age < 18) %>%\n  mutate(weight_c = weight - mean(weight))\nnrow(young_how)\n#> [1] 192\nb4h2 <- brm(height ~ 1 + weight_c, data = young_how, family = gaussian,\n            prior = c(prior(normal(138, 20), class = Intercept),\n                      prior(lognormal(0, 1), class = b, lb = 0),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h2.rds\"))\n\nsummary(b4h2)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + weight_c \n#>    Data: young_how (Number of observations: 192) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept   108.36      0.60   107.17   109.55 1.00     6976     5427\n#> weight_c      2.72      0.07     2.58     2.85 1.00     7770     5680\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     8.36      0.42     7.59     9.22 1.00     7618     5811\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nmod_fits <- tibble(weight = seq_range(young_how$weight, 100)) %>% \n  mutate(weight_c = weight - mean(young_how$weight)) %>%\n  add_epred_draws(b4h2) %>%\n  group_by(weight) %>%\n  mean_qi(.epred, .width = 0.89)\n\nmod_preds <- tibble(weight = seq_range(young_how$weight, 100)) %>% \n  mutate(weight_c = weight - mean(young_how$weight)) %>%\n  add_predicted_draws(b4h2) %>%\n  group_by(weight) %>%\n  mean_qi(.prediction, .width = 0.89)\n\nggplot(young_how, aes(x = weight)) +\n  geom_point(aes(y = height), alpha = 0.4) +\n  geom_ribbon(data = mod_preds, aes(ymin = .lower, ymax = .upper),\n              alpha = 0.2) +\n  geom_lineribbon(data = mod_fits,\n                  aes(y = .epred, ymin = .lower, ymax = .upper),\n                  fill = \"grey60\", size = 1) +\n  labs(x = \"Weight\", y = \"Height\")\nfull_how <- Howell1 %>%\n  mutate(log_weight = log(weight),\n         log_weight_c = log_weight - mean(log_weight))\n\nb4h3 <- brm(height ~ 1 + log_weight_c, data = full_how, family = gaussian,\n            prior = c(prior(normal(158, 20), class = Intercept),\n                      prior(lognormal(0, 1), class = b, lb = 0),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h3\"))\nsummary(b4h3)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: height ~ 1 + log_weight_c \n#>    Data: full_how (Number of observations: 544) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept      138.27      0.22   137.83   138.70 1.00     8706     6181\n#> log_weight_c    47.08      0.38    46.33    47.81 1.00     8601     6066\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     5.13      0.15     4.85     5.44 1.00     8332     5723\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nhow_fits <- tibble(weight = seq_range(full_how$weight, 100)) %>% \n  mutate(log_weight = log(weight),\n         log_weight_c = log_weight - mean(full_how$log_weight)) %>%\n  add_epred_draws(b4h3) %>%\n  group_by(weight) %>%\n  mean_qi(.epred, .width = 0.97)\n\nhow_preds <- tibble(weight = seq_range(full_how$weight, 100)) %>% \n  mutate(log_weight = log(weight),\n         log_weight_c = log_weight - mean(full_how$log_weight)) %>%\n  add_predicted_draws(b4h3) %>%\n  group_by(weight) %>%\n  mean_qi(.prediction, .width = 0.97)\n\nggplot(full_how, aes(x = weight)) +\n  geom_point(aes(y = height), alpha = 0.4) +\n  geom_ribbon(data = how_preds, aes(ymin = .lower, ymax = .upper),\n              alpha = 0.2) +\n  geom_lineribbon(data = how_fits,\n                  aes(y = .epred, ymin = .lower, ymax = .upper),\n                  fill = \"grey60\", size = 1) +\n  labs(x = \"Weight\", y = \"Height\")\nn <- 1000\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 178, 20),\n       beta1 = rlnorm(n, 0, 1),\n       beta2 = rnorm(n, 0, 1)) %>%\n  expand(nesting(group, alpha, beta1, beta2),\n         weight = seq(25, 70, length.out = 100)) %>%\n  mutate(height = alpha + (beta1 * weight) + (beta2 * (weight ^ 2))) %>%\n  ggplot(aes(x = weight, y = height, group = group)) +\n  geom_line(alpha = 1 / 10) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, color = \"red\") +\n  annotate(geom = \"text\", x = 25, y = 0, hjust = 0, vjust = 1,\n           label = \"Embryo\") +\n  annotate(geom = \"text\", x = 25, y = 272, hjust = 0, vjust = 0,\n           label = \"World's tallest person (272cm)\") +\n  coord_cartesian(ylim = c(-25, 300)) +\n  labs(x = \"Weight\", y = \"Height\")\nn <- 1000\ntibble(group = seq_len(n),\n       alpha = rnorm(n, -190, 5),\n       beta1 = rnorm(n, 13, 0.2),\n       beta2 = runif(n, -0.13, -0.1)) %>%\n  expand(nesting(group, alpha, beta1, beta2),\n         weight = seq(25, 70, length.out = 100)) %>%\n  mutate(height = alpha + (beta1 * weight) + (beta2 * (weight ^ 2))) %>%\n  ggplot(aes(x = weight, y = height, group = group)) +\n  geom_line(alpha = 1 / 10) +\n  geom_hline(yintercept = c(0, 272), linetype = 2:1, color = \"red\") +\n  annotate(geom = \"text\", x = 25, y = -3, hjust = 0, vjust = 1,\n           label = \"Embryo\") +\n  annotate(geom = \"text\", x = 25, y = 275, hjust = 0, vjust = 0,\n           label = \"World's tallest person (272cm)\") +\n  coord_cartesian(ylim = c(-25, 300)) +\n  labs(x = \"Weight\", y = \"Height\")\ndata(cherry_blossoms)\n\ncb_temp <- cherry_blossoms %>%\n  drop_na(doy, temp) %>%\n  mutate(temp_c = temp - mean(temp),\n         temp_s = temp_c / sd(temp),\n         temp_s2 = temp_s ^ 2,\n         temp_s3 = temp_s ^ 3)\n\n# linear model\nlin_mod <- brm(doy ~ 1 + temp_c, data = cb_temp, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-linear\"))\n\n# quadratic model\nqad_mod <- brm(doy ~ 1 + temp_s + temp_s2, data = cb_temp, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b, coef = \"temp_s\"),\n                         prior(normal(0, 1), class = b, coef = \"temp_s2\"),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-quadratic\"))\n\n# cubic model\ncub_mod <- brm(doy ~ 1 + temp_s + temp_s2 + temp_s3, data = cb_temp,\n               family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b, coef = \"temp_s\"),\n                         prior(normal(0, 1), class = b, coef = \"temp_s2\"),\n                         prior(normal(0, 1), class = b, coef = \"temp_s3\"),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-cubic\"))\n\n# spline model\nknots_30 <- quantile(cb_temp$temp, probs = seq(0, 1, length.out = 30))\nB_30 <- bs(cb_temp$temp, knots = knots_30[-c(1, 30)],\n           degree = 3, intercept = TRUE)\n\ncb_temp_30 <- cb_temp %>% \n  mutate(B = B_30)\n\nspl_mod <- brm(doy ~ 1 + B, data = cb_temp_30, family = gaussian,\n               prior = c(prior(normal(100, 10), class = Intercept),\n                         prior(normal(0, 10), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp4\", \"b4h5-spline\"))\ngrid <- tibble(temp = seq_range(cb_temp_30$temp, 100)) %>% \n  mutate(temp_c = temp - mean(cb_temp$temp),\n         temp_s = temp_c / sd(cb_temp$temp),\n         temp_s2 = temp_s ^ 2,\n         temp_s3 = temp_s ^ 3)\n\nknots_30 <- quantile(grid$temp, probs = seq(0, 1, length.out = 30))\nB_30 <- bs(grid$temp, knots = knots_30[-c(1, 30)],\n           degree = 3, intercept = TRUE)\n\ngrid <- grid %>% \n  mutate(B = B_30)\n\n\nfits <- bind_rows(\n  add_epred_draws(grid, lin_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Linear\"),\n  add_epred_draws(grid, qad_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Quadratic\"),\n  add_epred_draws(grid, cub_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Cubic\"),\n  add_epred_draws(grid, spl_mod) %>%\n    summarize(mean_qi(.epred, .width = c(0.67, 0.89, 0.97)),\n              .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.epred = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Spline\")\n) %>%\n  ungroup() %>%\n  mutate(model = factor(model, levels = c(\"Linear\", \"Quadratic\", \"Cubic\",\n                                          \"Spline\")))\n\npreds <- bind_rows(\n  add_predicted_draws(grid, lin_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Linear\"),\n  add_predicted_draws(grid, qad_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Quadratic\"),\n  add_predicted_draws(grid, cub_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Cubic\"),\n  add_predicted_draws(grid, spl_mod) %>%\n    summarize(mean_qi(.prediction, .width = 0.89), .groups = \"drop\") %>% \n    select(-B) %>% \n    rename(.prediction = y, .lower = ymin, .upper = ymax) %>% \n    mutate(model = \"Spline\")\n) %>%\n  ungroup() %>%\n  mutate(model = factor(model, levels = c(\"Linear\", \"Quadratic\", \"Cubic\",\n                                          \"Spline\")))\n\nggplot(cb_temp, aes(x = temp)) +\n  facet_wrap(~model, nrow = 2) +\n  geom_point(aes(y = doy), alpha = 0.2) +\n  geom_ribbon(data = preds, aes(ymin = .lower, ymax = .upper),\n              alpha = 0.2) +\n  geom_lineribbon(data = fits, aes(y = .epred, ymin = .lower, ymax = .upper),\n                  size = .6) +\n  scale_fill_brewer(palette = \"Blues\", breaks = c(0.67, 0.89, 0.97)) +\n  labs(x = \"March Temperature\", y = \"Day in Year\")\ncb_dat <- cherry_blossoms %>%\n  drop_na(doy)\n\nknot_list <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15))\n\nB <- bs(cb_dat$year,\n        knots = knot_list[-c(1, 15)],\n        degree = 3, intercept = TRUE)\nn <- 50\ntibble(.draw = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       w = purrr::map(seq_len(n),\n                      function(x, knots) {\n                        w <- rnorm(n = knots + 2, 0, 10)\n                        return(w)\n                      },\n                      knots = 15)) %>%\n  mutate(mu = map2(alpha, w,\n                   function(alpha, w, b) {\n                     res <- b %*% w\n                     res <- res + alpha\n                     res <- res %>%\n                       as_tibble(.name_repair = ~\".value\") %>%\n                       mutate(year = cb_dat$year, .before = 1)\n                     return(res)\n                   },\n                   b = B)) %>%\n  unnest(cols = mu) %>%\n  ggplot(aes(x = year, y = .value)) +\n  geom_vline(xintercept = knot_list, alpha = 0.5) +\n  geom_line(aes(group = .draw)) +\n  expand_limits(y = c(60, 140)) +\n  labs(x = \"Year\", y = \"Day in Year\")\nn <- 50\ntibble(.draw = seq_len(n),\n       alpha = rnorm(n, 100, 10),\n       w = purrr::map(seq_len(n),\n                      function(x, knots) {\n                        w <- rnorm(n = knots + 2, 0, 1)\n                        return(w)\n                      },\n                      knots = 15)) %>%\n  mutate(mu = map2(alpha, w,\n                   function(alpha, w, b) {\n                     res <- b %*% w\n                     res <- res + alpha\n                     res <- res %>%\n                       as_tibble(.name_repair = ~\".value\") %>%\n                       mutate(year = cb_dat$year, .before = 1)\n                     return(res)\n                   },\n                   b = B)) %>%\n  unnest(cols = mu) %>%\n  ggplot(aes(x = year, y = .value)) +\n  geom_vline(xintercept = knot_list, alpha = 0.5) +\n  geom_line(aes(group = .draw)) +\n  expand_limits(y = c(60, 140)) +\n  labs(x = \"Year\", y = \"Day in Year\")\ncb_dat <- cherry_blossoms %>%\n  drop_na(doy)\n\nknots_15 <- quantile(cb_dat$year, probs = seq(0, 1, length.out = 15))\nB_15 <- bs(cb_dat$year, knots = knots_15[-c(1, 15)],\n           degree = 3, intercept = TRUE)\n\ncb_dat_15 <- cb_dat %>% \n  mutate(B = B_15)\n\nb4h8 <- brm(doy ~ 0 + B, data = cb_dat_15, family = gaussian,\n            prior = c(prior(normal(0, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h8\"))\n\nepred_draws(b4h8, cb_dat_15) %>%\n  summarize(mean_qi(.epred, .width = 0.89), .groups = \"drop\") %>%\n  ggplot(aes(x = year, y = doy)) +\n  geom_point(alpha = 0.2) +\n  geom_hline(yintercept = mean(cb_dat$doy), linetype = \"dashed\") +\n  geom_lineribbon(aes(y = y, ymin = ymin, ymax = ymax),\n                  alpha = 0.8, fill = \"#009FB7\") +\n  labs(x = \"Year\", y = \"Day in Year\")\nb4h8_2 <- brm(doy ~ 0 + B, data = cb_dat_15, family = gaussian,\n            prior = c(prior(normal(100, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp4\", \"b4h8-2\"))\n\nepred_draws(b4h8_2, cb_dat_15) %>%\n  summarize(mean_qi(.epred, .width = 0.89), .groups = \"drop\") %>%\n  ggplot(aes(x = year, y = doy)) +\n  geom_point(alpha = 0.2) +\n  geom_hline(yintercept = mean(cb_dat$doy), linetype = \"dashed\") +\n  geom_lineribbon(aes(y = y, ymin = ymin, ymax = ymax),\n                  alpha = 0.8, fill = \"#009FB7\") +\n  labs(x = \"Year\", y = \"Day in Year\")"},{"path":"linear-models-causal-inference.html","id":"homework-1","chapter":"Week 2: Linear Models & Causal Inference","heading":"2.3 Homework","text":"1. Construct linear regression weight predicted height, using adults (age 18 greater) Howell1 dataset. heights listed recorded !Kung census, weights recorded individuals. Provide predicted weights 89% compatibility intervals individuals. , fill table , using model-based predictions.similar question 4H1 . whereas problem knew weight wanted predict height, know height want predict weight. ’ll start estimating linear model.Using model, can use tidybayes::add_predicted_draws() get model based predictions fill table.2. Howell1 dataset, consider people younger 13 years old. Estimate causal association age weight. Assume age influences weight two paths. First, age influences height, height influences weight. Second, age directly influences weight age-related changes muscle growth body proportions. implies causal model (DAG):Use linear regression estimate total (just direct) causal effect year growth weight. sure carefully consider priors. Try using prior predictive simulation assess imply.example, \\(H\\) pipe. Including \\(H\\) model close pipe, removing indirect effect age weight. Therefore, include age predictor weight model.prior distributions, can assume positive relationship age weight, ’ll use lognormal prior slope. intercept represents weight age 0 (.e., birth weight). typically around 3-4 kg, ’ll use normal prior mean 4 standard deviation 1. results wide range plausible regression lines:priors, can now estimate model.Visualizing posterior, see 89% compatibility interval causal effect age weight (b_age) 1.25 1.42 kg/year.3. Now suppose causal association age weight might different boys girls. Use single linear regression, categorical variable sex, estimate total causal effect age weight separately boys girls. girls boys differ? Provide one posterior contrasts summary.can create separate regression lines sex sex (factor) model formula. ’re now estimating intercept sex, use ~ 0 code indicate estimating global intercept, treating sex variable index variable. also use non-linear syntax {brms}. details approach, see Solomon Kurz’s section indicator variables.comparing two intercepts, see posterior distribution girls (b_a_sex1) slightly lower average distribution boys (b_a_sex2); however, lot overlap distributions. Similarly, posterior distributions slopes also show boys (b_b_sex2) slightly higher slope average girls (b_b_sex1).Let visualize regression lines look like. Overall, boys slightly higher intercept appear increase slightly higher rate. , distributions pretty similar overall.However, interested mean difference boys girls, difference means. estimate contrast need calculate posterior simulations sex. represents distribution expected weights individuals sex. can take difference two distributions. posterior distribution difference, shown figure . Overall, difference relatively small, appear increase age.4 - OPTIONAL CHALLENGE. data data(Oxboys) (rethinking package) growth records 26 boys measured 9 periods. want model growth. Specifically, model increments growth one period (Occasion data table) next. increment simply difference height one occasion height previous occasion. Since none boys shrunk study, growth increments greater zero. Estimate posterior distribution increments. Constrain distribution always positive—possible model think boys can shrink year year. Finally compute posterior distribution total growth 9 occasions.Let’s start looking data.first thing need convert individual height measures increments height. straightforward using dplyr::lag().Now want model increments always positive. easy setting prior, increments outcome, parameter model. can {brms} using lognormal family instead gaussian family used point. tell {brms} outcome normally distributed (.e., gaussian), rather lognormally distributed (.e., constrained positive). Specifically, model defined :\\[\\begin{align}\n  y_i &\\sim \\text{Lognormal}(\\alpha,\\sigma) \\\\\n  \\alpha &\\sim \\text{Normal}(0,0.3) \\\\\n  \\sigma &\\sim \\text{Exponential}(4)\n\\end{align}\\]priors? lot trial error prior predictive simulations. lognormal distribution super intuitive , played different priors found looked reasonable . code prior predictive simulation. prior expects growth increments around 1cm. 89% interval .41cm 1.7cm per occasion. seems reasonable, ’ll move forward.estimate model {brms} specifying family = lognormal.Using model, can examine posterior distribution incremental growth. average, expect boys grow 1.5cm per occasion.cumulative growth across occasions, need simulation 8 incremental changes sum . 9 occasions, expect boys grow 13cm, 89% highest density compatibility interval 8.6cm 18.5cm.","code":"\ndata(Howell1)\nhow_dat <- Howell1 %>%\n  filter(age >= 18) %>%\n  mutate(height_c = height - mean(height))\n\nw2h1 <- brm(weight ~ 1 + height_c, data = how_dat, family = gaussian,\n            prior = c(prior(normal(178, 20), class = Intercept),\n                      prior(normal(0, 10), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw2\", \"w2h1\"))\ntibble(individual = 1:3,\n       height = c(140, 160, 175)) %>%\n  mutate(height_c = height - mean(how_dat$height)) %>%\n  add_predicted_draws(w2h1) %>%\n  mean_qi(.prediction, .width = 0.89) %>%\n  mutate(range = glue(\"[{sprintf('%0.1f', .lower)}--\",\n                      \"{sprintf('%0.1f', .upper)}]\"),\n         .prediction = sprintf(\"%0.1f\", .prediction)) %>%\n  select(individual, height, exp = .prediction, range) %>%\n  kbl(align = \"c\", booktabs = TRUE,\n      col.names = c(\"Individual\", \"height\", \"expected weight\", \"89% interval\"))\nset.seed(123)\nn <- 50\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 4, 1),\n       beta = rlnorm(n, 0, 1)) %>%\n  expand(nesting(group, alpha, beta), age = 0:12) %>%\n  mutate(weight = alpha + beta * age) %>%\n  ggplot(aes(x = age, y = weight, group = group)) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 12, 2)) +\n  coord_cartesian(xlim = c(0, 12), ylim = c(0, 30)) +\n  labs(x = \"Age\", y = \"Weight\")\ndata(Howell1)\nkid_dat <- Howell1 %>%\n  filter(age < 13)\n\nw2h2 <- brm(weight ~ 1 + age, data = kid_dat, family = gaussian,\n            prior = c(prior(normal(4, 1), class = Intercept),\n                      prior(normal(0, 1), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw2\", \"w2h2\"))\ndraws <- gather_draws(w2h2, b_Intercept, b_age, sigma)\n\nmean_qi(draws, .width = 0.89)\n#> # A tibble: 3 × 7\n#>   .variable   .value .lower .upper .width .point .interval\n#>   <chr>        <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 b_age         1.34   1.25   1.42   0.89 mean   qi       \n#> 2 b_Intercept   7.00   6.42   7.59   0.89 mean   qi       \n#> 3 sigma         2.58   2.34   2.85   0.89 mean   qi\n\nggplot(draws, aes(x = .value, y = .variable)) +\n  stat_halfeye(.width = 0.89) +\n  labs(x = \"Parameter value\", y = \"Parameter\")\nkid_dat <- kid_dat %>% \n  mutate(sex = male + 1,\n         sex = factor(sex))\n\nw2h3 <- brm(\n  bf(weight ~ 0 + a + b * age,\n     a ~ 0 + sex,\n     b ~ 0 + sex,\n     nl = TRUE),\n  data = kid_dat, family = gaussian,\n  prior = c(prior(normal(4, 1), class = b, coef = sex1, nlpar = a),\n            prior(normal(4, 1), class = b, coef = sex2, nlpar = a),\n            prior(normal(0, 1), class = b, coef = sex1, nlpar = b),\n            prior(normal(0, 1), class = b, coef = sex2, nlpar = b),\n            prior(exponential(1), class = sigma)),\n  iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n  file = here(\"fits\", \"hw2\", \"w2h3\")\n)\ndraws <- gather_draws(w2h3, b_a_sex1, b_a_sex2, b_b_sex1, b_b_sex2, sigma)\n\nmean_qi(draws, .width = 0.89)\n#> # A tibble: 5 × 7\n#>   .variable .value .lower .upper .width .point .interval\n#>   <chr>      <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 b_a_sex1    6.50   5.79   7.21   0.89 mean   qi       \n#> 2 b_a_sex2    7.11   6.34   7.86   0.89 mean   qi       \n#> 3 b_b_sex1    1.36   1.25   1.47   0.89 mean   qi       \n#> 4 b_b_sex2    1.48   1.37   1.60   0.89 mean   qi       \n#> 5 sigma       2.49   2.26   2.74   0.89 mean   qi\n\nggplot(draws, aes(x = .value, y = .variable)) +\n  stat_halfeye(.width = 0.89) +\n  labs(x = \"Parameter value\", y = \"Parameter\")\nnew_age <- expand_grid(age = 0:12, sex = factor(c(1, 2)))\n\nall_lines <- new_age %>% \n  add_epred_draws(w2h3) %>% \n  ungroup() %>% \n  mutate(group = paste0(sex, \"_\", .draw))\n\nplot_lines <- all_lines %>%\n  filter(.draw %in% sample(unique(.data$.draw), size = 1000)) %>% \n  select(-.draw)\n\nanimate_lines <- all_lines %>%\n  filter(.draw %in% sample(unique(.data$.draw), size = 50))\n\nggplot(animate_lines, aes(x = age, y = .epred, color = sex, group = group)) +\n  geom_line(data = plot_lines, alpha = 0.01, show.legend = FALSE) + \n  geom_point(data = kid_dat, aes(x = age, y = weight, color = sex),\n             inherit.aes = FALSE) +\n  geom_line(alpha = 1, show.legend = FALSE, color = \"black\") +\n  scale_color_okabeito(labels = c(\"Girls\", \"Boys\")) +\n  scale_x_continuous(breaks = seq(0, 12, 2)) +\n  labs(x = \"Age\", y = \"Weight (kg)\", color = NULL) +\n  guides(color = guide_legend(override.aes = list(size = 3))) +\n  transition_states(.draw, 0, 1)\ncontrast <- new_age %>% \n  add_predicted_draws(w2h3) %>% \n  ungroup() %>% \n  select(age, sex, .draw, .prediction) %>% \n  mutate(sex = fct_recode(sex,\n                          \"Girls\" = \"1\",\n                          \"Boys\" = \"2\")) %>% \n  pivot_wider(names_from = sex, values_from = .prediction) %>% \n  mutate(diff = Boys - Girls)\n\nggplot(contrast, aes(x = age, y = diff)) +\n  stat_lineribbon(aes(fill_ramp = stat(.width)), .width = ppoints(50),\n                  fill = \"#009FB7\", show.legend = FALSE) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_fill_ramp_continuous(from = \"transparent\", range = c(1, 0)) +\n  scale_x_continuous(breaks = seq(0, 12, 2)) +\n  labs(x = \"Age\", y = \"Weight difference (kg; Boys-Girls)\")\ndata(Oxboys)\nhead(Oxboys)\n#>   Subject     age height Occasion\n#> 1       1 -1.0000    140        1\n#> 2       1 -0.7479    143        2\n#> 3       1 -0.4630    145        3\n#> 4       1 -0.1643    147        4\n#> 5       1 -0.0027    148        5\n#> 6       1  0.2466    150        6\nincrements <- Oxboys %>% \n  group_by(Subject) %>% \n  mutate(previous_height = lag(height),\n         change = height - previous_height) %>% \n  filter(!is.na(change)) %>% \n  ungroup()\n\nincrements\n#> # A tibble: 208 × 6\n#>    Subject     age height Occasion previous_height change\n#>      <int>   <dbl>  <dbl>    <int>           <dbl>  <dbl>\n#>  1       1 -0.748    143.        2            140.  2.90 \n#>  2       1 -0.463    145.        3            143.  1.40 \n#>  3       1 -0.164    147.        4            145.  2.30 \n#>  4       1 -0.0027   148.        5            147.  0.600\n#>  5       1  0.247    150.        6            148.  2.5  \n#>  6       1  0.556    152.        7            150.  1.5  \n#>  7       1  0.778    153.        8            152.  1.60 \n#>  8       1  0.994    156.        9            153.  2.5  \n#>  9       2 -0.748    139.        2            137.  2.20 \n#> 10       2 -0.463    140.        3            139.  1    \n#> # … with 198 more rows\nset.seed(123)\n\nn <- 1000\ntibble(alpha = rnorm(n, mean = 0, sd = 0.3),\n       sigma = rexp(n, rate = 4)) %>% \n  mutate(sim_change = rlnorm(n, meanlog = alpha, sdlog = sigma)) %>% \n  ggplot(aes(x = sim_change)) +\n  geom_density() +\n  labs(x = \"Prior Expectation for Incremental Growth (cm)\", y = \"Density\")\nw2h4 <- brm(change ~ 1, data = increments, family = lognormal,\n            prior = c(prior(normal(0, 0.3), class = Intercept),\n                      prior(exponential(4), class = sigma)),\n            iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw2\", \"w2h4\"))\nset.seed(234)\n\nas_draws_df(w2h4) %>% \n  mutate(post_change = rlnorm(n(), meanlog = b_Intercept, sdlog = sigma)) %>% \n  ggplot(aes(x = post_change)) +\n  geom_density() +\n  labs(x = \"Posterior Distribution for Incremental Growth (cm)\",\n       y = \"Density\")\nset.seed(345)\n\nas_draws_df(w2h4) %>% \n  mutate(all_change = map2_dbl(b_Intercept, sigma, ~sum(rlnorm(8, .x, .y)))) %>% \n  ggplot(aes(x = all_change)) +\n  stat_halfeye(aes(fill = stat(between(x, 8.6, 18.5))), color = NA) +\n  scale_fill_manual(values = c(\"#009FB7\", NA),\n                    labels = \"89% Compatibility Interval\",\n                    breaks = \"TRUE\", na.value = \"#F0F0F0\") +\n  labs(x = \"Posterior Distribution for Cumulative Growth (cm)\",\n       y = \"Density\", fill = NULL)"},{"path":"causes-confounds-colliders.html","id":"causes-confounds-colliders","chapter":"Week 3: Causes, Confounds & Colliders","heading":"Week 3: Causes, Confounds & Colliders","text":"third week covers Chapter 5 (Many Variables & Spurious Waffles) Chapter 6 (Haunted DAG & Causal Terror).","code":""},{"path":"causes-confounds-colliders.html","id":"lectures-2","chapter":"Week 3: Causes, Confounds & Colliders","heading":"3.1 Lectures","text":"Lecture 5:Lecture 6:","code":""},{"path":"causes-confounds-colliders.html","id":"exercises-2","chapter":"Week 3: Causes, Confounds & Colliders","heading":"3.2 Exercises","text":"","code":""},{"path":"causes-confounds-colliders.html","id":"chapter-5","chapter":"Week 3: Causes, Confounds & Colliders","heading":"3.2.1 Chapter 5","text":"5E1. linear models multiple linear regressions?\n\\[\\begin{align}\n(1)\\ \\ \\mu_i &= \\alpha + \\beta x_i \\\\\n(2)\\ \\ \\mu_i &= \\beta_x x_i + \\beta_z z_i \\\\\n(3)\\ \\ \\mu_i &= \\alpha + \\beta(x_i - z_i) \\\\\n(4)\\ \\ \\mu_i &= \\alpha + \\beta_x x_i + \\beta_z z_i\n\\end{align}\\]Numbers 2 4 multiple regressions. Number 1 contains one predictor variable. Number 3, although two variables appear model, also uses difference \\(x\\) \\(z\\) single predictor. numbers 2 4 contain multiple predictor variables.5E2. Write multiple regression evaluate claim: Animal diversity linearly related latitude, controlling plant diversity. just need write model definition.denote animal diversity \\(\\), latitude \\(L\\), plant diversity \\(P\\). linear model can defined follows:\\[\\begin{align}\n  A_i &\\sim \\text{Normal}(\\mu, \\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_L L_i + \\beta_P P_i\n\\end{align}\\]5E3. Write multiple regression evaluate claim: Neither amount funding size laboratory good predictor time PhD degree; together variables positively associated time degree. Write model definition indicate side zero slope parameter .denote time PhD degree \\(T\\), amount funding \\(F\\), size laboratory \\(L\\). model defined :\\[\\begin{align}\n  T_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_F F_i + \\beta_L L_i\n\\end{align}\\]\\(\\beta_F\\) \\(\\beta_L\\) positive. order funding lab size show positive relationship together relationship separately, two variables need negatively correlated (.e., large labs less funding per student small labs funding per student). positively associated outcome, negatively associated real world. Thus, analyzing variables isolation may mask positive relationship.5E4. Suppose single categorical predictor 4 levels (unique values), labeled , B, C D. Let \\(A_i\\) indicator variable 1 case \\(\\) category \\(\\). Also suppose \\(B_i\\), \\(C_i\\), \\(D_i\\) categories. Now following linear models inferentially equivalent ways include categorical variable regression? Model inferentially equivalent ’s possible compute one posterior distribution posterior distribution another model.\n\\[\\begin{align}\n(1)\\ \\ \\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_D D_i \\\\\n(2)\\ \\ \\mu_i &= \\alpha + \\beta_A A_i + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\\\\n(3)\\ \\ \\mu_i &= \\alpha + \\beta_B B_i + \\beta_C C_i + \\beta_D D_i \\\\\n(4)\\ \\ \\mu_i &= \\alpha_A A_i + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i \\\\\n(5)\\ \\ \\mu_i &= \\alpha_A (1 - B_i - C_i - D_i) + \\alpha_B B_i + \\alpha_C C_i + \\alpha_D D_i\n\\end{align}\\]Numbers 1, 3, 4, 5 inferentially equivalent. Numbers 1 3 use 3 4 indicator variables, meaning can always calculate 4th three estimated intercept. Number 4 equivalent index variable approach, inferentially equivalent indicator variable approach. Finally, Number 5 mathematically equivalent Number 4, assuming observation can belong 1 4 groups.Number 2 valid model representation, model contain \\(k-1\\) indicator variables intercept (using indicator variable approach). 4 included definition Number 2, expect estimation problems (model estimate ).5M1. Invent example spurious correlation. outcome variable correlated predictor variables. predictors entered model, correlation outcome one predictors mostly vanish (least greatly reduced).(simulated) example, ’ll predict ice cream sales temperature number shark attacks. First, ’ll fit two bivariate regressions, mod_t mod_s, include temperature shark attacks predictors, respectively. ’ll estimate multivariate regression, mod_all, includes predictors.plot shows posterior distributions \\(\\beta\\) coefficients temperature shark attacks. expected, positive relationship ice cream sales bivariate models. However, predictors included mod_all, posterior distribution b_shark moves zero, whereas distribution b_temp remains basically . Thus, relationship ice cream sales shark attacks spurious correlation, informed temperature.5M2. Invent example masked relationship. outcome variable correlated predictor variables, opposite directions. two predictor variables correlated one another.(also simulated) example, ’ll predict academic test score amount instruction student received number days missed class. First, ’ll fit two bivariate regressions, mod_i mod_d, include instruction days away predictors, respectively. ’ll estimate multivariate regression, mod_test, includes predictors.figure shows posterior distributions \\(\\beta\\) parameters models. can see full model, mod_test, b_instruction gets positive b_days_away gets negative.5M3. sometimes observed best predictor fire risk presence firefighters—State localities many firefighters also fires. Presumably firefighters cause fires. Nevertheless, spurious correlation. Instead fires cause firefighters. Consider reversal causal inference context divorce marriage data. might high divorce rate cause higher marriage rate? Can think way evaluate relationship, using multiple regression?high divorce rate means people available population single-people available marry. Additionally, people may getting divorced specific purpose marrying someone else. evaluate , add marriage number, “re-marry” indicator. expect coefficient marriage rate get closer zero predictor added model.5M4. divorce data, States high numbers members Church Jesus Christ Latter-day Saints (LDS) much lower divorce rates regression models expected. Find list LDS population State use numbers predictor variable, predicting divorce rate using marriage rate, median age marriage, percent LDS population (possibly standardized). may want consider transformations raw percent LDS variable.can pull LDS membership official LDS website, state populations United States census website. Conveniently, pulled together World Population Review (copied January 4, 2022). data saved data/lds-data-2021.csv. can combine existing divorce data data(\"WaffleDivorce\"). analysis, ’ll use LDS membership per capita. Specifically, number LDS members per 100,000 state’s population. standardized, along predictor variables, done chapter examples.’re now ready estimate model.Finally, can visualize estimates. intercept coefficients Marriage MedianAgeMarriage nearly identical model m5.3 text. Thus, appears new predictor, LDS per capita, contributing unique information. expected, higher population LDS members state associated lower divorce rate.5M5. One way reason multiple causation hypotheses imagine detailed mechanisms predictor variables may influence outcomes. example, sometimes argued price gasoline (predictor variable) positively associated lower obesity rates (outcome variable). However, least two important mechanisms price gas reduce obesity. First, lead less driving therefore exercise. Second, lead less driving, leads less eating , leds less consumption huge restaurant meals. Can outline one multiple regressions address two mechanisms? Assume can predictor data need.Let’s assume four variables: rate obesity (\\(O\\)), price gasoline (\\(G\\)), amount driving (\\(D\\)), amount exercise (\\(E\\)), rate eating restaurants (\\(R\\)). Given variables, can outline first proposed mechanism implies:\\(D\\) negatively associated \\(P\\)\\(E\\) negatively associated \\(D\\)\\(O\\) negatively associated \\(E\\)chain causality, outcome input next chain. second mechanism similar:\\(D\\) negatively associated \\(P\\)\\(R\\) positively associated \\(D\\)\\(O\\) positively associated \\(R\\)5H1. divorce example, suppose DAG : \\(M \\rightarrow \\rightarrow D\\). implied conditional independencies graph? data consistent ?can use {dagitty} (Textor et al., 2021) see implied conditional independencies. code see DAG implies divorce rate independent marriage rate conditional median age marriage.shown model m5.3, data consistent implied conditional independency. Thus data consistent multiple DAGs. Specifically, support Markov Equivalent DAGs. second DAG list, D <- -> M, DAG investigated text.5H2. Assuming DAG divorce example indeed \\(M \\rightarrow \\rightarrow D\\), fit new model use estimate counterfactual effect halving State’s marriage rate \\(M\\). Using counterfactual example chapter (starting page 140) template.First, ’ll estimate model consistent DAG. two regressions , ’ll use {brms}’s multivariate syntax (.e., bf(); see ).can simulate counterfactuals plot {ggplot2}. see negative relationship marriage rate age. , marriage rate increases, age marriage decreases average. Specifically, \\(\\) moves -0.7 units every 1 unit increase \\(M\\), reflected estimate A_M model summary . counterfactual \\(M\\) \\(D\\), see positive relationship. relationship \\(M\\) \\(\\) \\(\\) \\(D\\) negative. words, \\(M\\) increases \\(\\) decreases, \\(\\) decreases \\(D\\) increases. every one unit increase \\(M\\), expect 0.4 unit increase \\(D\\). can also derived multiplying slope coefficients model summary (.e., −0.56 × −0.69 ≈ 0.4).question specifically asks counterfactual effect halving state’s marriage rate. dependent original marriage rate . problem, ’ll use average marriage rate across states, 20. Half 10, standardized units used fit model -2.66. Looking counterfactual plot, moving \\(M\\) 0 \\(M\\) -2.66 result decrease divorce rate 1 standard deviation. can calculate directly using code well. also see estimated difference -1.03 standard deviations, 89% posterior interval -3.14 1.08.5H3. Return milk energy model, m5.7. Suppose true causal relationship among variables :Now compute counterfactual effect \\(K\\) doubling \\(M\\). need account direct indirect paths causation. Use counterfactual example chapter (starting page 140) template.First, need estimate model.Now can calculate counterfactual. \\(M\\) represents mass, ’ll use evenly spaced values 0.5 80, captures range observed mass data. ’ll predict new values \\(K\\) masses.can see plot, mass log-transformed model, non-linear relationship manipulated mass counterfactual. Thus, impact doubling mass depends original mass. previous example, ’ll use average mass (≈15kg) baseline. Thus standardized log units used estimate model, ’re comparing \\(M\\) value 0.746 1.154. code shows doubling mass small effect \\(K\\) quick wide compatibility interval.5H4. open practice problem engage imagination. divorce data, States southern United States many highest divorce rates. Add South indicator variable analysis. First, draw one DAGs represent ideas Southern American culture might influence three variables (\\(D\\), \\(M\\), \\(\\)). list testable implications DAGs, , fit one models evaluate implications. think influence “Southerness” ?First, let’s draw DAG using {ggdag} (Barrett, 2021).proposed model, hypothesizing southerness influences median age marriage marriage rate. can use {dagitty} view testable implications:Given DAG, divorce rate independent southerness condition median age marriage marriage rate. Let’s estimate model test . ’ll include median age marriage (\\(\\)), marriage rate (\\(M\\)), southerness (\\(S\\)) model. \\(\\) \\(M\\) included, expect coefficient \\(S\\) zero, implied conditional independency holds.Now can look posterior distribution \\(\\beta\\) coefficient southerness.effect southerness close zero, slightly larger might expected. Thus, preliminary evidence DAG drawn may accurate. However, evidence strong enough refute proposed DAG either.","code":"\nset.seed(2022)\nn <- 100\ntemp <- rnorm(n)\nshark <- rnorm(n, temp)\nice_cream <- rnorm(n, temp)\n\nspur_exp <- tibble(ice_cream, temp, shark) %>%\n  mutate(across(everything(), standardize))\n\nmod_t <- brm(ice_cream ~ 1 + temp, data = spur_exp, family = gaussian,\n             prior = c(prior(normal(0, 0.2), class = Intercept),\n                       prior(normal(0, 0.5), class = b),\n                       prior(exponential(1), class = sigma)),\n             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n             file = here(\"fits\", \"chp5\", \"b5m1-t\"))\n\nmod_s <- brm(ice_cream ~ 1 + shark, data = spur_exp, family = gaussian,\n             prior = c(prior(normal(0, 0.2), class = Intercept),\n                       prior(normal(0, 0.5), class = b),\n                       prior(exponential(1), class = sigma)),\n             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n             file = here(\"fits\", \"chp5\", \"b5m1-s\"))\n\nmod_all <- brm(ice_cream ~ 1 + temp + shark, data = spur_exp, family = gaussian,\n               prior = c(prior(normal(0, 0.2), class = Intercept),\n                         prior(normal(0, 0.5), class = b),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp5\", \"b5m1-all\"))\nbind_rows(\n  spread_draws(mod_t, b_temp) %>%\n    mutate(model = \"mod_t\"),\n  spread_draws(mod_s, b_shark) %>%\n    mutate(model = \"mod_s\"),\n  spread_draws(mod_all, b_temp, b_shark) %>%\n    mutate(model = \"mod_all\")\n) %>%\n  pivot_longer(cols = starts_with(\"b_\"), names_to = \"parameter\",\n               values_to = \"value\") %>%\n  drop_na(value) %>%\n  mutate(model = factor(model, levels = c(\"mod_t\", \"mod_s\", \"mod_all\")),\n         parameter = factor(parameter, levels = c(\"b_temp\", \"b_shark\"))) %>%\n  ggplot(aes(x = value, y = fct_rev(model))) +\n  facet_wrap(~parameter, nrow = 1) +\n  stat_halfeye(.width = 0.89) +\n  labs(x = \"Parameter Value\", y = \"Model\")\nset.seed(2020)\nn <- 100\nu <- rnorm(n)\ndays_away <- rnorm(n, u)\ninstruction <- rnorm(n, u)\ntest_score <- rnorm(n, instruction - days_away)\n\nmask_exp <- tibble(test_score, instruction, days_away) %>%\n  mutate(across(everything(), standardize))\n\nmod_i <- brm(test_score ~ 1 + instruction, data = mask_exp, family = gaussian,\n             prior = c(prior(normal(0, 0.2), class = Intercept),\n                       prior(normal(0, 0.5), class = b),\n                       prior(exponential(1), class = sigma)),\n             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n             file = here(\"fits\", \"chp5\", \"b5m2-i\"))\n\nmod_d <- brm(test_score ~ 1 + days_away, data = mask_exp, family = gaussian,\n             prior = c(prior(normal(0, 0.2), class = Intercept),\n                       prior(normal(0, 0.5), class = b),\n                       prior(exponential(1), class = sigma)),\n             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n             file = here(\"fits\", \"chp5\", \"b5m2-d\"))\n\nmod_test <- brm(test_score ~ 1 + instruction + days_away, data = mask_exp,\n                family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp5\", \"b5m2-test\"))\nbind_rows(\n  spread_draws(mod_i, b_instruction) %>%\n    mutate(model = \"mod_i\"),\n  spread_draws(mod_d, b_days_away) %>%\n    mutate(model = \"mod_d\"),\n  spread_draws(mod_test, b_instruction, b_days_away) %>%\n    mutate(model = \"mod_test\")\n) %>%\n  pivot_longer(cols = starts_with(\"b_\"), names_to = \"parameter\",\n               values_to = \"value\") %>%\n  drop_na(value) %>%\n  mutate(model = factor(model, levels = c(\"mod_i\", \"mod_d\", \"mod_test\")),\n         parameter = factor(parameter, levels = c(\"b_instruction\",\n                                                  \"b_days_away\"))) %>%\n  ggplot(aes(x = value, y = fct_rev(model))) +\n  facet_wrap(~parameter, nrow = 1) +\n  stat_halfeye(.width = 0.89) +\n  labs(x = \"Parameter Value\", y = \"Model\")\nlds <- read_csv(here(\"data\", \"lds-data-2021.csv\"),\n                col_types = cols(.default = col_integer(),\n                                 state = col_character())) %>%\n  mutate(lds_prop = members / population,\n         lds_per_capita = lds_prop * 100000)\n\ndata(\"WaffleDivorce\")\nlds_divorce <- WaffleDivorce %>%\n  as_tibble() %>%\n  select(Location, Divorce, Marriage, MedianAgeMarriage) %>%\n  left_join(select(lds, state, lds_per_capita),\n            by = c(\"Location\" = \"state\")) %>%\n  mutate(lds_per_capita = log(lds_per_capita)) %>%\n  mutate(across(where(is.numeric), standardize)) %>% \n  filter(!is.na(lds_per_capita))\n\nlds_divorce\n#> # A tibble: 49 × 5\n#>    Location    Divorce Marriage MedianAgeMarriage lds_per_capita\n#>    <chr>         <dbl>    <dbl>             <dbl>          <dbl>\n#>  1 Alabama       1.65    0.0226            -0.606         -0.423\n#>  2 Alaska        1.54    1.55              -0.687          1.21 \n#>  3 Arizona       0.611   0.0490            -0.204          1.42 \n#>  4 Arkansas      2.09    1.66              -1.41          -0.123\n#>  5 California   -0.927  -0.267              0.600          0.409\n#>  6 Colorado      1.05    0.892             -0.285          0.671\n#>  7 Connecticut  -1.64   -0.794              1.24          -0.909\n#>  8 Delaware     -0.433   0.786              0.439         -0.693\n#>  9 Florida      -0.652  -0.820              0.278         -0.466\n#> 10 Georgia       0.995   0.523             -0.124         -0.375\n#> # … with 39 more rows\nlds_mod <- brm(Divorce ~ 1 + Marriage + MedianAgeMarriage + lds_per_capita,\n               data = lds_divorce, family = gaussian,\n               prior = c(prior(normal(0, 0.2), class = Intercept),\n                         prior(normal(0, 0.5), class = b, coef = Marriage),\n                         prior(normal(0, 0.5), class = b, coef = MedianAgeMarriage),\n                         prior(normal(0, 0.5), class = b, coef = lds_per_capita),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp5\", \"b5m4\"))\nspread_draws(lds_mod, `b_.*`, regex = TRUE) %>%\n  pivot_longer(starts_with(\"b_\"), names_to = \"parameter\",\n               values_to = \"value\") %>%\n  ggplot(aes(x = value, y = parameter)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = \"Parameter Value\", y = \"Parameter\")\nlibrary(dagitty)\n\nmad_dag <- dagitty(\"dag{M -> A -> D}\")\nimpliedConditionalIndependencies(mad_dag)\n#> D _||_ M | A\nequivalentDAGs(mad_dag)\n#> [[1]]\n#> dag {\n#> A\n#> D\n#> M\n#> A -> D\n#> M -> A\n#> }\n#> \n#> [[2]]\n#> dag {\n#> A\n#> D\n#> M\n#> A -> D\n#> A -> M\n#> }\n#> \n#> [[3]]\n#> dag {\n#> A\n#> D\n#> M\n#> A -> M\n#> D -> A\n#> }\ndat <- WaffleDivorce %>%\n  select(A = MedianAgeMarriage,\n         D = Divorce,\n         M = Marriage) %>%\n  mutate(across(everything(), standardize))\n\nd_model <- bf(D ~ 1 + A)\na_model <- bf(A ~ 1 + M)\n\nb5h2 <- brm(d_model + a_model + set_rescor(FALSE),\n            data = dat, family = gaussian,\n            prior = c(prior(normal(0, 0.2), class = Intercept, resp = D),\n                      prior(normal(0, 0.5), class = b, resp = D),\n                      prior(exponential(1), class = sigma, resp = D),\n                      \n                      prior(normal(0, 0.2), class = Intercept, resp = A),\n                      prior(normal(0, 0.5), class = b, resp = A),\n                      prior(exponential(1), class = sigma, resp = A)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp5\", \"b5h2\"))\n\nsummary(b5h2)\n#>  Family: MV(gaussian, gaussian) \n#>   Links: mu = identity; sigma = identity\n#>          mu = identity; sigma = identity \n#> Formula: D ~ 1 + A \n#>          A ~ 1 + M \n#>    Data: dat (Number of observations: 50) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> D_Intercept     0.00      0.10    -0.20     0.20 1.00    10726     6132\n#> A_Intercept    -0.00      0.09    -0.17     0.18 1.00    10190     6073\n#> D_A            -0.56      0.11    -0.78    -0.34 1.00     9422     6605\n#> A_M            -0.69      0.10    -0.89    -0.49 1.00     8993     5538\n#> \n#> Family Specific Parameters: \n#>         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma_D     0.82      0.09     0.67     1.01 1.00    10660     5820\n#> sigma_A     0.71      0.07     0.58     0.88 1.00    10351     6034\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nas_draws_df(b5h2) %>%\n  as_tibble() %>%\n  select(.draw, b_D_Intercept:sigma_A) %>% \n  expand(nesting(.draw, b_D_Intercept, b_A_Intercept, b_D_A, b_A_M,\n                 sigma_D, sigma_A),\n         m = seq(from = -2, to = 2, length.out = 30)) %>%\n  mutate(a_sim = rnorm(n(), mean = b_A_Intercept + b_A_M * m, sd = sigma_A),\n         d_sim = rnorm(n(), mean = b_D_Intercept + b_D_A * a_sim, sd = sigma_D)) %>%\n  pivot_longer(ends_with(\"_sim\"), names_to = \"name\", values_to = \"value\") %>%\n  group_by(m, name) %>%\n  mean_qi(value, .width = c(0.89)) %>%\n  ungroup() %>%\n  mutate(name = case_when(name == \"a_sim\" ~ \"Counterfactual M &rarr; A\",\n                          TRUE ~ \"Counterfactual M &rarr; A &rarr; D\")) %>%\n  ggplot(aes(x = m, y = value, ymin = .lower, ymax = .upper)) +\n  facet_wrap(~name, nrow = 1) +\n  geom_smooth(stat = \"identity\") +\n  labs(x = \"Manipulated M\", y = \"Counterfactual\")\nas_draws_df(b5h2) %>% \n  mutate(a_avg = rnorm(n(), b_A_Intercept + b_A_M * 0, sigma_A),\n         a_hlf = rnorm(n(), b_A_Intercept + b_A_M * -2.66, sigma_A),\n         d_avg = rnorm(n(), b_D_Intercept + b_D_A * a_avg, sigma_D),\n         d_hlf = rnorm(n(), b_D_Intercept + b_D_A * a_hlf, sigma_D),\n         diff = d_hlf - d_avg) %>% \n  mean_hdi(diff, .width = 0.89)\n#> # A tibble: 1 × 6\n#>    diff .lower .upper .width .point .interval\n#>   <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 -1.03  -3.11   1.09   0.89 mean   hdi\ndata(milk)\n\nnew_milk <- milk %>%\n  select(kcal.per.g,\n         neocortex.perc,\n         mass) %>%\n  drop_na(everything()) %>%\n  mutate(log_mass = log(mass),\n         K = standardize(kcal.per.g),\n         N = standardize(neocortex.perc),\n         M = standardize(log_mass))\n\nK_model <- bf(K ~ 1 + M + N)\nN_model <- bf(N ~ 1 + M)\n\nb5h3 <- brm(K_model + N_model + set_rescor(FALSE),\n            data = new_milk, family = gaussian,\n            prior = c(prior(normal(0, 0.2), class = Intercept, resp = K),\n                      prior(normal(0, 0.5), class = b, resp = K),\n                      prior(exponential(1), class = sigma, resp = K),\n                      \n                      prior(normal(0, 0.2), class = Intercept, resp = N),\n                      prior(normal(0, 0.5), class = b, resp = N),\n                      prior(exponential(1), class = sigma, resp = N)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp5\", \"b5h3\"))\n\nsummary(b5h3)\n#>  Family: MV(gaussian, gaussian) \n#>   Links: mu = identity; sigma = identity\n#>          mu = identity; sigma = identity \n#> Formula: K ~ 1 + M + N \n#>          N ~ 1 + M \n#>    Data: new_milk (Number of observations: 17) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>             Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> K_Intercept     0.00      0.14    -0.27     0.28 1.00     8841     5684\n#> N_Intercept     0.00      0.13    -0.25     0.26 1.00     9335     5242\n#> K_M            -0.68      0.27    -1.17    -0.11 1.00     5097     5278\n#> K_N             0.57      0.26     0.02     1.05 1.00     4924     5189\n#> N_M             0.66      0.17     0.32     0.98 1.00     8326     5454\n#> \n#> Family Specific Parameters: \n#>         Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma_K     0.81      0.17     0.56     1.21 1.00     6085     5711\n#> sigma_N     0.72      0.14     0.50     1.05 1.00     8236     5409\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nmilk_cf <- as_draws_df(b5h3) %>%\n  as_tibble() %>% \n  select(.draw, b_K_Intercept:sigma_N) %>% \n  expand(nesting(.draw, b_K_Intercept, b_N_Intercept, b_K_M, b_K_N, b_N_M,\n                 sigma_K, sigma_N),\n         mass = seq(from = 0.5, to = 80, by = 0.5)) %>%\n  mutate(log_mass = log(mass),\n         M = (log_mass - mean(new_milk$log_mass)) / sd(new_milk$log_mass),\n         n_sim = rnorm(n(), mean = b_N_Intercept + b_N_M * M, sd = sigma_N),\n         k_sim = rnorm(n(), mean = b_K_Intercept + b_K_N * n_sim + b_K_M * M,\n                       sd = sigma_K)) %>%\n  pivot_longer(ends_with(\"_sim\"), names_to = \"name\", values_to = \"value\") %>%\n  group_by(mass, name) %>%\n  mean_qi(value, .width = c(0.89)) %>%\n  ungroup() %>%\n  filter(name == \"k_sim\") %>%\n  mutate(name = case_when(name == \"n_sim\" ~ \"Counterfactual effect M on N\",\n                          TRUE ~ \"Total Counterfactual effect of M on K\"))\n\nggplot(milk_cf, aes(x = mass, y = value, ymin = .lower, ymax = .upper)) +\n  geom_smooth(stat = \"identity\") +\n  labs(x = \"Manipulated Mass\", y = \"Counterfactual K\")\n(log(c(15, 30)) - mean(log(milk$mass))) / sd(log(milk$mass)) \n#> [1] 0.746 1.154\n\nas_draws_df(b5h3) %>% \n  mutate(n_avg = rnorm(n(), b_N_Intercept + b_N_M * 0.746, sigma_N),\n         n_dbl = rnorm(n(), b_N_Intercept + b_N_M * 1.154, sigma_N),\n         k_avg = rnorm(n(), b_K_Intercept + b_K_M * 0.746 + b_K_N * n_avg,\n                       sigma_K),\n         k_dbl = rnorm(n(), b_K_Intercept + b_K_M * 1.154 + b_K_N * n_dbl,\n                       sigma_K),\n         diff = k_dbl - k_avg) %>% \n  median_hdi(diff, .width = 0.89)\n#> # A tibble: 1 × 6\n#>     diff .lower .upper .width .point .interval\n#>    <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>    \n#> 1 -0.151  -2.45   1.82   0.89 median hdi\ndag_coords <-\n  tibble(name = c(\"S\", \"A\", \"M\", \"D\"),\n         x = c(1, 1, 2, 3),\n         y = c(3, 1, 2, 1))\n\ndagify(D ~ A + M,\n       M ~ A + S,\n       A ~ S,\n       coords = dag_coords) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\", size = 10) +\n  geom_dag_edges(edge_color = \"black\", edge_width = 2,\n                 arrow_directed = grid::arrow(length = grid::unit(15, \"pt\"),\n                                              type = \"closed\")) +\n  theme_void()\ndiv_dag <- dagitty(\"dag{S -> M -> D; S -> A -> D; A -> M}\")\nimpliedConditionalIndependencies(div_dag)\n#> D _||_ S | A, M\ndata(\"WaffleDivorce\")\n\nsouth_divorce <- WaffleDivorce %>%\n  as_tibble() %>%\n  select(D = Divorce,\n         A = MedianAgeMarriage,\n         M = Marriage,\n         S = South) %>%\n  drop_na(everything()) %>%\n  mutate(across(where(is.double), standardize))\n\nb5h4 <- brm(D ~ A + M + S, data = south_divorce, family = gaussian,\n            prior = c(prior(normal(0, 0.2), class = Intercept),\n                      prior(normal(0, 0.5), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp5\", \"b5h4\"))\nspread_draws(b5h4, b_S) %>%\n  ggplot(aes(x = b_S)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = expression(beta[S]), y = \"Density\")"},{"path":"causes-confounds-colliders.html","id":"chapter-6","chapter":"Week 3: Causes, Confounds & Colliders","heading":"3.2.2 Chapter 6","text":"6E1. List three mechanisms multiple regression can produce false inferences causal effects.Three mechanisms :Multicollinearity. occurs two variables strongly assocaited, conditional variables model.Post-treatment Bias. occurs inclusion variable “blocks” effect another.Collider Bias. occurs two variables unrelated , related third variable. inclusion third variable creates statistical association model first two.6E2. one mechanisms previous problem, provide example choice, perhaps research.educational research, often interested instructional interventions. example, might provide teacher professional development (\\(D\\)). expect professional development positive impact instruction (\\(\\)) teacher provides, turn improve student performance (\\(P\\)). However, student performance also influenced pre-existing knowledge state (\\(K\\)) teacher’s students. interested estimating effect \\(D\\) \\(\\), inclusion \\(P\\) model example post-treatment bias, quality instruction (influencing performance, \\(P\\)) masks effect professional development shown DAG.6E3. List four elemental confounds. Can explain conditional dependencies ?Fork. classic “confounder” example. variable \\(Z\\) common cause \\(X\\) \\(Y\\), resulting correlation \\(X\\) \\(Y\\). words, \\(X\\) independent \\(Y\\), conditional \\(Z\\), , math notation, \\(X \\!\\perp\\!\\!\\!\\perp Y | Z\\).Pipe. variable \\(X\\) influences, another variable \\(Z\\), turn influences outcome \\(Y\\). good example post-treatment bias described . treatment (\\(X\\)) meant influence outcome (\\(Y\\)), intermediary (\\(Z\\)). Including \\(X\\) \\(Z\\) can make appear though \\(X\\) effect \\(Y\\). Thus, fork, \\(X\\) independent \\(Y\\), conditional \\(Z\\). math : \\(X \\!\\perp\\!\\!\\!\\perp Y | Z\\).Collider. Unlike fork pipe, collider induces correlation otherwise unassociated variables. Thus, \\(X\\) independent \\(Y\\), conditional \\(Z\\). math notation, can written \\(X \\\\!\\perp\\!\\!\\!\\perp Y|Z\\).Descendant. final case, descendant, \\(D\\), influenced another variable DAG, case \\(Z\\). Conditioning descendant impact partially conditioning parent variable. example, \\(Z\\) collider. Thus, kept \\(Z\\) model, included \\(D\\), get partial collider bias, \\(D\\) contains information \\(Z\\). much bias introduced depends strength relationship descendant parent.6E4. biased sample like conditioning collider? Think example open chapter.collider, association introduced two variables third added. biased sample like un-measured collider. example started chapter, association trustworthiness newsworthiness. However, selection status added model, appears negative association. Now imagine data funded proposals. , implicit conditioning selection status, funded proposals. potentially nefarious, collider isn’t explicitly data, variance (.e., proposals data selection score 1).6M1. Modify DAG page 186 include variable \\(V\\), unobserved cause \\(C\\) \\(Y\\): \\(C \\leftarrow V \\rightarrow Y\\). Reanalyze DAG. many paths connect \\(X\\) \\(Y\\)? must closed? variables condition now?new DAG looks like :now 5 paths \\(X\\) \\(Y\\). three paths original DAG, 2 new paths introduced \\(V\\).\\(X \\rightarrow Y\\)\\(X \\leftarrow U \\rightarrow B \\leftarrow C \\rightarrow Y\\)\\(X \\leftarrow U \\rightarrow B \\leftarrow C \\leftarrow V \\rightarrow Y\\)\\(X \\leftarrow U \\leftarrow \\rightarrow C \\rightarrow Y\\)\\(X \\leftarrow U \\leftarrow \\rightarrow C \\leftarrow V \\rightarrow Y\\)want keep path 1 open, causal path interest. Paths 2 3 already closed, \\(B\\) collider. Paths 4 5 open \\(\\) fork. original DAG, close either \\(\\) \\(C\\); however, new DAG, \\(C\\) collider. Therefore, conditioned \\(C\\), ’d opening new path \\(V\\). Therefore, want make inferences causal effect \\(X\\) \\(Y\\), must condition \\(\\) (assuming DAG correct). can confirm using dagitty::adjustmentSets().6M2. Sometimes, order avoid multicollinearity, people inspect pairwise correlations among predictors including model. bad procedure, matters conditional association, association variables included model. highlight , consider DAG \\(X \\rightarrow Z \\rightarrow Y\\). Simulate data DAG correlation \\(X\\) \\(Z\\) large. include model prediction \\(Y\\). observe multicollinearity? ? different legs example chapter?First, ’ll generate data confirm x z highly correlated simulation.generated data prettyNum(nrow(dat), big.mark = \",\") observations, correlation fmt_prop(sim_cor, 3) x z. Now let’s estimate model regressing y x z.model summary, can see x z estimates relatively narrow posterior distributions. contrast example legs chapter, estimated standard deviations \\(beta\\) parameters much larger magnitudes parameters, posterior distributions significant overlap. Thus, appear though observing multicollinearity.due causal model gave rise (simulated) data. legs example chapter, legs predicted height (left DAG ). example, \\(Z\\) predicts outcome. DAG language, \\(Z\\) pipe. Therefore, model estimated, golem looking \\(X\\) tells us, conditional \\(Z\\). answer case “much” \\(X\\) \\(Z\\) highly correlated, posterior \\(X\\) centered zero. leg model condition either predictors, direct paths outcome variable. Thus, whether model multicollinearity depends pairwise relationship, also causal model.\n6M3. Learning analyze DAGs requires practice. four DAGs , state variables, , must adjust (condition ) estimate total causal influence \\(X\\) \\(Y\\).\nUpper Left: two back-door paths \\(X\\) \\(Y\\): \\(X \\leftarrow Z \\rightarrow Y\\) \\(X \\leftarrow Z \\leftarrow \\rightarrow Y\\). first path \\(Z\\) fork, second, \\(Z\\) pipe \\(\\) fork. Thus, cases, conditioning \\(Z\\) close path. can confirm dagitty::adjustmentSets().Upper Right: , two paths \\(X\\) \\(Y\\): \\(X \\rightarrow Z \\rightarrow Y\\) \\(X \\rightarrow Z \\leftarrow \\rightarrow Y\\). first path indirect causal effect \\(X\\) \\(Y\\), included estimating total causal effect \\(X\\) \\(Y\\). second path, \\(Z\\) collider, path already closed therefore action needed. Thus, adjustments needed estimate total effect \\(X\\) \\(Y\\).Lower Left: two paths \\(X\\) \\(Y\\): \\(X \\leftarrow \\rightarrow Z \\leftarrow Y\\) \\(X \\rightarrow Z \\leftarrow Y\\). one causal path \\(X\\) \\(Y\\), direct path. back-door paths, \\(Z\\) collider paths closed unless include , additional variables need added model.Lower Right: Finally, DAG also two causal paths \\(X\\) \\(Y\\). direct effect, indirect effect \\(Z\\). one additional path, \\(X \\leftarrow \\rightarrow Z \\rightarrow Y\\). Conditioning \\(\\) close path.6H1. Use Waffle House data, data(WaffleDivorce), find total causal influence number Waffle Houses divorce rate. Justify model models causal graph.problem, ’ll use DAG given chapter example:order estimate total causal effect Waffle Houses (\\(W\\)) divorce rate (\\(D\\)), condition either \\(S\\) \\(\\) \\(M\\). simplicity, ’ll condition \\(S\\).can use brms::brm() estimate model.Finally, can see causal estimate Waffle Houses divorce rate looking posterior distribution b_W parameter. , can see estimate small, indicating number Waffle Houses little causal impact divorce rate state.6H2. Build series models test implied conditional independencies causal graph used previous problem. tests fail, think graph needs ammended? graph need fewer arrows? Feel free nominate variables aren’t data.First need get conditional independencies implied DAG.three models need estimate evaluate implied conditional independencies. one can estimated brms::brm().Finally, can example posterior distributions implied conditional independencies. implied conditional independency tested first model, \\(\\!\\perp\\!\\!\\!\\perp W | S\\), appears met, \\(beta_W\\) coefficient model centered zero. true third implied conditional independency, \\(M \\!\\perp\\!\\!\\!\\perp W | S\\). second implied conditional independency, \\(D \\!\\perp\\!\\!\\!\\perp S | , M, W\\), less clear, posterior distribution overlaps zero, indicate slightly positive relationship divorce rate state’s “southern” status, even adjusting median age marriage, marriage rate, number waffle houses. likely variables missing model related divorce rate also southerness. example, religiosity, family size, education plausibly impact divorce rates show regional differences United States.three problems based data. data data(foxes) 116 foxes 30 different urban groups England. foxes like street gangs. Group size varies 2 8 individuals. group maintains urban territory. territories larger others. area variable encodes information. territories also avgfood others. want model weight fox. problems , assume following DAG:6H3. Use model infer total causal influence area weight. increasing area available fox make heavier (healthier)? might want standardize variables. Regardless, use prior predictive simulation show model’s prior predictions stay within possible outcome range.First, let’s load data standardize variables.first question, back-door paths area weight. arrows represent indirect effects area weight. Therefore, want condition avgfood groupsize, close one indirect paths needed estimate total causal effect. can confirm dagitty::adjustmentSets().Thus, model question can written :\\[\\begin{align}\n  W_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_A \\\\\n  \\alpha &\\sim \\text{Normal}(0, 0.2) \\\\\n  \\beta_A &\\sim \\text{Normal}(0, 0.5) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]Now, let’s prior predictive simulations makes sure ’re within bounds might expected. Nearly lines within bounds might expected.Finally, can estimate model. Conditional DAG data, doesn’t appear though area size causal effect weight foxes sample.6H4. Now infer causal impact adding food territory. make foxes heavier? covariates need adjust estimate total causal influence food?estimate total causal impact food, don’t need adjust variables. direct path average food weight indirect path group size. condition group size, path closed left direct effect. However, want total effect, adjustment made.estimate model regressing weight average food available, see effect food weight. Given DAG, expected. impact food weight, expected see impact area weight previous problem, area upstream average food causal model.6H5. Now infer causal impact group size. covariates need adjust ? Looking posterior distribution resulting model, think explains data? , can explain estimates three problems? go together?assessing causal impact group size, one back-door path: \\(G \\leftarrow F \\rightarrow W\\). path, average food, \\(F\\), fork, condition isolate effect group size.estimate model, see negative effect group size controlling food. also now see positive effect average food controlling group size. Thus, causal effect group size decrease weight. Logically makes sense, less food fox. model also indicates direct effect average food increase weight. , group size held constant, food results weight. However, total causal effect food weight, saw last problem, nothing. food also leads larger groups, turn decreases weight. masking effect, also called post-treatment bias.6H6. Consider research question. Draw DAG represent . testable implications DAG? variables condition close backdoor paths? unobserved variables omitted? reasonable colleague imagine additional threats causal inference ignored?question, ’ll use instructional intervention described question 6E2. DAG shown . DAG, ’re interested effect new professional development intervention (\\(D\\)) student performance (\\(P\\)). However, professional development works improving teacher’s instruction (\\(\\)), student performance also affected students’ pre-existing knowledge (\\(K\\)).Based DAG, three testable implications:\\(D \\!\\perp\\!\\!\\!\\perp K\\)\\(D \\!\\perp\\!\\!\\!\\perp P | \\)\\(\\!\\perp\\!\\!\\!\\perp K\\)interested causal impact professional development intervention \\(D\\) student performance \\(P\\), back-door paths close. Conditioning quality instruction introduce post-treatment bias mask effect professional development student performance.many factors included DAG known impact quality instruction student performance. include socioeconomic socioemotional state student, student motivation, school funding, class size, -home educational support, name just . Students also grouped classes schools, likely group-level effects accounted .6H7. DAG made previous problem, can write data generating simulation ? Can design one statistical models produce causal estimates? , try calculate interesting counterfactuals. , use simulation estimate size bias might expect. conditions , example, infer opposite true causal effect?Given DAG, can generate data estimate causal models test . First, let’s generate data. sake example, ’ll assume group level effects teachers equally effective. assumptions wouldn’t hold real data, suffice exercise.Now let’s estimate model find causal effect professional development student performance.\\[\\begin{align}\n  P_i &\\sim \\text{Normal}(\\mu_i, \\sigma) \\\\\n  \\mu_i &= \\alpha + \\beta_D \\\\\n  \\alpha &\\sim \\text{Normal}(0,0.2) \\\\\n  \\beta_D &\\sim \\text{Normal}(0,0.5) \\\\\n  \\sigma &\\sim \\text{Exponential}(1)\n\\end{align}\\]can see , expected given data simulated, strong effect professional development student performance. students whose teachers received professional development showing 0.88 standard deviation increase performance compared students whose teachers receive professional development.","code":"\ned_dag <- dagitty(\"dag { D -> I -> P <- K }\")\ncoordinates(ed_dag) <- list(x = c(D = 1, I = 1.5, P = 2, K = 2.5),\n                            y = c(D = 3, I = 2, P = 1, K = 2))\n\nggplot(ed_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\", size = 10) +\n  geom_dag_edges(edge_color = \"black\", edge_width = 2,\n                 arrow_directed = grid::arrow(length = grid::unit(15, \"pt\"),\n                                              type = \"closed\")) +\n  theme_void()\ndag_coords <- tibble(name = c(\"X\", \"U\", \"A\", \"B\", \"C\", \"Y\", \"V\"),\n                     x = c(1, 1, 2, 2, 3, 3, 3.5),\n                     y = c(1, 2, 2.5, 1.5, 2, 1, 1.5))\n\ndagify(Y ~ X + C + V,\n       X ~ U,\n       U ~ A,\n       B ~ U + C,\n       C ~ A + V,\n       coords = dag_coords) %>%\n  ggplot(aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_point(data = . %>% filter(name %in% c(\"U\", \"V\")),\n                 shape = 1, stroke = 2, color = \"black\") +\n  geom_dag_text(color = \"black\", size = 10) +\n  geom_dag_edges(edge_color = \"black\", edge_width = 2,\n                 arrow_directed = grid::arrow(length = grid::unit(15, \"pt\"),\n                                              type = \"closed\")) +\n  theme_void()\nnew_dag <- dagitty(\"dag { U [unobserved]\n                          V [unobserved]\n                          X -> Y\n                          X <- U <- A -> C -> Y\n                          U -> B <- C\n                          C <- V -> Y }\")\n\nadjustmentSets(new_dag, exposure = \"X\", outcome = \"Y\")\n#> { A }\nset.seed(1984)\n\nn <- 1000\ndat <- tibble(x = rnorm(n)) %>%\n  mutate(z = rnorm(n, mean = x, sd = 0.1),\n         y = rnorm(n, mean = z),\n         across(everything(), standardize))\n\nsim_cor <- cor(dat$x, dat$z)\nsim_cor\n#> [1] 0.995\nb6m2 <- brm(y ~ 1 + x + z, data = dat, family = gaussian,\n            prior = c(prior(normal(0, 0.2), class = Intercept),\n                      prior(normal(0, 0.5), class = b),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"chp6\", \"b6m2\"))\n\nas_draws_df(b6m2) %>%\n  as_tibble() %>% \n  select(b_Intercept, b_x, b_z, sigma) %>%\n  pivot_longer(everything()) %>%\n  ggplot(aes(x = value, y = name)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97))\ndag1 <- dagitty(\"dag{ X <- Z <- A -> Y <- X; Y <- Z }\")\nadjustmentSets(dag1, exposure = \"X\", outcome = \"Y\")\n#> { Z }\ndag2 <- dagitty(\"dag{ X -> Z <- A -> Y <- X; Y <- Z }\")\nadjustmentSets(dag2, exposure = \"X\", outcome = \"Y\")\n#>  {}\ndag3 <- dagitty(\"dag{ Y -> Z <- A -> X -> Y; X -> Z }\")\nadjustmentSets(dag3, exposure = \"X\", outcome = \"Y\")\n#>  {}\ndag4 <- dagitty(\"dag{ Y <- Z <- A -> X -> Y; X -> Z }\")\nadjustmentSets(dag4, exposure = \"X\", outcome = \"Y\")\n#> { A }\nwaffle_dag <- dagitty(\"dag { S -> W -> D <- A <- S -> M -> D; A -> M }\")\ncoordinates(waffle_dag) <- list(x = c(A = 1, S = 1, M = 2, W = 3, D = 3),\n                                y = c(A = 1, S = 3, M = 2, W = 3, D = 1))\n\nggplot(waffle_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\", size = 10) +\n  geom_dag_edges(edge_color = \"black\", edge_width = 2,\n                 arrow_directed = grid::arrow(length = grid::unit(15, \"pt\"),\n                                              type = \"closed\")) +\n  theme_void()\nadjustmentSets(waffle_dag, exposure = \"W\", outcome = \"D\")\n#> { A, M }\n#> { S }\ndata(\"WaffleDivorce\")\nwaffle <- WaffleDivorce %>%\n  as_tibble() %>%\n  select(D = Divorce,\n         A = MedianAgeMarriage,\n         M = Marriage,\n         S = South,\n         W = WaffleHouses) %>%\n  mutate(across(-S, standardize),\n         S = factor(S))\n\nwaff_mod <- brm(D ~ 1 + W + S, data = waffle, family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp6\", \"b6h1\"))\nspread_draws(waff_mod, b_W) %>%\n  ggplot(aes(x = b_W)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = expression(beta[W]), y = \"Density\")\nimpliedConditionalIndependencies(waffle_dag)\n#> A _||_ W | S\n#> D _||_ S | A, M, W\n#> M _||_ W | S\nwaff_ci1 <- brm(A ~ 1 + W + S, data = waffle, family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp6\", \"b6h2-1\"))\n\nwaff_ci2 <- brm(D ~ 1 + S + A + M + W, data = waffle, family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp6\", \"b6h2-2\"))\n\nwaff_ci3 <- brm(M ~ 1 + W + S, data = waffle, family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp6\", \"b6h2-3\"))\nlbls <- c(expression(\"Model 1:\"~beta[W]),\n          expression(\"Model 2:\"~beta[S]),\n          expression(\"Model 3:\"~beta[W]))\n\nbind_rows(\n  gather_draws(waff_ci1, b_W) %>%\n    ungroup() %>%\n    mutate(model = \"ICI 1\"),\n  gather_draws(waff_ci2, b_S1) %>%\n    ungroup() %>%\n    mutate(model = \"ICI 2\"),\n  gather_draws(waff_ci3, b_W) %>%\n    ungroup() %>%\n    mutate(model = \"ICI 3\")\n) %>%\n  ggplot(aes(x = .value, y= model)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  scale_y_discrete(labels = lbls) +\n  labs(x = \"Parameter Estimate\", y = \"Implied Conditional Independency\")\ndata(foxes)\n\nfox_dat <- foxes %>%\n  as_tibble() %>%\n  select(area, avgfood, weight, groupsize) %>%\n  mutate(across(everything(), standardize))\n\nfox_dat\n#> # A tibble: 116 × 4\n#>      area avgfood    weight groupsize\n#>     <dbl>   <dbl>     <dbl>     <dbl>\n#>  1 -2.24  -1.92    0.414       -1.52 \n#>  2 -2.24  -1.92   -1.43        -1.52 \n#>  3 -1.21  -1.12    0.676       -1.52 \n#>  4 -1.21  -1.12    1.30        -1.52 \n#>  5 -1.13  -1.32    1.12        -1.52 \n#>  6 -1.13  -1.32   -1.08        -1.52 \n#>  7 -2.02  -1.52    0.000291    -1.52 \n#>  8 -2.02  -1.52   -0.371       -1.52 \n#>  9  0.658 -0.0591  1.35        -0.874\n#> 10  0.658 -0.0591  0.896       -0.874\n#> # … with 106 more rows\nfox_dag <- dagitty(\"dag{ area -> avgfood -> groupsize -> weight <- avgfood }\")\nadjustmentSets(fox_dag, exposure = \"area\", outcome = \"weight\")\n#>  {}\nset.seed(2020)\n\nn <- 1000\ntibble(group = seq_len(n),\n       alpha = rnorm(n, 0, 0.2),\n       beta = rnorm(n, 0, 0.5)) %>%\n  expand(nesting(group, alpha, beta),\n         area = seq(from = -2, to = 2, length.out = 100)) %>%\n  mutate(weight = alpha + beta * area) %>%\n  ggplot(aes(x = area, y = weight, group = group)) +\n  geom_line(alpha = 1 / 10) +\n  geom_hline(yintercept = c((0 - mean(foxes$weight)) / sd(foxes$weight),\n                            (max(foxes$weight) - mean(foxes$weight)) /\n                              sd(foxes$weight)),\n             linetype = c(\"dashed\", \"solid\"), color = \"red\") +\n  annotate(geom = \"text\", x = -2, y = -3.83, hjust = 0, vjust = 1,\n           label = \"No weight\") +\n  annotate(geom = \"text\", x = -2, y = 2.55, hjust = 0, vjust = 0,\n           label = \"Maximum weight\") +\n  expand_limits(y = c(-4, 4)) +\n  labs(x = \"Standardized Area\", y = \"Standardized Weight\")\narea_mod <- brm(weight ~ 1 + area, data = fox_dat, family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b,),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp6\", \"b6h3\"))\n\nas_draws_df(area_mod) %>%\n  as_tibble() %>%\n  select(b_Intercept, b_area, sigma) %>%\n  pivot_longer(everything()) %>%\n  mutate(name = factor(name, levels = c(\"b_Intercept\", \"b_area\", \"sigma\"))) %>%\n  ggplot(aes(x = value, y = fct_rev(name))) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = \"Parameter Estimate\", y = \"Parameter\")\nadjustmentSets(fox_dag, exposure = \"avgfood\", outcome = \"weight\")\n#>  {}\nfood_mod <- brm(weight ~ 1 + avgfood, data = fox_dat, family = gaussian,\n                prior = c(prior(normal(0, 0.2), class = Intercept),\n                          prior(normal(0, 0.5), class = b,),\n                          prior(exponential(1), class = sigma)),\n                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                file = here(\"fits\", \"chp6\", \"b6h4\"))\n\nas_draws_df(food_mod) %>%\n  as_tibble() %>%\n  select(b_Intercept, b_avgfood, sigma) %>%\n  pivot_longer(everything()) %>%\n  mutate(name = factor(name, levels = c(\"b_Intercept\", \"b_avgfood\", \"sigma\"))) %>%\n  ggplot(aes(x = value, y = fct_rev(name))) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = \"Parameter Estimate\", y = \"Parameter\")\nadjustmentSets(fox_dag, exposure = \"groupsize\", outcome = \"weight\")\n#> { avgfood }\ngrp_mod <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,\n               family = gaussian,\n               prior = c(prior(normal(0, 0.2), class = Intercept),\n                         prior(normal(0, 0.5), class = b,),\n                         prior(exponential(1), class = sigma)),\n               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n               file = here(\"fits\", \"chp6\", \"b6h5\"))\n\nas_draws_df(grp_mod) %>%\n  as_tibble() %>%\n  select(b_Intercept, b_avgfood, b_groupsize, sigma) %>%\n  pivot_longer(everything()) %>%\n  mutate(name = factor(name, levels = c(\"b_Intercept\", \"b_avgfood\",\n                                        \"b_groupsize\", \"sigma\"))) %>%\n  ggplot(aes(x = value, y = fct_rev(name))) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = \"Parameter Estimate\", y = \"Parameter\")\ned_dag <- dagitty(\"dag { D -> I -> P <- K }\")\ncoordinates(ed_dag) <- list(x = c(D = 1, I = 1.5, P = 2, K = 2.5),\n                            y = c(D = 3, I = 2, P = 1, K = 2))\n\nggplot(ed_dag, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_text(color = \"black\", size = 10) +\n  geom_dag_edges(edge_color = \"black\", edge_width = 2,\n                 arrow_directed = grid::arrow(length = grid::unit(15, \"pt\"),\n                                              type = \"closed\")) +\n  theme_void()\nimpliedConditionalIndependencies(ed_dag)\n#> D _||_ K\n#> D _||_ P | I\n#> I _||_ K\nadjustmentSets(ed_dag, exposure = \"D\", outcome = \"P\")\n#>  {}\nset.seed(2010)\n\nstudents <- 500\n\ned_dat <- tibble(k = rnorm(students, mean = 0, sd = 2),\n                 d = sample(0L:1L, students, replace = TRUE),\n                 i = rnorm(students, mean = 1 + 3 * d),\n                 p = k + rnorm(students, 0.8 * i)) %>%\n  mutate(across(where(is.double), standardize))\n\ned_dat\n#> # A tibble: 500 × 4\n#>          k     d        i      p\n#>      <dbl> <int>    <dbl>  <dbl>\n#>  1 -0.472      1  1.61     1.05 \n#>  2  0.0678     1  0.986    0.973\n#>  3  1.09       1  1.05     1.42 \n#>  4  0.290      0 -1.60    -0.385\n#>  5 -0.131      1  1.28     0.731\n#>  6  1.54       0 -0.00907  1.11 \n#>  7 -0.474      0 -1.35    -0.794\n#>  8 -1.47       1  0.345   -0.425\n#>  9  0.735      0 -0.559    0.439\n#> 10  0.695      0 -0.372    0.155\n#> # … with 490 more rows\ned_mod <- brm(p ~ 1 + d, data = ed_dat, family = gaussian,\n              prior = c(prior(normal(0, 0.2), class = Intercept),\n                        prior(normal(0, 0.5), class = b),\n                        prior(exponential(1), class = sigma)),\n              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n              file = here(\"fits\", \"chp6\", \"b6h7-causal\"))\n\nas_draws_df(ed_mod) %>%\n  as_tibble() %>%\n  select(b_Intercept, b_d, sigma) %>%\n  pivot_longer(everything()) %>%\n  ggplot(aes(x = value, y = name)) +\n  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +\n  labs(x = \"Parameter Value\", y = \"Parameter\")"},{"path":"causes-confounds-colliders.html","id":"homework-2","chapter":"Week 3: Causes, Confounds & Colliders","heading":"3.3 Homework","text":"1. first two problems based data. data data(foxes) 116 foxes 30 different urban groups England. fox groups like street gangs. Group size (groupsize) varies 2 8 individuals. group maintains (almost exclusive) urban territory. territories larger tahn others. area variable encodes information. territories also avgfood others. food influences weight fox. Assume DAG:\\(F\\) avgfood, \\(G\\) groupsize, \\(\\) area, \\(W\\) weight.Use backdoor criterion estimate total causal influence \\(\\) \\(F\\). effect increasing area territory amount food inside ?one path \\(\\) \\(F\\), direct path. Thus backdoors, additional variables need included model. can confirm dagitty.Given , can estimate model {brms}. ’ll standardize variables make setting priors easier.see fairly strong effect area average amount food. credible interval area well zero. increase 1 standard deviation area expect see .9 standard deviation increase food. Logically makes sense, greater area means prey available.2. Now infer total direct causal effects adding food \\(F\\) territory weight \\(W\\) foxes. covariates need adjust case? light estimates problem previous one, think going foxes? Feel free speculate—matters justify speculation.backdoor paths \\(F\\) \\(W\\), covariates need added evaluating total effect food weight. want direct effect, need close path \\(F \\rightarrow G \\rightarrow W\\) adding group size covariate. can confirm dagitty.Now estimate models. First total effect. model see basically effect food weight. 95% interval {brms} output -0.21 0.15 mean -0.02. effect just likely positive negative.Next estimate direct effect. see stratify group size, see strong positive effect food weight. indicates within given group size, food associated weight.Altogether, results seem suggest masking effect. , food available, foxes move territory, increasing group size. continues equilibrium reached amount food available equally good (equally bad) within territory. Thus, total effect food negligible food becomes available, group size increases amount food available individual fox remains relatively stable.3. Reconsider Table 2 Fallacy example (Lecture 6), time unobserved confound \\(U\\) influences smoking \\(S\\) stroke \\(Y\\). ’s modified DAG:First use backdoor criterion determine adjustment set allows estimate causal effect \\(X\\) \\(Y\\), .e., \\(P(Y|\\text{}(X))\\). Second explain proper interpretation coefficient implied regression model corresponds adjustment set. coefficients (slopes) causal ? need fit models. Just think implications.5 paths \\(X\\) \\(Y\\):\\(X \\rightarrow Y\\)\\(X \\leftarrow S \\rightarrow Y\\)\\(X \\leftarrow \\rightarrow Y\\)\\(X \\leftarrow \\rightarrow S \\rightarrow Y\\)\\(X \\leftarrow \\rightarrow S \\leftarrow U \\rightarrow Y\\)Path 1 effect \\(X\\) \\(Y\\), want. rest backdoor paths must closed. Conditioning \\(\\) \\(S\\) close paths, can confirm dagitty.However, unobserved variable \\(U\\) important implications interpretation model. three slope coefficients canonical Table 2: \\(\\beta_X\\), \\(\\beta_S\\), \\(\\beta_A\\). focus investigation, \\(\\beta_X\\) represents causal effect \\(X\\) \\(Y\\). However, \\(S\\) collider, \\(\\beta_S\\) \\(\\beta_A\\) confounded \\(U\\). Thus, coefficients interpreted type causal effect.4 - OPTIONAL CHALLENGE. Write synthetic data simulation causal model shown Problem 3. sure include unobserved confound simulation. Choose functional relationships like—don’t get epidemiology correct. just need honor causal structure. design regression model estimate influence \\(X\\) \\(Y\\) use synthetic data. large sample need reliably estimate \\(P(Y|\\text{}(X))\\)? Define “reliably” like, justify definition.Since ’re going simulating bunch data sets, ’ll start writing function generate single data set based DAG. function allows us specify sample size relationship \\(X\\) \\(Y\\). Notice although \\(U\\) used generate data, variable removed return data set, unobserved (select(-U)).’ll test one example data set. use sim_dat() function just created generate data set 10 subjects positive relationship \\(X\\) \\(Y\\) 0.3.worked! got back estimate \\(X\\) close 0.3, equally likely positive negative. sample size 10.question asks us sample size needed reliable estimate \\(\\beta_X\\). two ways (least) think . first “correct” estimate average. , mean difference true value (0.3 example data set) estimated posterior mean (0.37 model summary). Thus, difference 0.07. measure also referred mean absolute difference. , absolute indicates normally take absolute value difference positive negative deviations don’t cancel .measure reliability might consider width chosen credible interval. posterior interval wide, might say unreliable estimate \\(\\beta_X\\), wide range values model views plausible. smaller interval reliable, indicate high degree confidence estimate narrow range values.simulation, ’ll generate 100 data sets sample size, fit model data set, calculate outcome measures. make things easier, ’ll build another function. First generate data, fit model (suppressMessages() capture.output() just suppress model fitting messages since ’ll fitting many models), use mean_hdi() calculate measures reliability.’ll investigate sample sizes 10, 20, 50, 100, 250, 500, 1,000. sample size, ’ll conduct 100 replications (.e., 100 data sets per sample size condition). following code create 100 rows sample size, apply sim_func() sample size row.Looking results can see replication, absolute value difference mean posterior true value (0.3; abs_error), well width 89% compatibility interval.Across replications, see average absolute error similar sample sizes. However, much variability lower sample sizes. , sample size 10, ’s unexpected see estimate 0.5. larger samples, range bias values gets narrower, less likely see estimates far true value.also see small sample sizes result large variable interval widths. sample size gets larger, width 89% compatibility interval decreases becomes consistent (.e., width similar replication).","code":"\nfox_dag <- dagitty(\"dag{ A -> F -> G -> W <- F }\")\n\nadjustmentSets(fox_dag, exposure = \"A\", outcome = \"F\")\n#>  {}\ndata(foxes)\n\nfox_dat <- foxes %>%\n  as_tibble() %>%\n  select(area, avgfood, weight, groupsize) %>%\n  mutate(across(everything(), standardize))\n\nfood_on_area <- brm(avgfood ~ 1 + area, data = fox_dat, family = gaussian,\n                    prior = c(prior(normal(0, 0.2), class = Intercept),\n                              prior(normal(0, 0.5), class = b,),\n                              prior(exponential(1), class = sigma)),\n                    iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                    file = here(\"fits\", \"hw3\", \"w3h1\"))\n\nsummary(food_on_area)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: avgfood ~ 1 + area \n#>    Data: fox_dat (Number of observations: 116) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.00      0.04    -0.09     0.09 1.00     8076     6131\n#> area          0.88      0.04     0.79     0.96 1.00     8389     5927\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.48      0.03     0.42     0.54 1.00     7997     6226\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nadjustmentSets(fox_dag, exposure = \"F\", outcome = \"W\", effect = \"total\")\n#>  {}\n\nadjustmentSets(fox_dag, exposure = \"F\", outcome = \"W\", effect = \"direct\")\n#> { G }\nfood_total <- brm(weight ~ 1 + avgfood, data = fox_dat, family = gaussian,\n                  prior = c(prior(normal(0, 0.2), class = Intercept),\n                            prior(normal(0, 0.5), class = b,),\n                            prior(exponential(1), class = sigma)),\n                  iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                  file = here(\"fits\", \"hw3\", \"w3h2-total\"))\n\nsummary(food_total)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: weight ~ 1 + avgfood \n#>    Data: fox_dat (Number of observations: 116) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    -0.00      0.08    -0.17     0.16 1.00     7369     5854\n#> avgfood      -0.02      0.09    -0.21     0.15 1.00     7826     6205\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     1.01      0.07     0.89     1.15 1.00     7839     6107\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nfood_direct <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,\n                   family = gaussian,\n                   prior = c(prior(normal(0, 0.2), class = Intercept),\n                             prior(normal(0, 0.5), class = b,),\n                             prior(exponential(1), class = sigma)),\n                   iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n                   file = here(\"fits\", \"hw3\", \"w3h2-direct\"))\n\nsummary(food_direct)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: weight ~ 1 + avgfood + groupsize \n#>    Data: fox_dat (Number of observations: 116) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    -0.00      0.08    -0.16     0.16 1.00     5680     4403\n#> avgfood       0.47      0.18     0.11     0.84 1.00     4022     3986\n#> groupsize    -0.57      0.19    -0.93    -0.21 1.00     4021     4085\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.96      0.06     0.85     1.09 1.00     5544     5204\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nstroke_dag <- dagitty(\"dag{\n  A -> S -> X -> Y;\n  A -> X; A -> Y; S -> Y;\n  S <- U -> Y \n  X [exposure]\n  Y [outcome]\n  U [unobserved]\n}\")\n\nadjustmentSets(stroke_dag)\n#> { A, S }\nsim_dat <- function(n = 100, bx = 0) {\n  tibble(U = rnorm(n, 0, 1),\n         A = rnorm(n, 0, 1)) %>% \n    mutate(S = rnorm(n, A + U, 1),\n           X = rnorm(n, A + S, 1),\n           Y = rnorm(n, A + S + (bx * X) + U, 1)) %>% \n    select(-U) %>% \n    mutate(across(everything(), standardize))\n}\ndat1 <- sim_dat(n = 10, bx = 0.3)\n\nmod1 <- brm(Y ~ 1 + X + S + A, data = dat1, family = gaussian,\n            prior = c(prior(normal(0, 0.2), class = Intercept),\n                      prior(normal(0, 0.5), class = b,),\n                      prior(exponential(1), class = sigma)),\n            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,\n            file = here(\"fits\", \"hw3\", \"w3h4\"))\n\nsummary(mod1, prob = 0.89)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: Y ~ 1 + X + S + A \n#>    Data: dat1 (Number of observations: 10) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 0; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Population-Level Effects: \n#>           Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.00      0.07    -0.11     0.11 1.00     4872     3998\n#> X             0.37      0.28    -0.09     0.82 1.00     2809     3786\n#> S             0.75      0.15     0.51     0.98 1.00     3526     4020\n#> A            -0.14      0.23    -0.49     0.23 1.00     3348     4173\n#> \n#> Family Specific Parameters: \n#>       Estimate Est.Error l-89% CI u-89% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.24      0.08     0.14     0.38 1.00     2994     3461\n#> \n#> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nsim_func <- function(n, bx = 0) {\n  dat <- sim_dat(n = n, bx = bx)\n  \n  suppressMessages(output <- capture.output(\n    mod <- brm(Y ~ 1 + X + S + A, data = dat, family = gaussian,\n               prior = c(prior(normal(0, 0.2), class = Intercept),\n                         prior(normal(0, 0.5), class = b,),\n                         prior(exponential(1), class = sigma)),\n               refresh = 0,\n               iter = 4000, warmup = 2000, chains = 4, cores = 4)\n  ))\n  \n  as_draws_df(mod, \"b_X\") %>% \n    mean_hdi(.width = 0.89) %>% \n    mutate(abs_error = abs(b_X - bx),\n           int_width = .upper - .lower) %>% \n    select(abs_error, int_width)\n}\nsim_results <- expand_grid(sample_size = c(10, 20, 50, 100, 250, 500, 1000),\n                           replication = seq_len(100)) %>% \n  mutate(results = map(sample_size, sim_func, bx = 0.3)) %>% \n  unnest(results) %>% \n  write_rds(here(\"fits\", \"hw3\", \"w3h4-sim.rds\"))\nsim_results\n#> # A tibble: 700 × 4\n#>    sample_size replication abs_error int_width\n#>          <dbl>       <int>     <dbl>     <dbl>\n#>  1          10           1    0.206      0.774\n#>  2          10           2    0.265      1.08 \n#>  3          10           3    0.0771     0.907\n#>  4          10           4    0.378      0.954\n#>  5          10           5    0.109      0.991\n#>  6          10           6    0.0423     0.970\n#>  7          10           7    0.460      0.764\n#>  8          10           8    0.0568     0.999\n#>  9          10           9    0.0854     0.706\n#> 10          10          10    0.0566     1.01 \n#> # … with 690 more rows\nsim_results %>% \n  select(-replication) %>% \n  pivot_longer(-sample_size, names_to = \"measure\", values_to = \"value\") %>% \n  mutate(measure = factor(measure, levels = c(\"abs_error\", \"int_width\"),\n                          labels = c(\"Absolute Error\", \"89% Interval Width\"))) %>% \n  ggplot(aes(x = factor(sample_size), y = value)) +\n  facet_wrap(~measure, nrow = 1) +\n  stat_interval(.width = c(0.67, 0.89, 0.97)) +\n  scale_color_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),\n                     breaks = c(0.67, 0.89, 0.97)) +\n  labs(x = \"Sample Size\", y = \"Value\", color = \"Interval\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
