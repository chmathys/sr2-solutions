# Interactions & MCMC

The fifth week covers Chapter 8 (Conditional Manatees) and Chapter 9 (Markov Chain Monte Carlo).

## Lectures

Lecture 9:

<iframe width="560" height="315" src="https://www.youtube.com/embed/QhHfo6-Bx8o" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Lecture 10:

<iframe width="560" height="315" src="https://www.youtube.com/embed/v-j0UmWf3Us" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Exercises

### Chapter 8

:::question
> **8E1.** For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect.

> (1) Bread dough rises because of yeast.
> (2) Education leads to higher income.
> (3) Gasoline makes a car go.
:::

For the first, the amount of heat can moderate the relationship between yeast and the amount the dough rises. In the second example, ethnicity may give rise to an interaction, as individuals of some races face more systemic challenges (i.e., it's easier for white people with low education to get a job than people of color). Thus, education may have more of an effect for minorities. Finally, a car also requires wheels. Wheels with no gas and gas with no wheels both prevent the car from moving; only with both will the car go.

:::question
> **8E2.** Which of the following explanations invokes an interaction?

> (1) Caramelizing onions requires cooking over low heat and making sure the onions do not dry out.
> (2) A car will go faster when it has more cylinders or when it has better fuel injector.
> (3) Most people acquire their political beliefs from their parents, unless they get them instead from their friends.
> (4) Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.).
:::

The first example invokes an interaction. The caramelization depends on both heat and moisture, but heat will also impact the moisture level.

I have minimal car knowledge, but I presume that a good fuel injector with a 6 cylinder engine is better than a poor fuel injector with a 6 cylinder engine. The impact of cylinders would depend on the fuel injector. Similarly the effect of the fuel injector may depend on the number of cylinders. This would be an interaction.

In the third example, it appears as though there are three important predictors: your parents political beliefs, your friends political beliefs, and how political your friends are. The description implies an interaction between how political your friends are and your parents political beliefs. As your friends become more political, the influence of your parents political beliefs decreases.

Finally, without any expertise in the domain area, I have no reason to think that sociability would influence the effect of appendages on intelligence, or that appendages would influence the effect of sociability on intelligence. This implies no interaction.

:::question
> **8E3.** For each of the explanations in **8E2**, write a linear model that expresses the stated relationship.
:::

For onion caramelization, the mathematical model is:

$$
\begin{align}
C_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_HH_i + \beta_MM_i + \beta_{HM}H_iM_i
\end{align}
$$

The mathematical model for car speed is very similar:

$$
\begin{align}
S_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_CC_i + \beta_FF_i + \beta_{CF}C_iF_i
\end{align}
$$

The third model we are predicting political beliefs ($B$) from parents' political beliefs ($P$), friends political beliefs ($F$), how political your friends are ($H$), and the interaction be $P$ and $H$.

$$
\begin{align}
B_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_PP_i + \beta_FF_i + \beta_HH_i + \beta_{PH}P_iH_i
\end{align}
$$

The fourth model is a simple regression with no interaction.

$$
\begin{align}
I_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_SS_i + \beta_AA_i
\end{align}
$$

:::question
> **8M1.** Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of teh water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?
:::

In the chapter example, we saw that at cool temperatures, the blooms were best predicted by water, shade, and their interaction. Here, we learn that neither water nor light matter at a high temperature. This implies a three-way interaction. Just as in the chapter where water had no effect when there was no light, we see that neither water nor light have an effect when temperature is high.

:::question
> **8M2.** Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?
:::

For this model, we can use the interaction model from the chapter with an additional predictor ($C$) that is 0 when the temperature is hot and 1 when the temperature is cold.

$$
\begin{align}
B_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= C_i \times (\alpha + \beta_WW_i + \beta_SS_i + \beta_{WS}W_iS_i)
\end{align}
$$

:::question
> **8M3.** In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a "species interaction." Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not?
:::

In order to predict raven population size based on wolf population size, we should include data on the territory area for the wolves, the number of wolves, and amount of food available, and finally the number of ravens. This is similar to the types of data included in `data(foxes)`. I would expect the relationship to be non-linear. When there are no wolves, I would expect there to be no ravens. As the number of wolves increases, the number of ravens would also increase. However, eventually the number of wolves would increase to the point that the wolves exhaust the food supply, leaving no food left for the ravens, at which point the raven population would begin to shrink. Thus, if we created a counter factual plot with wolves on the x-axis and ravens on the y-axis, I would expect to see at linear trend at first, but then the raven population level off or drop when there is a large numbers of wolves.

:::question
> **8M4.** Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior asumptions mean for the interaction prior, if anything?
:::

There are a few issues constraining different coefficients with bounded parameters (especially negative parameters) in {brms}. For some details, see [this thread](https://discourse.mc-stan.org/t/negative-prior-distribution/18093/2) on the Stan discourse. To get around this, rather than constrain the shade parameter to be negative, we'll create a new variable which is the opposite of shade, `light` and the corresponding `light_cent`. We can then constrain this parameter to be positive, just as the `water_cent` parameter is constrained to be positive. Finally, because of the new `light` variable the positivity constraints on the main effects, we also want to constrain the interaction term to be positive. That is, if both water and light increase, we expect an additional additive effect.

```{r e8m4-1}
library(rethinking)
data("tulips")

tulip_dat <- tulips %>%
  as_tibble() %>%
  mutate(light = -1 * shade,
         blooms_std = blooms / max(blooms),
         water_cent = water - mean(water),
         shade_cent = shade - mean(shade),
         light_cent = light - mean(light))

b8m4 <- tulip_dat %>%
  brm(blooms_std ~ 1 + water_cent + light_cent + water_cent:light_cent,
      data = ., family = gaussian,
      prior = c(prior(normal(0.5, 0.25), class = Intercept),
                prior(normal(0, 0.25), class = b, lb = 0),
                prior(exponential(1), class = sigma)),
      iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
      file = here("fits", "chp8", "b8m4.rds"))

summary(b8m4)
```

Looking at the the posterior predicted blooms, the results are very similar to what we saw in Figure 8.7 from the text. This indicates that the prior constraints did not have a large effect on the predicted values.

```{r e8m4-2}
new_tulip <- crossing(water_cent = -1:1, 
                      light_cent = -1:1)

points <- tulip_dat %>%
  expand(nesting(water_cent, light_cent, blooms_std)) %>%
  mutate(light_grid = glue("light_cent = {light_cent}"))

fitted(b8m4, newdata = new_tulip, summary = FALSE, nsamples = 50) %>%
  as.data.frame() %>%
  set_names(glue_data(new_tulip, "{water_cent}_{light_cent}")) %>%
  rowid_to_column() %>%
  pivot_longer(-rowid, names_to = c("water_cent", "light_cent"),
               names_sep = "_", values_to = ".draw") %>%
  mutate(water_cent = as.integer(water_cent),
         light_cent = as.integer(light_cent),
         light_grid = glue("light_cent = {light_cent}")) %>%
  ggplot(aes(x = water_cent, y = .draw, group = rowid)) +
  facet_grid(cols = vars(light_grid)) +
  geom_line(alpha = 0.4) +
  geom_point(data = points, aes(x = water_cent, y = blooms_std),
             inherit.aes = FALSE, color = "#009FB7") +
  scale_x_continuous(breaks = -1:1) +
  labs(x = "water")
```

Finally, let's look at the prior predictive simulation for this model. Overall the priors might be a little too uninformative, especially for the interaction. The interaction has the most impact in the far right panel of the below figure, and there are many lines that exceed the expected boundaries in this panel.

```{r e8m4-3}
b8m4p <- update(b8m4, sample_prior = "only",
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                file = here("fits", "chp8", "b8m4p.rds"))

fitted(b8m4p, newdata = new_tulip, summary = FALSE, nsamples = 50) %>%
  as.data.frame() %>%
  set_names(glue_data(new_tulip, "{water_cent}_{light_cent}")) %>%
  rowid_to_column() %>%
  pivot_longer(-rowid, names_to = c("water_cent", "light_cent"),
               names_sep = "_", values_to = ".draw") %>%
  mutate(water_cent = as.integer(water_cent),
         light_cent = as.integer(light_cent),
         light_grid = glue("light_cent = {light_cent}")) %>%
  ggplot(aes(x = water_cent, y = .draw, group = rowid)) +
  facet_grid(cols = vars(light_grid)) +
  geom_line(aes(alpha = rowid == 1), show.legend = FALSE) +
  geom_hline(yintercept = c(0, 1), linetype = "dashed") +
  scale_alpha_manual(values = c(0.2, 1)) +
  scale_x_continuous(breaks = -1:1) +
  labs(x = "water")
```


:::question
> **8H1.** Return to the `data(tulips)` example in the chapter. Now include the `bed` variable as a predictor in the interaction model. Don't interact `bed` with the other predictors; just include it as a main effect. Note that `bed` is categorical. So to use it properly, you will need to either construct dummy variables, or rather an index variable, as explained in Chapter 5.
:::

The `bed` variable is already a factor variable in the `tulips` data, so we can just add it to the formula in `brm()`. To use indicator variables instead of a dummy variable, we remove the separate intercept (i.e., `~ 0` in the formula below). Because of what's coming in the next question, we'll use `light` instead of `shade` for this model also.

```{r e8h1}
b8h1 <- brm(blooms_std ~ 0 + water_cent + light_cent + bed + 
              water_cent:light_cent,
            data = tulip_dat, family = gaussian,
            prior = c(prior(normal(0.5, 1), class = b, coef = beda),
                      prior(normal(0.5, 1), class = b, coef = bedb),
                      prior(normal(0.5, 1), class = b, coef = bedc),
                      prior(normal(0, 0.25), class = b, coef = water_cent),
                      prior(normal(0, 0.25), class = b, coef = light_cent),
                      prior(normal(0, 0.25), class = b,
                            coef = "water_cent:light_cent"),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp8", "b8h1.rds"))

summary(b8h1)
```

:::question
> **8H2.** Use WAIC to compare the model from **8H1** to a model that omits `bed`. What do you infer from this comparison? Can you reconcile the WAIC results with the posterior distribution of the `bed` coefficients?
:::

For the comparison, we'll compare `b8h1` to the model `b8m4`, which is the model from the chapter, with new prior distributions that constrain the main effect of water to be positive and the main effect of shade to be negative.

Model `b8h1` is the preferred model; however, the standard error of the difference is larger than the magnitude of the difference, indicating that the WAIC is not able to meaningfully differentiate between the two models. For predictive purposes, this means that the inclusion of the `bed` variable does not significantly improve the model. It should also be noted that both of the models have some exceptionally large penalty values, so these analyses may be unreliable, and we should consider re-fitting the models with a distribution with fatter tails.

```{r e8h2}
b8m4 <- add_criterion(b8m4, criterion = "waic",
                      overwrite = TRUE, force_save = TRUE)
b8h1 <- add_criterion(b8h1, criterion = "waic",
                      overwrite = TRUE, force_save = TRUE)

loo_compare(b8m4, b8h1, criterion = "waic") %>%
  print(simplify = FALSE)
```

:::question
> **8H3.** Consider again the `data(rugged)` data on economic development and terrain ruggedness, examined in this chapter. One of the African countries in that example Seychelles, is far outside the cloud of other nations, being a rare country with both relatively high GDP and high ruggedness. Seychelles is also unusual, in that it is a group of islands far from the coast of mainland Africa, and its main economic activity is tourism.

> (a) Focus on model `m8.5` from the chapter. Use WAIC pointwise penalties and PSIS Pareto *k* values to measure relative influence of each country. By these criteria, is Seychelles influencing the results? Are there other nations that are relatively influential? If so, can you explain why?
> (b) Now use robust regression, as described in the previous chapter. Modify `m8.5` to se a Student-t distribution with $\nu = 2$. Does this change the results in a substantial way?
:::

In the text, model `m8.5` uses the tulips data, so I'm assuming this is a typo and we should be looking at model `m8.3`, which is the interaction model from the terrain ruggedness example in the chapter. So, let's first create a {brms} version of model `m8.3`.

```{r e8h3-1}
data("rugged")
rugged_dat <- rugged %>%
  as_tibble() %>%
  select(country, rgdppc_2000, rugged, cont_africa) %>%
  drop_na(rgdppc_2000) %>%
  mutate(log_gdp = log(rgdppc_2000),
         log_gdp_std = log_gdp / mean(log_gdp),
         rugged_std = rugged / max(rugged),
         rugged_std_cent = rugged_std - mean(rugged_std),
         cid = factor(cont_africa, levels = c(1, 0),
                      labels = c("African", "Not African")))

b8h3 <- brm(
  bf(log_gdp_std ~ 0 + a + b * rugged_std_cent,
     a ~ 0 + cid,
     b ~ 0 + cid,
     nl = TRUE),
  data = rugged_dat, family = gaussian,
  prior = c(prior(normal(1, 0.1), class = b, coef = cidAfrican, nlpar = a),
            prior(normal(1, 0.1), class = b, coef = cidNotAfrican, nlpar = a),
            prior(normal(0, 0.3), class = b, coef = cidAfrican, nlpar = b),
            prior(normal(0, 0.3), class = b, coef = cidNotAfrican, nlpar = b),
            prior(exponential(1), class = sigma)),
  iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
  file = here("fits", "chp8", "b8h3.rds")
)

b8h3 <- add_criterion(b8h3, criterion = c("loo", "waic"), overwrite = TRUE)

summary(b8h3)
```

Now let's take a look at the Pareto *k* and $p_{\Tiny\text{WAIC}}$ values from the model. Seychelles does appear to be influential. The Pareto $k$ value is not above the above the 0.7 threshold; however, the $p_{WAIC}$ value is above the 0.4 threshold. In addition to Seychelles, Switzerland also appears to be influential. Like Seychelles, Switzerland is also an extremely rugged country that gets a great deal of economic activity through tourism.

```{r e8h3-2}
library(gghighlight)

tibble(pareto_k = b8h3$criteria$loo$diagnostics$pareto_k,
       p_waic = b8h3$criteria$waic$pointwise[, "p_waic"]) %>%
  rowid_to_column(var = "obs") %>%
  left_join(rugged_dat %>%
              select(country) %>%
              rowid_to_column(var = "obs"),
            by = "obs") %>%
  ggplot(aes(x = pareto_k, y = p_waic)) +
  geom_vline(xintercept = 0.7, linetype = "dashed") +
  geom_hline(yintercept = 0.4, linetype = "dashed") +
  geom_point() +
  gghighlight(pareto_k > 0.7 | p_waic > 0.4, n = 1, label_key = country,
              label_params = list(size = 3)) +
  labs(x = expression(Pareto~italic(k)), y = expression(p[WAIC]))
```

Now for part (b), we'll use a Student-t distribution with $\nu = 2$.

```{r e8h3-3}
b8h3_t <- brm(
  bf(log_gdp_std ~ 0 + a + b * rugged_std_cent,
     a ~ 0 + cid,
     b ~ 0 + cid,
     nu = 2,
     nl = TRUE),
  data = rugged_dat, family = student,
  prior = c(prior(normal(1, 0.1), class = b, coef = cidAfrican, nlpar = a),
            prior(normal(1, 0.1), class = b, coef = cidNotAfrican, nlpar = a),
            prior(normal(0, 0.3), class = b, coef = cidAfrican, nlpar = b),
            prior(normal(0, 0.3), class = b, coef = cidNotAfrican, nlpar = b),
            prior(exponential(1), class = sigma)),
  iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
  file = here("fits", "chp8", "b8h3_t.rds")
)

b8h3_t <- add_criterion(b8h3_t, criterion = c("loo", "waic"), overwrite = TRUE)
```

A comparison of PSIS-LOO criterion indicates that our Student-t model may be overfit, as the original model is strongly preferred.

```{r e8h3-4}
loo_compare(b8h3, b8h3_t)
```

Looking at the parameter estimates, it doesn't appear that the Student-t distribution had too much of an effect, as the estimates and interval are pretty similar across the two models.

```{r e8h3-5}
fixef(b8h3)

fixef(b8h3_t)
```

:::{.question .code-question}
> **8H4.** The values in `data(nettles)` are data on language diversity in 74 nations.^[Data from @nettle1998] The meaning of each column is given below.

> (1) `country`: Name of the country
> (2) `num.lang`: Number of recognized languages spoken
> (3) `area`: Area in square kilometers
> (4) `k.pop`: Population, in thousands
> (5) `num.stations`: Number of weather stations that provided data for the next two columns
> (6) `mean.growing.season`: Average length of growing season, in months
> (7) `sd.growing.season`: Standard deviation of length of growing season, in months

> Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people don't need large social networks to buffer them against risk of food shortfalls. The means cultural groups can be smaller and more self-sufficient, leading to more languages per capita. Use the number of languages per capita as the outcome:

```{r e8h4-intro, eval = FALSE}
d$lang.per.cap <- d$num.lang / d$k.pop
```

> Use the logarithm of this new variable as your regression outcome. (A count model would be better here, but you'll learn those later, in Chapter 11.) This problem is open ended, allowing you to decide how you address the hypotheses and the uncertain advice the modeling provides. If you think you need to use WAIC anyplace, please do. If you think you need certain priors, argue for them. If you think you need to plot predictions in a certain way, please do. Just try to honestly evaluate the main effects of both `mean.growing.season` and `sd.growing.season`, as well as their two-way interaction. Here are three parts to help.

> (a) Evaluate the hypothesis that language diversity, as measured by `log(lang.per.cap)`, is positively associated with average length of the growing season, `mean.growing.season`. Consider `log(area)` in your regression(s) as a covariate (not an interaction). Interpret your results.
> (b) Now evaluate the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, `sd.growing.season`. This hypothesis follows from uncertainty in harvest favoring social insurance through larger social networks and therefore fewer languages. Again, consider `log(area)` as a covariate (not an interaction). Interpret your results.
> (c) Finally, evaluate the hypothesis that `mean.growing.season` and `sd.growing.season` interact to synergistically reduce language diversity. The idea is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect windfalls to be used during the droughts.
:::

First, ...

## Session Info {-}

<details><summary>View the session information used to render this week.</summary>
```{r 01-session-info}
devtools::session_info()
```
</details>
