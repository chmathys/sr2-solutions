# Linear Models

The second week covers Chapter 4 (Geocentric Models).

## Lectures

Lecture 3:

<iframe width="560" height="315" src="https://www.youtube.com/embed/h5aPo5wXN8E" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Lecture 4:

<iframe width="560" height="315" src="https://www.youtube.com/embed/ENxTrFf9a7c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Exercises

### Chapter 4

> **4E1.** In the model definition below, which line is the likelihood?
\begin{align}
  y_i &\sim \text{Normal}(\mu,\sigma) \\
  \mu &\sim \text{Normal}(0,10) \\
  \sigma &\sim \text{Exponential}(1)
\end{align}

The likelihood is given by the first line, $y_i \sim \text{Normal}(\mu,\sigma)$. The other lines represent the prior distributions for $\mu$ and $\sigma$.

> **4E2.** In the model definition just above, how many parameters are in the posterior distribution?

There are two parameters, $\mu$ and $\sigma$.

> **4E3.** Using the model definition above, write down the appropriate form of Bayes' theorem that includes the proper likelihood and priors.

$$
\text{Pr}(\mu,\sigma|y) = \frac{\prod_i\text{Normal}(y_i|\mu,\sigma)\text{Normal}(\mu|0,10)\text{Exponential}(\sigma|1)}{\int\int\prod_i\text{Normal}(y_i|\mu,\sigma)\text{Normal}(\mu|0,10)\text{Exponential}(\sigma|1)d \mu d \sigma}
$$

> **4E4.** In the model definition below, which line is the linear model?
\begin{align}
  y_i &\sim \text{Normal}(\mu,\sigma) \\
  \mu_i &= \alpha + \beta x_i \\
  \alpha &\sim \text{Normal}(0,10) \\
  \beta &\sim \text{Normal}(0,1) \\
  \sigma &\sim \text{Exponential}(2)
\end{align}

The linear model is the second line, $\mu_i = \alpha + \beta x_i$.

> **4E5.** In the model definition just above, how many parameters are in the posterior distribution?

There are now three model parameters: $\alpha$, $\beta$, and $\sigma$. The mean, $\mu$ is no longer a parameter, as it is defined deterministically, as a function of other parameters in the model.

> **4M1.** For the model definition below, simulate observed *y* values from the prior (not the posterior).
\begin{align}
  y_i &\sim \text{Normal}(\mu,\sigma) \\
  \mu &\sim \text{Normal}(0,10) \\
  \sigma &\sim \text{Exponential}(1)
\end{align}

```{r e4m1}
library(tidyverse)

sim <- tibble(mu = rnorm(n = 10000, mean = 0, sd = 10),
       sigma = rexp(n = 10000, rate = 1)) %>%
  mutate(y = rnorm(n = 10000, mean = mu, sd = sigma))

ggplot(sim, aes(x = y)) +
  geom_density() +
  labs(x = "y", y = "Density")
```

> **4M2.** Trasnlate the model just above into a `quap` formula.

```{r e4m2}
flist <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dexp(1)
)
```

> **4M3.** Translate the `quap` model formula below into a mathematical model definition.
```r
  y ~ dnorm( mu , sigma ),
  mu <- a + b*x,
  a ~ dnorm( 0 , 10 ),
  b ~ dunif( 0 , 1 ),
  sigma ~ dexp( 1 )
```

\begin{align}
  y_i &\sim \text{Normal}(\mu_i,\sigma) \\
  \mu_i &= a + bx_i \\
  a &\sim \text{Normal}(0, 10) \\
  b &\sim \text{Uniform}(0, 1) \\
  \sigma &\sim \text{Exponential}(1)
\end{align}

> **4M4.** A sample of students is measured for height each year for 3 years. After the third year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

\begin{align}
  h_i &\sim \text{Normal}(\mu_i, \sigma) \\
  \mu_i &= \alpha + \beta(x_i - \bar{x}) \\
  \alpha &\sim \text{Normal}(169, 20) \\
  \beta &\sim \text{Normal}(0, 8) \\
  \sigma &\sim \text{Exponential}(1)
\end{align}

Because height is centered, $\alpha$ represents the average height in the average year (i.e., year 2). The prior of $\text{Normal}(165,20)$ was chosen assuming that height is measured in centimeters and that that sample is of adults (e.g., college students). The mean of 169cm represents the average height people aged 20 and older in the United States [@avg_height]. Further, @avg_height report the 5th and 95th percentiles for females and males as [150,174] and [163,188], respectively. A standard deviation of 20 presumes 95% of the probability between 169 $\pm$ 40, or [129,209], easily encapsulating the ranges seen in previous observed data.

At this point, we have no reason to believe that year would have any effect on height, as we're assuming that all students are adults, and therefore likely done growing. Accordingly, we choose a prior centered on zero. The standard deviation of the prior of 5 represents plausible growth (or shrinkage). During growth spurts, height growth averages 6--13 cm/year. The standard deviation of 8 encompasses the range we might expect to see if growth were occurring at a high rate.

Prior predictive simulations for $\alpha$ and $\beta$ also appear to give reasonably plausible regression lines, given our current assumptions.

```{r e4m4}
n <- 1000
tibble(group = seq_len(n),
       alpha = rnorm(n, 169, 20),
       beta = rnorm(n, 0, 8)) %>%
  expand(nesting(group, alpha, beta), year = c(1, 3)) %>%
  mutate(height = alpha + beta * (year - 2)) %>%
  ggplot(aes(x = year, y = height, group = group)) +
  geom_line(alpha = 1/10) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, color = "red") +
  annotate(geom = "text", x = 1, y = 0, hjust = 0, vjust = 1,
           label = "Embryo") +
  annotate(geom = "text", x = 1, y = 272, hjust = 0, vjust = 0,
           label = "World's tallest person (272cm)") +
  coord_cartesian(ylim = c(-25, 300)) +
  labs(x = "Year", y = "Height")
```

Finally, the exponential prior on $\sigma$ assumes an average deviation of 1.

> **4M5.** Now suppose I remind you that every student got taller each year. Does this information lead you to change your choice of priors? How?

Yes. Because we know that an increase in year will always lead to increased height, we know that $\beta$ will be positive. Therefore, our prior should reflect this by using, for example, a log-normal distribution.

$$
\beta \sim \text{Log-Normal}(2,0.5)
$$

This log-normal prior has an 89% highest density interval between about 2.3 and 14.1cm growth per year. 

```{r e4m5-1, cache = TRUE}
library(tidybayes)

samples <- rlnorm(1e8, 2, 0.5)
bounds <- median_hdi(samples, .width = 0.89)

ggplot() +
  stat_function(data = tibble(x = c(0, 30)), mapping = aes(x = x),
                geom = "line", fun = dlnorm,
                args = list(meanlog = 2, sdlog = 0.5)) +
  geom_ribbon(data = tibble(x = seq(bounds$ymin, bounds$ymax, 0.01)),
              aes(x = x, ymin = 0, ymax = dlnorm(x, 2, 0.5)),
              alpha = 0.8) +
  labs(x = expression(beta), y = "Density")
```

Prior predictive simulations of plausible lines using this new log-normal prior indicate that these priors still represent plausible values, while also constraining all lines to have a positive slope.

```{r e4m5-2}
n <- 1000
tibble(group = seq_len(n),
       alpha = rnorm(n, 169, 20),
       beta = rlnorm(n, 2, 0.75)) %>%
  expand(nesting(group, alpha, beta), year = c(1, 3)) %>%
  mutate(height = alpha + beta * (year - 2)) %>%
  ggplot(aes(x = year, y = height, group = group)) +
  geom_line(alpha = 1/10) +
  geom_hline(yintercept = c(0, 272), linetype = 2:1, color = "red") +
  annotate(geom = "text", x = 1, y = 0, hjust = 0, vjust = 1,
           label = "Embryo") +
  annotate(geom = "text", x = 1, y = 272, hjust = 0, vjust = 0,
           label = "World's tallest person (272cm)") +
  coord_cartesian(ylim = c(-25, 300)) +
  labs(x = "Year", y = "Height")
```

> **4M6.** Now suppose I tell you that the variance among heights for students of the same age is never more than 64cm. How does this lead you to revise your priors?

A variance of 64cm corresponds to a standard deviation of 8cm. Our current prior of $\sigma \sim \text{Exponential}(1)$ gives very little probability mass to values greater than 8. However, we can also see that the vast majority of the probability mass for this prior is less than 5cm. Therefore, if we think the standard deviation could get close to 8cm, we might consider reducing the rate of the exponential prior to allow for slightly larger values.

```{r e4m6}
ggplot(tibble(x = c(0, 20)), aes(x = x)) +
  stat_function(fun = dexp, args = list(rate = 1)) +
  labs(x = expression(sigma), y = "Density")
```

> **4M7.** Refit model `m4.3` from the chapter, but omit the mean weight `xbar` this time. Compare the new model's posterior to that of the original model. In particular, look at the covariance among the parameters. What is different? Then compare the posterior predictions of both models.

First, we'll reproduce `m4.3` using `quap()`, and then again using `brm()`. The covariance matrix from both is the same, as we would expect!

```{r e4m7-1}
library(rethinking)
library(brms)

data(Howell1)
dat <- Howell1 %>%
  filter(age >= 18) %>%
  mutate(weight_c = weight - mean(weight))

# first, duplicate model with `quap`
m4.3 <- quap(alist(height ~ dnorm(mu, sigma),
                   mu <- a + b * (weight_c),
                   a ~ dnorm(178, 20),
                   b ~ dlnorm(0, 1),
                   sigma ~ dunif(0, 50)),
             data = dat)

round(vcov(m4.3), 3)

# and then with brms
b4.3 <- brm(height ~ 1 + weight_c, data = dat, family = gaussian,
            prior = c(prior(normal(178, 20), class = Intercept),
                      prior(lognormal(0, 1), class = b, lb = 0),
                      prior(uniform(0, 50), class = sigma)),
            iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "b4.3-0"))

posterior_samples(b4.3) %>%
  select(-lp__) %>%
  cov() %>%
  round(digits = 3)
```

Now, let's using the non-centered parameterization. This time, we'll only use `brm()`.

```{r e4m7-2}
b4.3_nc <- brm(height ~ 1 + weight, data = dat, family = gaussian,
               prior = c(prior(normal(178, 20), class = Intercept),
                         prior(lognormal(0, 1), class = b, lb = 0),
                         prior(uniform(0, 50), class = sigma)),
               iter = 28000, warmup = 27000, chains = 4, cores = 4, seed = 1234,
               file = here("fits", "b4.3_nc"))

posterior_samples(b4.3_nc) %>%
  select(-lp__) %>%
  cov() %>%
  round(digits = 3)
```

We now see non-zero covariances between the parameters. Lets compare the posterior predictions. We'll generate hypothetical outcome plots which are animated to show the uncertainty in estimates [see @hops1; @hops2]. Here we'll just animate the estimated regression line using {[gganimate](https://gganimate.com/)} [@R-gganimate]. We can see that the predictions from the two models are nearly identical.

```{r e4m7-3}
library(gganimate)

weight_seq <- tibble(weight = seq(25, 70, length.out = 100)) %>%
  mutate(weight_c = weight - mean(dat$weight))

predictions <- bind_rows(
  predict(b4.3, newdata = weight_seq) %>%
    as_tibble() %>%
    bind_cols(weight_seq) %>%
    mutate(type = "Centered"),
  predict(b4.3_nc, newdata = weight_seq) %>%
    as_tibble() %>%
    bind_cols(weight_seq) %>%
    mutate(type = "Non-centered")
)

fits <- bind_rows(
  weight_seq %>%
    add_fitted_draws(b4.3) %>%
    mutate(type = "Centered"),
  weight_seq %>%
    add_fitted_draws(b4.3_nc) %>%
    mutate(type = "Non-centered")
) %>%
  ungroup()

bands <- fits %>%
  group_by(type, weight) %>%
  median_qi(.value, .width = c(.67, .89, .97))

lines <- fits %>%
  filter(.draw %in% sample(unique(.data$.draw), size = 50))

ggplot(lines, aes(x = weight)) +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions, aes(ymin = Q2.5, ymax = Q97.5), alpha = 0.3) +
  geom_lineribbon(data = bands, aes(y = .value, ymin = .lower, ymax = .upper),
                  color = NA) +
  scale_fill_brewer(palette = "Blues", breaks = c(.67, .89, .97)) +
  geom_line(aes(y = .value, group = .draw)) +
  geom_point(data = dat, aes(y = height), shape = 1, alpha = 0.7) +
  labs(x = "Weight", y = "Height", fill = "Interval") +
  theme(legend.position = "bottom") +
  transition_states(.draw, 0, 1)
```

> **4M8.** In the chapter, we used 15 knots with the cherry blossom spline. Increase the number of know and observe what happens to the resulting spline. Then adjust also the width of the prior on the weights---change the standard deviation of the prior and watch what happens. What do you think the combination of know number and hte prior on the weights controls?

Because {brms} estimates splines a little differently than {rethinking} (see [here](https://bookdown.org/connect/#/apps/4857/access)), I'll use {rethinking} for estimating the spline models. First lets look again at the 15-knot spline model, and then double the number of knots and play with the prior. As expected, increasing the number of knots increases the "wiggly-ness" of the spline. We can also see that tightening the prior on the weights takes away some of the "wiggly-ness". 

```{r e4m8-1}
library(splines)
library(tidybayes.rethinking)
library(colorblindr)

data(cherry_blossoms)
dat <- cherry_blossoms %>%
  drop_na(doy)

# original m4.7 model
knots_15 <- quantile(dat$year, probs = seq(0, 1, length.out = 15))
B_15 <- bs(dat$year, knots = knots_15[-c(1, 15)],
        degree = 3, intercept = TRUE)

m4.7 <- quap(alist(D ~ dnorm(mu, sigma),
                   mu <- a + B_15 %*% w,
                   a ~ dnorm(100, 10),
                   w ~ dnorm(0, 10),
                   sigma ~ dexp(1)),
             data = list(D = dat$doy, B = B_15),
             start = list(w = rep(0, ncol(B_15))))

# double the number of knots
knots_30 <- quantile(dat$year, probs = seq(0, 1, length.out = 30))
B_30 <- bs(dat$year, knots = knots_30[-c(1, 30)],
        degree = 3, intercept = TRUE)

m4.7_30 <- quap(alist(D ~ dnorm(mu, sigma),
                      mu <- a + B_30 %*% w,
                      a ~ dnorm(100, 10),
                      w ~ dnorm(0, 10),
                      sigma ~ dexp(1)),
                data = list(D = dat$doy, B_30 = B_30),
                start = list(w = rep(0, ncol(B_30))))

# and modify the prior
m4.7_30_tight_prior <- quap(alist(D ~ dnorm(mu, sigma),
                                  mu <- a + B_30 %*% w,
                                  a ~ dnorm(100, 10),
                                  w ~ dnorm(0, 2),
                                  sigma ~ dexp(1)),
                            data = list(D = dat$doy, B_30 = B_30),
                            start = list(w = rep(0, ncol(B_30))))

# create plot data
spline_15 <- fitted_draws(m4.7, newdata = dat) %>%
  group_by(.row) %>%
  slice_sample(n = 1000) %>%
  group_by(year) %>%
  median_hdci(.value, .width = 0.89) %>%
  mutate(knots = "15 knots")

spline_30 <- fitted_draws(m4.7_30, newdata = dat) %>%
  group_by(.row) %>%
  slice_sample(n = 1000) %>%
  group_by(year) %>%
  median_hdci(.value, .width = 0.89) %>%
  mutate(knots = "30 knots")

spline_30_tight_prior <- fitted_draws(m4.7_30_tight_prior, newdata = dat) %>%
  group_by(.row) %>%
  slice_sample(n = 1000) %>%
  group_by(year) %>%
  median_hdci(.value, .width = 0.89) %>%
  mutate(knots = "30 knots; Tight prior")

all_splines <- bind_rows(spline_15, spline_30, spline_30_tight_prior)

ggplot(dat, aes(x = year)) +
  geom_point(aes(y = doy), alpha = 0.2) +
  geom_ribbon(data = all_splines, aes(ymin = .lower, ymax = .upper),
              alpha = 0.7, fill = palette_OkabeIto[1]) +
  facet_wrap(~knots, ncol = 1) +
  labs(x = "Year", y = "Day in Year")
```


## Session Info {-}

<details><summary>View the session information used to render this chapter.</summary>
```{r 01-session-info}
devtools::session_info()
```
</details>
