# Ordered Categories and Multilevel Models {#week-6}

The sixth week covers [Chapter 12 (Monsters and Mixtures)](https://bookdown.org/content/4857/monsters-and-mixtures.html) and [Chapter 13 (Models With Memory)](https://bookdown.org/content/4857/models-with-memory.html). Chapter 13 is also covered in [Week 7](#week-7), so those exercises are included in that week.

## Lectures

Lecture 11:

```{r lecture-11, echo = FALSE, out.width = "100%"}
knitr::include_url("https://www.youtube.com/embed/-397DMPooR8")
```

Lecture 12:

```{r lecture-12, echo = FALSE, out.width = "100%"}
knitr::include_url("https://www.youtube.com/embed/SocRgsf202M")
```

## Exercises

### Chapter 12

:::question
> **12E1.** What is the difference between an *ordered* categorical variable and an unordered one? Define and then give an example of each.
:::

Both ordered and unordered variables are constrained to discrete values. However, ordered variables have a fixed order to the magnitudes, although not necessarily a fixed distance between values. For example, education level (high school, undergraduate, postgraduate), population density (rural, suburban, urban), and socioeconomic status (low, high) could all be conceived of as. Unordered variables have no ordering. For example, gender (male, female, non-binary) and race (African American, Asian, White, etc.) have no ordering. That is, no categories is "more" or "greater" than another.


:::question
> **12E2.** What kind of link function does an ordered logistic regression employ? How does it differ from an ordinary logit link?
:::

And ordered logistic regression uses the cumulative logit link function. Whereas the normal logit link can be used the represent a discrete probability of a single event, the cumulative logit link represents the cumulative probability of an event. That is, the linear model with the cumulative logit link function defines the log-odds of the the specified category *or lower*.


:::question
> **12E3.** When count data are zero-inflated, using a model that ignores zero-inflation will tend to induce which kind of inferential error?
:::

Ignoring zero-inflation will result in an underestimate of the event rate. If there are additional zeros, the mean will necessarily be lower. So if we treat the data as a single process, rather than multiple processes, we will estimate a lower mean rate.


:::question
> **12E4.** Over-dispersion is common in count data. Give an example of a natural process that might produce over-dispersed counts. Can you also give an example of a process that might produce *under*-dispersed counts?
:::

Over-dispersion occurs whenever there is variation in the rates. This could be common, for example, in restaurants. So you work at a large restaurant chain and are estimating the number of desserts sold across all of the franchises over the last month. The aggregated counts will likely be over-dispersed because some franchises sell more desserts than others (i.e., the average rate is not the same across franchises).

Under-dispersion is the opposite effect. This is when there is less variation than might be expected. This can occur when there is high auto-correlation between observed counts. Continuing with the restaurant theme, imagine we are estimating how many orders are filled an hour. It's plausible that the rate at which orders are filled would be partially dependent on how many orders are waiting to be filled (i.e., we might work faster if there are a lot of people waiting). In this type of queued-data model, the counts will be highly correlated, and my be under-dispersed.


:::question
> **12M1.** At a certain university, employees are annually rated from 1 to 4 on their productivity, with 1 being least productive and 4 most productive. In a certain department at this university in a certain year, the numbers of employees receiving each rating were (from 1 to 4): 12, 36, 7, 41. Compute the log cumulative odds of each rating.
:::

```{r e12m1-1}
# raw counts
counts <- c(12, 36, 7, 41)
counts

# raw proportions
props <- counts / sum(counts)
props

# cumulative proportions
cum_props <- cumsum(props)
cum_props

# cumulative log odds
log(cum_props / (1 - cum_props))
```


:::question
> **12M2.** Make a version of Figure 12.5 for the employee ratings data given just above.
:::

```{r e12m2-1}
tibble(rating = c(1:4),
       cum_props = cum_props,
       start = lag(cum_props, default = 0)) %>% 
  ggplot(aes(x = rating, y = cum_props)) +
  geom_point(size = 3) +
  geom_line() +
  geom_segment(aes(x = rating, xend = rating, y = 0, yend = cum_props), size = 2) +
  geom_segment(aes(x = rating + 0.05, xend = rating + 0.05,
                   y = start, yend = cum_props),
               color = "#009FB7", size = 2) +
  geom_text(aes(x = rating + 0.1, y = (start + cum_props) / 2, label = rating),
            color = "#009FB7", size = 4) +
  labs(x = "Rating", y = "Cumulative proportion")
```


:::question
> **12M3.** Can you modify the derivation of the zero-inflated Poisson distribution (ZIPoisson) from the chapter to construct a zero-inflated binomial distribution?
:::

Like the ZIPoisson, the ZIBinomial has two ways that we can observe a 0. There is a single probability that determines whether or not a 0 is observed in the first process, $p_0$. When a 0 is not observed from the first process, we still might observe a 0 from the binomial process. For the binomial, we'll define the probability of success as $q$, and have $n$ trials. Combining the two processes, the probability of any 0 is:

$$
\Pr(0|p_0,q,n)=p_0 + (1 - p_0)(1 - q)^n
$$

In word form, we get a 0 from process one with probability $p_0$. Then, if not from process one (i.e., $(1-p_0)$), we could observe a 0 from the binomial model. That is, the probability of not success is $(1-q)$, raised to the number of trials, $n$.

Similarly, the probability of any non-zero observation is:

$$
\Pr(y|p_0,q,n) = (1-p_0)\frac{n!}{y!(n-y)!}q^y(1-q)^{n-y}
$$

This expression is the the binomial model, with $(1-p_0)$ appended to the front. That is, this is the binomial probability of observing a given value, times the probability that the first process didn't generate a 0.

## Homework
