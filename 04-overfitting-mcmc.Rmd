# Overfitting / MCMC

The fourth week covers [Chapter 7 (Ulysses' Compass)](https://bookdown.org/content/4857/ulysses-compass.html), [Chapter 8 (Conditional Manatees)](https://bookdown.org/content/4857/conditional-manatees.html), and [Chapter 9 (Markov Chain Monte Carlo)](https://bookdown.org/content/4857/markov-chain-monte-carlo.html).

## Lectures

Lecture 7:

```{r lecture-7, echo = FALSE, out.width = "100%"}
knitr::include_url("https://www.youtube.com/embed/odGAAJDlgp8")
```

Lecture 8:

```{r lecture-8, echo = FALSE, out.width = "100%"}
knitr::include_url("https://www.youtube.com/embed/Qqz5AJjyugM")
```

## Exercises

### Chapter 7

:::question
> **7E1.** State the three motivating criteria that define information entropy. Try to express each in your own words.
:::

Information is defined as *the reduction in uncertainty when we learn and outcome*. The motivating criteria for defining information entropy revolve around the measure of uncertainty that is used to derive information.

The first is that the measure of uncertainty must be continuous. This prevents large changes in the uncertainty measure resulting from relatively small changes in probabilities. Such a phenomenon often occurs when researchers use a *p*-value cutoff of .05 to claim "significance." Often, the difference between "significant" and "non-significant" results is itself non-significant `r emo::ji("exploding_head")`.

The second is that the measure of uncertainty should increase as the number of possible events increases. When there are more potential outcomes, there are more predictions that have to be made, and therefore more uncertainty about which outcome will be observed. For example, if your friend asks you to guess a number between 1 and 100, you are much less likely to guess correctly than if you were guessing a number between 1 and 2.

The third and final criteria is that the measure of uncertainty should be additive. These means that if we calculate the uncertainty for two sets of outcomes (e.g., heads or tail on a coin flip and the results of a thrown die), then the uncertainty of combinations of events (e.g., heads and "3") should be equal to the sum of the uncertainties from the two separate events. 

Information entropy is the only function that satisfies all three criteria.

:::question
> **7E2.** Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?
:::

Entropy is the average log-probability of an event. The formula is given as

\begin{equation}
  H(p) = -\text{E}\log(p_i) = -\sum_{i=1}^np_i\log(p_i)
\end{equation}

Thus, for each probability $p_i$, we multiply $p_i$ by $\log(p_i)$, sum all the values, and then multiply the sum by negative one. To implement this, we'll first write a couple of functions to do the calculations. We could do this without functions, but functions will allow us to handle cases where $p_i = 0$, as will be the case in a couple of problems. The first function, `p_logp()`, returns `0` if `p` is 0, and returns `p * log(p)` otherwise. The `calc_entropy()` function is a wrapper around `p_logp()`, applying `p_logp()` to each element of a vector of probabilities, summing the results, and multiplying the sum by -1.

```{r e7e2-1}
p_logp <- function(p) {
  if (p == 0) return(0)
  p * log(p)
}
calc_entropy <- function(x) {
  avg_logprob <- sum(map_dbl(x, p_logp))
  -1 * avg_logprob
}
```

Applying these functions to the probabilities in this problem results in an entropy of about 0.61. Note this is the same as the weather example in the text, because in both cases there were two events with probabilities of 0.3 and 0.7.

```{r e7e2-2}
probs <- c(0.7, 0.3)
calc_entropy(probs)
```

:::question
> **7E3.** Suppose a four-sided die is loaded such that, when tossed onto a table, it shows "1" 20%, "2" 25%, "3" 25%, and "4" 30% of the time. What is the entropy of this die?
:::

Now we have four outcomes. We can reuse our code from above, substituting the new probabilities into the vector `probs`. This results in an entropy of about 1.38. As expected, because there are now more outcomes, the entropy is higher than what was observed in the previous problem.

```{r e7e3}
probs <- c(0.20, 0.25, 0.25, 0.30)
calc_entropy(probs)
```

:::question
> **7E4.** Suppose another four-sided die is loaded such that it never shows "4." The other three sides show equally often. What is the entropy of this die?
:::

Again, we can copy our code from above, replace the probabilities. Even though there are four outcomes specified, there are effectively three outcomes, as the outcome "4" has probability 0. Thus, we would expect entropy to decrease, as there are fewer possible outcomes than in the previous problem. This is indeed what we find, as this die's entropy is about 1.1.

```{r e7e4}
probs <- c(1, 1, 1, 0)
probs <- probs / sum(probs)
probs

calc_entropy(probs)
```

:::question
> **7M1.** Write down and compare the definitions of AIC and WAIC. Which of these criteria is most general? Which assumptions are required to transform the more general criterion into a less general one?
:::

The AIC is defined as follows, where $\text{lppd}$ is the log-pointwise-predictive density, and $p$ is the number of free parameters in the posterior distribution.

$$
\text{AIC} = -2\text{lppd} + 2p
$$

In contrast, the WAIC is defined as:

$$
\text{WAIC}(y,\Theta) = -2\Big(\text{lppd} - \sum_i \text{var}_{\theta}\log p(y_i | \theta)\Big)
$$

If we distribute the $-2$ through, this looks remarkably similar to the AIC formula, with the exception of the final $p$ term. Whereas the AIC uses 2 times the number of free parameters, the WAIC uses 2 times the sum of the log-probability variances from each observation.

The WAIC is more general than the AIC, as the AIC assumes that priors are flat or overwhelmed by the likelihood, the posterior distribution is approximately multivariate Gaussian, and the sample size is much greater than the number of parameters. If all of these assumptions are met, then we would expect the AIC and WAIC to be about the same.

:::question
> **7M2.** Explain the difference between model *selection* and model *comparison*. What information is lost under model selection?
:::

Model selection refers to just picking the model that has the lowest (i.e., best) criterion value and discarding other models. When we take this approach, we lose information about the relative model accuracy that can be seen across the criterion values for the candidate models. This information can inform how confident we are in the models. Additionally, the model selection paradigm cares only about predictive accuracy and ignores causal inference. Thus, a model may be selected that has confounds or that would not correctly inform an intervention.

In contrast, model comparison uses multiple models to understand how the variables included influence prediction and affect implied conditional independencies in a causal model. Thus, we preserve information and can make more holistic judgments about our data and models.

:::question
> **7M3.** When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.
:::

All of the information criteria are defined based on the log-pointwise-predictive density, defined as follows, where $y$ is the data, $\Theta$ is the posterior distribution, $S$ is the number of samples, and $I$ is the number of samples.

$$
\text{lppd}(y,\Theta) = \sum_i\log\frac{1}{S}\sum_sp(y_i|\Theta_s)
$$

In words, this means take the log of the average probability across samples of each observation $i$ and sum them together. Thus, a larger sample size will necessarily lead to a smaller log-pointwise-predictive-density, even if the data generating process and models are exactly equivalent (i.e., when the LPPD values are negative, the sum will get more negative as the sample size increases). More observations are entered into the sum, leading to a smaller final lppd, which will in turn increase the information criteria. We can run a quick simulation to demonstrate. For three different sample sizes, we'll simulate 100 data sets from the exact same data generation process, estimate a the exact same linear model, and then calculate the LPPD, WAIC, and PSIS for each.

Visualizing the distribution of the LPPD, WAIC, and PSIS across simulated data sets with each sample size, we see that the LPPD gets more negative as the sample size increases, even though the data generation process and estimated model are identical. Accordingly, the WAIC and PSIS increase. Note that the WAIC and PSIS values are approximately $-2 \times \text{lppd}$. Thus, if we fit one model with 100 observations and second model with 1,000 observations, we might conclude from the WAIC and PSIS that the first model with 100 observations has much better predictive accuracy, because the WAIC and PSIS values are lower. However, this would be only an artifact of different sample sizes, and may not actually represent true differences between the models.

<details><summary>Simulation Code</summary>

```{r e7m3-1, eval = !file.exists(here("fits", "chp7", "b7m3-sim.rds"))}
gen_data <- function(n) {
  tibble(x1 = rnorm(n = n)) %>%
    mutate(y = rnorm(n = n, mean = 0.3 + 0.8 * x1),
           across(everything(), standardize))
}
fit_model <- function(dat) {
  suppressMessages(output <- capture.output(
    mod <- brm(y ~ 1 + x1, data = dat, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = "b"),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 3000, chains = 4, cores = 4, seed = 1234)
  ))
  
  return(mod)
}
calc_info <- function(model) {
  mod_lppd <- log_lik(model) %>% 
    as_tibble(.name_repair = "minimal") %>% 
    set_names(paste0("obs_", 1:ncol(.))) %>% 
    rowid_to_column(var = "iter") %>% 
    pivot_longer(-iter, names_to = "obs", values_to = "logprob") %>% 
    mutate(prob = exp(logprob)) %>% 
    group_by(obs) %>% 
    summarize(log_prob_score = log(mean(prob))) %>% 
    pull(log_prob_score) %>% 
    sum()
  
  mod_waic <- suppressWarnings(waic(model)$estimates["waic", "Estimate"])
  mod_psis <- suppressWarnings(loo(model)$estimates["looic", "Estimate"])
  
  tibble(lppd = mod_lppd, waic = mod_waic, psis = mod_psis)
}

sample_sim <- tibble(sample_size = rep(c(100, 500, 1000), each = 100)) %>%
  mutate(sample_data = map(sample_size, gen_data),
         model = map(sample_data, fit_model),
         infc = map(model, calc_info)) %>%
  select(-sample_data, -model) %>% 
  unnest(infc) %>% 
  write_rds(here("fits", "chp7", "b7m3-sim.rds"), compress = "gz")
```

```{r e7m3-2, include = FALSE, eval = file.exists(here("fits", "chp7", "b7m3-sim.rds"))}
sample_sim <- read_rds(here("fits", "chp7", "b7m3-sim.rds"))
```

```{r e7m3-3, fig.show = "hide"}
library(ggridges)

sample_sim %>%
  pivot_longer(cols = c(lppd, waic, psis)) %>%
  mutate(sample_size = fct_inorder(as.character(sample_size)),
         name = str_to_upper(name),
         name = fct_inorder(name)) %>%
  ggplot(aes(x = value, y = sample_size)) +
  facet_wrap(~name, nrow = 1, scales = "free_x") +
  geom_density_ridges(bandwidth = 4) +
  scale_y_discrete(expand = c(0, .1)) +
  scale_x_continuous(breaks = seq(-2000, 3000, by = 750)) +
  coord_cartesian(clip = "off") +
  labs(x = "Value", y = "Sample Size")
```

</details>

```{r e7m3-3-show, echo = FALSE}
knitr::include_graphics(knitr::fig_chunk("e7m3-3", "png"))
```


:::question
> **7M4.** What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.
:::

The penalty term of the WAIC, $p_{\Tiny\text{WAIC}}$ is defined as shown in the WAIC formula. Specifically, the penalty term is sum of the variances of the log probabilities for each observation.

$$
\text{WAIC}(y,\Theta) = -2\Big(\text{lppd} - \underbrace{\sum_i \text{var}_{\theta}\log p(y_i | \theta)}_{\text{penalty term}}\Big)
$$

Smaller variances in log probabilities will results in a lower penalty. If we restrict the prior to become more concentrated, we restrict the plausible range of the parameters. In other words, we restrict the variability in the posterior distribution. As the parameters become more consistent, the log probability of each observation will necessarily become more consistent also. Thus, the penalty term, or effective number of parameters, becomes smaller. We can again confirm with a small simulation.

In this simulation we keep everything constant (i.e., data generation, model), with the exception of the prior for the slope coefficients. We'll try three different priors: $\text{Normal}(0, 0.1)$, $\text{Normal}(0, 1)$, and $\text{Normal}(0, 10)$. Visualizing the results, we can see that the more constricted prior does indeed result in a smaller penalty or effective number of parameters.

<details><summary>Simulation Code</summary>

```{r e7m4-1, eval = !file.exists(here("fits", "chp7", "b7m4-sim.rds"))}
gen_data <- function(n) {
  tibble(x1 = rnorm(n = n),
         x2 = rnorm(n = n),
         x3 = rnorm(n = n)) %>%
    mutate(y = rnorm(n = n, mean = 0.3 + 0.8 * x1 +
                       0.6 * x2 + 1.2 * x3),
           across(everything(), standardize))
}
fit_model <- function(dat, p_sd) {
  suppressMessages(output <- capture.output(
    mod <- brm(y ~ 1 + x1 + x2 + x3, data = dat,
               family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior_string(glue("normal(0, {p_sd})"), class = "b"),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 3000, chains = 4, cores = 4, seed = 1234)
  ))
  
  return(mod)
}
calc_info <- function(model) {
  w <- suppressWarnings(brms::waic(model))
  p <- suppressWarnings(brms::loo(model))
  
  tibble(p_waic = w$estimates["p_waic", "Estimate"],
         p_loo = p$estimates["p_loo", "Estimate"])
}

prior_sim <- tibble(sample_size = 20,
                    prior_sd = rep(c(0.1, 1, 10), each = 100)) %>%
  mutate(sample_data = map(sample_size, gen_data),
         model = map2(sample_data, prior_sd, fit_model),
         infc = map(model, calc_info)) %>%
  select(-sample_data, -model) %>% 
  unnest(infc) %>% 
  write_rds(here("fits", "chp7", "b7m4-sim.rds"), compress = "gz")
```

```{r e7m4-2, include = FALSE, eval = file.exists(here("fits", "chp7", "b7m4-sim.rds"))}
prior_sim <- read_rds(here("fits", "chp7", "b7m4-sim.rds"))
```

```{r e7m4-3, fig.show = "hide"}
prior_sim %>%
  pivot_longer(cols = c(p_waic, p_loo)) %>%
  mutate(prior_sd = glue("&sigma; = {prior_sd}"),
         prior_sd = fct_inorder(prior_sd),
         name = factor(name, levels = c("p_waic", "p_loo"),
                       labels = c("p<sub>WAIC</sub>", "p<sub>PSIS</sub>"))) %>%
  ggplot(aes(x = value, y = prior_sd, height = stat(density))) +
  facet_wrap(~name, nrow = 1) +
  geom_density_ridges(stat = "binline", bins = 20) +
  labs(x = "Effective Parameters", y = "Prior") +
  theme(axis.text.y = element_markdown())
```

</details>

```{r e7m4-3-show, echo = FALSE}
knitr::include_graphics(knitr::fig_chunk("e7m4-3", "png"))
```

:::question
> **7M5.** Provide an informal explanation of why informative priors reduce overfitting.
:::

Informative priors restrict the plausible values for parameters. By using informative priors, we can limit the values of parameters to values that are reasonable, given our scientific knowledge. Thus, we can keep the model from learning too much from our specific sample.

:::question
> **7M6.** Provide an informal explanation of why overly informative priors result in underfitting.
:::

In contrast to the previous question, making the prior too informative can be too restrictive on the parameter space. This prevents our model from learning enough from our sample. We basically just get our prior distributions back, without learning anything from the data that could help make future predictions.

:::question
> **7H1.** In 2007, *The Wall Street Journal* published an editorial ("We're Number One, Alas") with a graph of corportate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed at right), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in `data(Laffer)`. Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue.
:::

First, let's standardize the data and fit a straight line, a quadratic line, and a spline model.

```{r e7h1-1}
data(Laffer)

laf_dat <- Laffer %>%
  mutate(across(everything(), standardize),
         tax_rate2 = tax_rate ^ 2)

laf_line <- brm(tax_revenue ~ 1 + tax_rate, data = laf_dat, family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                file = here("fits", "chp7", "b7h1-line.rds"))

laf_quad <- brm(tax_revenue ~ 1 + tax_rate + tax_rate2, data = laf_dat,
                family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                file = here("fits", "chp7", "b7h1-quad.rds"))

laf_spln <- brm(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), data = laf_dat,
                family = gaussian,
                prior = c(prior(normal(0, 0.2), class = Intercept),
                          prior(normal(0, 0.5), class = b),
                          prior(normal(0, 0.5), class = sds),
                          prior(exponential(1), class = sigma)),
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                control = list(adapt_delta = 0.95),
                file = here("fits", "chp7", "bh71-spln.rds"))
```

Let's visualize the models:

<details><summary>Code to reproduce</summary>

```{r e7h1-2, fig.show = "hide", cache = TRUE}
tr_seq <- tibble(tax_rate = seq(0, 40, length.out = 100)) %>%
  mutate(tax_rate = (tax_rate - mean(Laffer$tax_rate)) / sd(Laffer$tax_rate),
         tax_rate2 = tax_rate ^ 2)

predictions <- bind_rows(
  predicted_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Linear"),
  predicted_draws(laf_quad, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)

fits <- bind_rows(
  epred_draws(laf_line, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Linear"),
  epred_draws(laf_quad, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Quadratic"),
  epred_draws(laf_spln, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .epred, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = laf_dat, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                    breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue",
       fill = "Interval")
```

</details>

```{r e7h1-2-show, echo = FALSE, out.width = "95%"}
knitr::include_graphics(knitr::fig_chunk("e7h1-2", "png"))
```


They all look pretty similar, but the quadratic and spline models do show a slight curve. Next, we can look at the PSIS (called LOO in {brms} and {rstan}) and WAIC comparisons. Neither the PSIS or WAIC is really able to differentiate the models in a meaningful way. However, it should be noted that both the PSIS and WAIC have Pareto or penalty values that are exceptionally large, which could make the criteria unreliable.

```{r e7h1-3}
library(loo)

laf_line <- add_criterion(laf_line, criterion = c("loo", "waic"))
laf_quad <- add_criterion(laf_quad, criterion = c("loo", "waic"))
laf_spln <- add_criterion(laf_spln, criterion = c("loo", "waic"))

loo_compare(laf_line, laf_quad, laf_spln, criterion = "waic")
loo_compare(laf_line, laf_quad, laf_spln, criterion = "loo")
```

:::question
> **7H2.** In the `Laffer` data, there is one country with a high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Student's t distribution to revist the curve fitting problem. How much does a curved relationship depend upon the outlier point.
:::

Because I used `brms::brm()` to estimate the models, we can't use the convenience functions to get the pointwise values for the PSIS and WAIC that are available in the {rethinking} package. So I'll write my own, called `criteria_influence()`. When we plot the Pareto *k* and $p_{\Tiny\text{WAIC}}$ values, we see that observation 12 is problematic in all three models.

```{r e7h2-1, out.width = "95%"}
library(gghighlight)

criteria_influence <- function(mod) {
  tibble(pareto_k = mod$criteria$loo$diagnostics$pareto_k,
         p_waic = mod$criteria$waic$pointwise[, "p_waic"]) %>%
    rowid_to_column(var = "obs")
}

influ <- bind_rows(
  criteria_influence(laf_line) %>%
    mutate(type = "Linear"),
  criteria_influence(laf_quad) %>%
    mutate(type = "Quadratic"),
  criteria_influence(laf_spln) %>%
    mutate(type = "Spline")
)

ggplot(influ, aes(x = pareto_k, y = p_waic)) +
  facet_wrap(~type, nrow = 1) +
  geom_vline(xintercept = 0.7, linetype = "dashed") +
  geom_hline(yintercept = 0.4, linetype = "dashed") +
  geom_point() +
  gghighlight(pareto_k > 0.7 | p_waic > 0.4, n = 1, label_key = obs,
              label_params = list(size = 3)) +
  labs(x = "Pareto *k*", y = "p<sub>WAIC</sub>")
```

Let's refit the model using a Student's t distribution to put larger tails on our outcome distribution, and then visualize our new models.

```{r e7h2-2}
laf_line2 <- brm(bf(tax_revenue ~ 1 + tax_rate, nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                 file = here("fits", "chp7", "b7h2-line.rds"))

laf_quad2 <- brm(bf(tax_revenue ~ 1 + tax_rate + tax_rate2, nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                 file = here("fits", "chp7", "b7h2-quad.rds"))

laf_spln2 <- brm(bf(tax_revenue ~ 1 + s(tax_rate, bs = "bs"), nu = 1),
                 data = laf_dat, family = student,
                 prior = c(prior(normal(0, 0.2), class = Intercept),
                           prior(normal(0, 0.5), class = b),
                           prior(normal(0, 0.5), class = sds),
                           prior(exponential(1), class = sigma)),
                 iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                 control = list(adapt_delta = 0.99),
                 file = here("fits", "chp7", "bh72-spln.rds"))
```

<details><summary>Code to reproduce</summary>

```{r e7h2-3, fig.show = "hide", cache = TRUE}
predictions <- bind_rows(
  predicted_draws(laf_line2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Linear"),
  predicted_draws(laf_quad2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Quadratic"),
  predicted_draws(laf_spln2, newdata = tr_seq) %>%
    median_qi(.width = 0.89) %>%
    mutate(type = "Spline")
)

fits <- bind_rows(
  epred_draws(laf_line2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Linear"),
  epred_draws(laf_quad2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Quadratic"),
  epred_draws(laf_spln2, newdata = tr_seq) %>%
    median_qi(.width = c(0.67, 0.89, 0.97)) %>%
    mutate(type = "Spline")
)

ggplot() +
  facet_wrap(~type, nrow = 1) +
  geom_ribbon(data = predictions,
              aes(x = tax_rate, ymin = .lower, ymax = .upper),
              alpha = 0.2) +
  geom_lineribbon(data = fits,
                  aes(x = tax_rate, y = .epred, ymin = .lower, ymax = .upper),
                  size = 0.6) +
  geom_point(data = laf_dat, aes(x = tax_rate, y = tax_revenue),
             alpha = 0.5) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                    breaks = c(0.67, 0.89, 0.97)) +
  labs(x = "Standardized Tax Rate", y = "Standardized Tax Revenue",
       fill = "Interval")
```

</details>

```{r e7h2-3-show, echo = FALSE, out.width = "95%"}
knitr::include_graphics(knitr::fig_chunk("e7h2-3", "png"))
```


The prediction intervals are a little bit narrower, which makes sense as the predictions are no longer being as influenced by the outlier. When we look at the new PSIS and WAIC estimates, we are no longer getting warning messages about large Pareto *k* values; however, we do still see warnings about large $p_{\Tiny\text{WAIC}}$ values. The comparisons also tell the same story as before, with no distinguishable differences between the models.

```{r 7h2-4}
laf_line2 <- add_criterion(laf_line2, criterion = c("loo", "waic"))
laf_quad2 <- add_criterion(laf_quad2, criterion = c("loo", "waic"))
laf_spln2 <- add_criterion(laf_spln2, criterion = c("loo", "waic"))

loo_compare(laf_line2, laf_quad2, laf_spln2, criterion = "waic")
loo_compare(laf_line2, laf_quad2, laf_spln2, criterion = "loo")
```

:::{.question .code-question}
> **7H3.** Consider three fictional Polynesian islands. On each there is a Royal Ornithologist charged by the king with surveying the bird population. They have each found the following proportions of 5 important bird species:

```{r e7h3-intro, message = FALSE, echo = FALSE, wrap = "table-question"}
library(kableExtra)
tibble(island = paste("Island", 1:3),
       a = c(0.2, 0.8, 0.05),
       b = c(0.2, 0.1, 0.15),
       c = c(0.2, 0.05, 0.7),
       d = c(0.2, 0.025, 0.05),
       e = c(0.2, 0.025, 0.05)) %>%
  mutate(across(where(is.double), ~sprintf("%0.3f", .x))) %>%
  kbl(align = c("l", rep("c", 5)), linesep = "",
      col.names = c(" ", paste("Species", LETTERS[1:5]))) %>%
  kable_styling()
```

> Notice that each row sums to 1, all the birds. This problem has two parts. It is not computationally complicated. But it is conceptually tricky. First, compute the entropy of each island's bird distribution. Interpret these entropy values. Second, use each island's bird distribution to predict the other two. This means to compute the KL divergence of each island from the others, treating each island as if it were a statistical model of the other islands. You should end up with 6 different KL divergence values. Which island predicts the others best? Why?
:::

First, lets compute the entropy for each each island.

```{r e7h3-1}
islands <- tibble(island = paste("Island", 1:3),
       a = c(0.2, 0.8, 0.05),
       b = c(0.2, 0.1, 0.15),
       c = c(0.2, 0.05, 0.7),
       d = c(0.2, 0.025, 0.05),
       e = c(0.2, 0.025, 0.05)) %>%
  pivot_longer(-island, names_to = "species", values_to = "prop")

islands %>%
  group_by(island) %>%
  summarize(prop = list(prop), .groups = "drop") %>%
  mutate(entropy = map_dbl(prop, calc_entropy))
```

The first island has the highest entropy. This is expected, because it has the most even distribution of bird species. All species are equally likely, so the observation of any one species is not surprising. In contrast, Island 2 has the lowest entropy. This is because the vast majority of birds on this island are Species A. Therefore, observing a bird that is not from Species A would be surprising.

For the second part of the question, we need to compute the KL divergence for each pair of islands. The KL divergence is defined as:

$$
D_{KL} = \sum_i p_i(\log(p_i) - \log(q_i))
$$

We'll write a function to do this calculation.

```{r e7h3-2}
d_kl <- function(p, q) {
  sum(p * (log(p) - log(q)))
}
```

Now, let's calculate $D_{KL}$ for each set of islands.

```{r e7h3-3}
crossing(model = paste("Island", 1:3),
         predicts = paste("Island", 1:3)) %>%
  filter(model != predicts) %>%
  left_join(islands, by = c("model" = "island")) %>%
  rename(model_prop = prop) %>%
  left_join(islands, by = c("predicts" = "island", "species")) %>%
  rename(predict_prop = prop) %>%
  group_by(model, predicts) %>%
  summarize(q = list(model_prop),
            p = list(predict_prop),
            .groups = "drop") %>%
  mutate(kl_distance = map2_dbl(p, q, d_kl))
```

These results show us that when using Island 1 to predict Island 2, the KL divergence is about 0.87. When we use Island 1 to predict Island 3, the KL divergence is about 0.63, and so on. Overall, the distances are shorter when we used Island 1 as the model. This is because Island 1 has the highest entropy. Thus, we are less surprised by the other islands, so there's a shorter distance. In contrast, Island 2 and Island 3 have very concentrated distributions, so predicting the other islands leads to more surprises, and therefore greater distances.

:::question
> **7H4.** Recall the marriage, age, and happiness collider bias example from Chapter 6. Run models `m6.9` and `m6.10` again (page 178). Compare these two models using WAIC (or PSIS, they will produce identical results). Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness? Can you explain why the answers to these two questions disagree?
:::

As a reminder, here is the DAG for this example, where $H$ is happiness, $M$ is marriage, and $A$ is age.

```{r e7h4-1, message = FALSE, out.width = "40%"}
library(dagitty)
library(ggdag)

hma_dag <- dagitty("dag{H -> M <- A}")
coordinates(hma_dag) <- list(x = c(H = 1, M = 2, A = 3),
                             y = c(H = 1, M = 1, A = 1))

ggplot(hma_dag, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = "black", size = 10) +
  geom_dag_edges(edge_color = "black", edge_width = 2,
                 arrow_directed = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  theme_void()
```

First, let's regenerate the data and estimate the models.

```{r e7h4-2, cache = TRUE}
d <- sim_happiness(seed = 1977, N_years = 1000)
dat <- d %>%
  filter(age > 17) %>%
  mutate(a = (age - 18) / (65 - 18),
         mid = factor(married + 1, labels = c("single", "married")))

b6.9 <- brm(happiness ~ 0 + mid + a, data = dat, family = gaussian,
            prior = c(prior(normal(0, 1), class = b, coef = midmarried),
                      prior(normal(0, 1), class = b, coef = midsingle),
                      prior(normal(0, 2), class = b, coef = a),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp7", "b7h4-6.9"))

b6.10 <- brm(happiness ~ 1 + a, data = dat, family = gaussian,
             prior = c(prior(normal(0, 1), class = Intercept),
                       prior(normal(0, 2), class = b, coef = a),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
             file = here("fits", "chp7", "b7h4-6.10"))
```

For the model comparison we'll use PSIS.

```{r e7h4-3}
b6.9 <- add_criterion(b6.9, criterion = "loo")
b6.10 <- add_criterion(b6.10, criterion = "loo")

loo_compare(b6.9, b6.10)
```

PSIS shows a strong preference for `b6.9`, which is the model that includes both age and marriage status. However, `b6.10` provides the correct causal inference, as no additional conditioning is needed.

```{r e7h4-4}
adjustmentSets(hma_dag, exposure = "A", outcome = "H")
```

The reason is that in this model, marital status is a collider. Adding this variable to the model add a real statistical association between happiness and age, which improves the predictions that are made. However, the association is not causal, so intervening on age (if that were possible), would not actually change happiness. Therefore it's important to consider the causal implications of your model before selecting one based on PSIS or WAIC alone.

:::question
> **7H5.** Revisit the urban fox data, `data(foxes)`, from the previous chapter's practice problems. Use WAIC or PSIS based model comparison on five different models, each using `weight` as the outcome, and containing these sets of predictor variables:  

> (1) `avgfood + groupsize + area`  
> (2) `avgfood + groupsize`  
> (3) `groupsize + area`  
> (4) `avgfood`  
> (5) `area`  

> Can you explain the relative differences in WAIC scores, using the fox DAG from the previous chapter? Be sure to pay attention to the standard error of the score differences (`dSE`).
:::

First, let's estimate the five models.

```{r e7h5-1}
data(foxes)

fox_dat <- foxes %>%
  as_tibble() %>%
  select(area, avgfood, weight, groupsize) %>%
  mutate(across(everything(), standardize))

b7h5_1 <- brm(weight ~ 1 + avgfood + groupsize + area, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "chp7", "b7h5_1"))

b7h5_2 <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "chp7", "b7h5_2"))

b7h5_3 <- brm(weight ~ 1 + groupsize + area, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "chp7", "b7h5_3"))

b7h5_4 <- brm(weight ~ 1 + avgfood, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "chp7", "b7h5_4"))

b7h5_5 <- brm(weight ~ 1 + area, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "chp7", "b7h5_5"))
```

Then we can calculate the WAIC for each model and the model comparisons.

```{r e7h5-2, warning = FALSE}
b7h5_1 <- add_criterion(b7h5_1, criterion = "waic")
b7h5_2 <- add_criterion(b7h5_2, criterion = "waic")
b7h5_3 <- add_criterion(b7h5_3, criterion = "waic")
b7h5_4 <- add_criterion(b7h5_4, criterion = "waic")
b7h5_5 <- add_criterion(b7h5_5, criterion = "waic")

comp <- loo_compare(b7h5_1, b7h5_2, b7h5_3, b7h5_4, b7h5_5, criterion = "waic")
comp
```

In general, the models don't appear all that different from each other. However, there does seem to be two groups of models: `b7h5_1`, `b7h5_2`, and `b7h5_3` are all nearly identical; and `b7h5_4` and `b7h5_5` are nearly identical.

<details><summary>Code to reproduce</summary>

```{r e7h5-3, fig.show = "hide"}
plot_comp <- comp %>%
  as_tibble(rownames = "model") %>%
  mutate(across(-model, as.numeric),
         model = fct_inorder(model))

waic_val <- plot_comp %>%
  select(model, waic, se = se_waic) %>%
  mutate(lb = waic - se,
         ub = waic + se)

diff_val <- plot_comp %>%
  select(model, waic, se = se_diff) %>%
  mutate(se = se * 2) %>%
  mutate(lb = waic - se,
         ub = waic + se) %>%
  filter(se != 0)

ggplot() +
  geom_pointrange(data = waic_val, mapping = aes(x = waic, xmin = lb, xmax = ub,
                                                 y = fct_rev(model))) +
  geom_pointrange(data = diff_val, mapping = aes(x = waic, xmin = lb, xmax = ub,
                                                 y = fct_rev(model)),
                  position = position_nudge(y = 0.2), shape = 2,
                  color = "#009FB7") +
  labs(x = "Deviance", y = NULL)
```

</details>

```{r e7h5-3-show, echo = FALSE}
knitr::include_graphics(knitr::fig_chunk("e7h5-3", "png"))
```

To understand why this is, we can return to the DAG for this example.

```{r e7h5-fox-dag, out.width = "40%", echo = FALSE}
suppressPackageStartupMessages(library(tidygraph))
suppressPackageStartupMessages(library(ggraph))

fox_dag <- dagitty("dag{ area -> avgfood -> groupsize -> weight <- avgfood }")
coordinates(fox_dag) <- list(x = c(area = 2, avgfood = 1, weight = 2,
                                   groupsize = 3),
                             y = c(area = 3, avgfood = 2, weight = 1,
                                   groupsize = 2))

fox_dag %>%
  tidy_dagitty() %>%
  as_tbl_graph() %>%
  mutate(x = c(2, 1, 3, 2),
         y = c(3, 2, 2, 1)) %>%
  ggraph(x = x, y = y) +
  geom_node_text(aes(label = name), color = "black", size = 10) + 
  geom_edge_link(aes(start_cap = label_rect(node1.name, fontsize = 30),
                     end_cap = label_rect(node2.name, fontsize = 30)),
                 edge_color = "black", edge_width = 2,
                 arrow = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  expand_limits(x = c(0.75, 3.25), y = c(0.9)) +
  theme_graph()
```

The first three models (`b7h5_1`, `b7h5_2`, and `b7h5_3`) all contain `groupsize` and one or both of `area` and `avgfood`. The reason these models is the same is that there are no back-door path from `area` or `avgfood` to `weight`. In other words, the effect of `area` adjusting for `groupsize` is the same as the effect of `avgfood` adjusting for `groupsize`, because the effect of `area` is routed entirely through `avgfood`.

Similarly, the last two models (`b7h5_4` and `b7h5_5`) are also nearly identical because of the relationship of `area` to `avgfood`. Because the effect of `area` is routed entirely through `avgfood`, including only `avgfood` or `area` should result in the same inferences.

### Chapter 8

:::question
> **8E1.** For each of the causal relationships below, name a hypothetical third variable that would lead to an interaction effect.

> (1) Bread dough rises because of yeast.
> (2) Education leads to higher income.
> (3) Gasoline makes a car go.
:::

For the first, the amount of heat can moderate the relationship between yeast and the amount the dough rises. In the second example, ethnicity may give rise to an interaction, as individuals of some races face more systemic challenges (i.e., it's easier for white people with low education to get a job than people of color). Thus, education may have more of an effect for minorities. Finally, a car also requires wheels. Wheels with no gas and gas with no wheels both prevent the car from moving; only with both will the car go.

:::question
> **8E2.** Which of the following explanations invokes an interaction?

> (1) Caramelizing onions requires cooking over low heat and making sure the onions do not dry out.
> (2) A car will go faster when it has more cylinders or when it has better fuel injector.
> (3) Most people acquire their political beliefs from their parents, unless they get them instead from their friends.
> (4) Intelligent animal species tend to be either highly social or have manipulative appendages (hands, tentacles, etc.).
:::

The first example is the only one that is, strictly speaking, an interaction. That is, low heat leads to camelization, conditional on the onions not drying out. In the second statement, it is phrased such that either more cylinders or a better fuel injector will result in faster speed. For number three, this is again phrased as an "or." People acquire beliefs from their parents or their friends. As written, there is nothing to indicate that the influence of the parents' beliefs depends on the influence of the friends' beliefs (or vice versa). Finally, the last statement is also phrased as an "or." There is no indicate that the influence of one factor impacts the influence of the other.


:::question
> **8E3.** For each of the explanations in **8E2**, write a linear model that expresses the stated relationship.
:::

For onion caramelization, the mathematical model is:

$$
\begin{align}
C_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_HH_i + \beta_MM_i + \beta_{HM}H_iM_i
\end{align}
$$

The mathematical model for car speed is very similar:

$$
\begin{align}
S_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_CC_i + \beta_FF_i
\end{align}
$$

There is a similar model for political beliefs:

$$
\begin{align}
B_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_PP_i + \beta_FF_i
\end{align}
$$

And finally the model for intelligence:

$$
\begin{align}
I_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_SS_i + \beta_MM_i
\end{align}
$$

:::question
> **8M1.** Recall the tulips example from the chapter. Suppose another set of treatments adjusted the temperature in the greenhouse over two levels: cold and hot. The data in the chapter were collected at the cold temperature. You find none of the plants grown under the hot temperature developed any blooms at all, regardless of the water and shade levels. Can you explain this result in terms of interactions between water, shade, and temperature?
:::

In the chapter example, we saw that at cool temperatures, the blooms were best predicted by water, shade, and their interaction. Here, we learn that neither water nor light matter at a high temperature. This implies a three-way interaction. Just as in the chapter where water had no effect when there was no light, we see that neither water nor light have an effect when temperature is high.

:::question
> **8M2.** Can you invent a regression equation that would make the bloom size zero, whenever the temperature is hot?
:::

For this model, we can use the interaction model from the chapter with an additional predictor ($C$) that is 0 when the temperature is hot and 1 when the temperature is cold.

$$
\begin{align}
B_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &= C_i \times (\alpha + \beta_WW_i + \beta_SS_i + \beta_{WS}W_iS_i)
\end{align}
$$

:::question
> **8M3.** In parts of North America, ravens depend upon wolves for their food. This is because ravens are carnivorous but cannot usually kill or open carcasses of prey. Wolves however can and do kill and tear open animals, and they tolerate ravens co-feeding at their kills. This species relationship is generally described as a "species interaction." Can you invent a hypothetical set of data on raven population size in which this relationship would manifest as a statistical interaction? Do you think the biological interaction could be linear? Why or why not?
:::

In order to predict raven population size based on wolf population size, we should include data on the territory area for the wolves, the number of wolves, and amount of food available, and finally the number of ravens. This is similar to the types of data included in `data(foxes)`. I would expect the relationship to be non-linear. When there are no wolves, I would expect there to be no ravens. As the number of wolves increases, the number of ravens would also increase. However, eventually the number of wolves would increase to the point that the wolves exhaust the food supply, leaving no food left for the ravens, at which point the raven population would begin to shrink. Thus, if we created a counter factual plot with wolves on the x-axis and ravens on the y-axis, I would expect to see at linear trend at first, but then the raven population level off or drop when there is a large numbers of wolves.

:::question
> **8M4.** Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior asumptions mean for the interaction prior, if anything?
:::

There are a few issues constraining different coefficients with bounded parameters (especially negative parameters) in {brms}. For some details, see [this thread](https://discourse.mc-stan.org/t/negative-prior-distribution/18093/2) on the Stan discourse. To get around this, rather than constrain the shade parameter to be negative, we'll create a new variable which is the opposite of shade, `light` and the corresponding `light_cent`. We can then constrain this parameter to be positive, just as the `water_cent` parameter is constrained to be positive. Finally, because of the new `light` variable the positivity constraints on the main effects, we also want to constrain the interaction term to be positive. That is, if both water and light increase, we expect an additional additive effect.

```{r e8m4-1}
library(rethinking)
data("tulips")

tulip_dat <- tulips %>%
  as_tibble() %>%
  mutate(light = -1 * shade,
         blooms_std = blooms / max(blooms),
         water_cent = water - mean(water),
         shade_cent = shade - mean(shade),
         light_cent = light - mean(light))

b8m4 <- brm(blooms_std ~ 1 + water_cent + light_cent + water_cent:light_cent,
            data = tulip_dat, family = gaussian,
            prior = c(prior(normal(0.5, 0.25), class = Intercept),
                      prior(normal(0, 0.25), class = b, lb = 0),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp8", "b8m4"))

summary(b8m4)
```

Looking at the the posterior predicted blooms, the results are very similar to what we saw in Figure 8.7 from the text. This indicates that the prior constraints did not have a large effect on the predicted values.

```{r e8m4-2}
new_tulip <- crossing(water_cent = -1:1, 
                      light_cent = -1:1)

points <- tulip_dat %>%
  expand(nesting(water_cent, light_cent, blooms_std)) %>%
  mutate(light_grid = glue("light_cent = {light_cent}"))

to_string <- as_labeller(c(`-1` = "Light = -1", `0` = "Light = 0", 
                           `1` = "Light = 1"))

new_tulip %>% 
  add_epred_draws(b8m4, ndraws = 50) %>% 
  ungroup() %>% 
  ggplot(aes(x = water_cent, y = .epred)) +
  facet_wrap(~light_cent, nrow = 1,
             labeller = to_string) +
  geom_line(aes(group = .draw), alpha = 0.4) +
  geom_point(data = points, aes(y = blooms_std), color = "#009FB7") +
  scale_x_continuous(breaks = -1:1) +
  labs(x = "Water (centered)", y = "Blooms (standardized)")
```

Finally, let's look at the prior predictive simulation for this model. Overall the priors might be a little too uninformative, especially for the interaction. The interaction has the most impact in the far right panel of the below figure, and there are many lines that exceed the expected boundaries in this panel.

```{r e8m4-3}
b8m4p <- update(b8m4, sample_prior = "only",
                iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
                file = here("fits", "chp8", "b8m4p.rds"))

new_tulip %>% 
  add_epred_draws(b8m4p, ndraws = 50) %>% 
  ungroup() %>% 
  ggplot(aes(x = water_cent, y = .epred)) +
  facet_wrap(~light_cent, nrow = 1,
             labeller = to_string) +
  geom_line(aes(group = .draw), alpha = 0.3) +
  geom_hline(yintercept = c(0, 1), linetype = "dashed") +
  scale_x_continuous(breaks = -1:1) +
  labs(x = "Water (centered)", y = "Blooms (standardized)")
```


:::question
> **8H1.** Return to the `data(tulips)` example in the chapter. Now include the `bed` variable as a predictor in the interaction model. Don't interact `bed` with the other predictors; just include it as a main effect. Note that `bed` is categorical. So to use it properly, you will need to either construct dummy variables, or rather an index variable, as explained in Chapter 5.
:::

The `bed` variable is already a factor variable in the `tulips` data, so we can just add it to the formula in `brm()`. To use indicator variables instead of a dummy variable, we remove the separate intercept (i.e., `~ 0` in the formula below). Because of what's coming in the next question, we'll use `light` instead of `shade` for this model also.

```{r e8h1}
b8h1 <- brm(blooms_std ~ 0 + water_cent + light_cent + bed + 
              water_cent:light_cent,
            data = tulip_dat, family = gaussian,
            prior = c(prior(normal(0, 0.25), class = b),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp8", "b8h1.rds"))

summary(b8h1)
```

:::question
> **8H2.** Use WAIC to compare the model from **8H1** to a model that omits `bed`. What do you infer from this comparison? Can you reconcile the WAIC results with the posterior distribution of the `bed` coefficients?
:::

For the comparison, we'll compare `b8h1` to the model `b8m4`, which is the model from the chapter, with new prior distributions that constrain the main effect of water to be positive and the main effect of shade to be negative (i.e., the main effect of light is positive).

Model `b8h1` is the preferred model; however, the standard error of the difference is larger than the magnitude of the difference, indicating that the WAIC is not able to meaningfully differentiate between the two models. For predictive purposes, this means that the inclusion of the `bed` variable does not significantly improve the model. It should also be noted that both of the models have some exceptionally large penalty values, so these analyses may be unreliable, and we should consider re-fitting the models with a distribution with fatter tails.

```{r e8h2}
b8m4 <- add_criterion(b8m4, criterion = "waic")
b8h1 <- add_criterion(b8h1, criterion = "waic")

loo_compare(b8m4, b8h1, criterion = "waic") %>%
  print(simplify = FALSE)
```

:::question
> **8H3.** Consider again the `data(rugged)` data on economic development and terrain ruggedness, examined in this chapter. One of the African countries in that example Seychelles, is far outside the cloud of other nations, being a rare country with both relatively high GDP and high ruggedness. Seychelles is also unusual, in that it is a group of islands far from the coast of mainland Africa, and its main economic activity is tourism.

> (a) Focus on model `m8.5` from the chapter. Use WAIC pointwise penalties and PSIS Pareto *k* values to measure relative influence of each country. By these criteria, is Seychelles influencing the results? Are there other nations that are relatively influential? If so, can you explain why?
> (b) Now use robust regression, as described in the previous chapter. Modify `m8.5` to se a Student-t distribution with $\nu = 2$. Does this change the results in a substantial way?
:::

In the text, model `m8.5` uses the tulips data, so I'm assuming this is a typo and we should be looking at model `m8.3`, which is the interaction model from the terrain ruggedness example in the chapter. So, let's first create a {brms} version of model `m8.3`.

```{r e8h3-1}
data("rugged")
rugged_dat <- rugged %>%
  as_tibble() %>%
  select(country, rgdppc_2000, rugged, cont_africa) %>%
  drop_na(rgdppc_2000) %>%
  mutate(log_gdp = log(rgdppc_2000),
         log_gdp_std = log_gdp / mean(log_gdp),
         rugged_std = rugged / max(rugged),
         rugged_std_cent = rugged_std - mean(rugged_std),
         cid = factor(cont_africa, levels = c(1, 0),
                      labels = c("African", "Not African")))

b8h3 <- brm(
  bf(log_gdp_std ~ 0 + a + b * rugged_std_cent,
     a ~ 0 + cid,
     b ~ 0 + cid,
     nl = TRUE),
  data = rugged_dat, family = gaussian,
  prior = c(prior(normal(1, 0.1), class = b, coef = cidAfrican, nlpar = a),
            prior(normal(1, 0.1), class = b, coef = cidNotAfrican, nlpar = a),
            prior(normal(0, 0.3), class = b, coef = cidAfrican, nlpar = b),
            prior(normal(0, 0.3), class = b, coef = cidNotAfrican, nlpar = b),
            prior(exponential(1), class = sigma)),
  iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
  file = here("fits", "chp8", "b8h3.rds")
)

b8h3 <- add_criterion(b8h3, criterion = c("loo", "waic"))

summary(b8h3)
```

Now let's take a look at the Pareto *k* and $p_{\Tiny\text{WAIC}}$ values from the model. Seychelles does appear to be influential. The Pareto $k$ value is not above the above the 0.7 threshold; however, the $p_{\Tiny\text{WAIC}}$ value is above the 0.4 threshold. No other countries have Pareto *k* or $p_{\Tiny\text{WAIC}}$ values over the thresholds.

```{r e8h3-2}
library(gghighlight)

tibble(pareto_k = b8h3$criteria$loo$diagnostics$pareto_k,
       p_waic = b8h3$criteria$waic$pointwise[, "p_waic"]) %>%
  rowid_to_column(var = "obs") %>%
  left_join(rugged_dat %>%
              select(country) %>%
              rowid_to_column(var = "obs"),
            by = "obs") %>%
  ggplot(aes(x = pareto_k, y = p_waic)) +
  geom_vline(xintercept = 0.7, linetype = "dashed") +
  geom_hline(yintercept = 0.4, linetype = "dashed") +
  geom_point() +
  gghighlight(pareto_k > 0.7 | p_waic > 0.4, n = 1, label_key = country,
              label_params = list(size = 3)) +
  labs(x = "Pareto *k*", y = "p<sub>WAIC</sub>")
```

Now for part (b), we'll use a Student-t distribution with $\nu = 2$.

```{r e8h3-3}
b8h3_t <- brm(
  bf(log_gdp_std ~ 0 + a + b * rugged_std_cent,
     a ~ 0 + cid,
     b ~ 0 + cid,
     nu = 2,
     nl = TRUE),
  data = rugged_dat, family = student,
  prior = c(prior(normal(1, 0.1), class = b, coef = cidAfrican, nlpar = a),
            prior(normal(1, 0.1), class = b, coef = cidNotAfrican, nlpar = a),
            prior(normal(0, 0.3), class = b, coef = cidAfrican, nlpar = b),
            prior(normal(0, 0.3), class = b, coef = cidNotAfrican, nlpar = b),
            prior(exponential(1), class = sigma)),
  iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
  file = here("fits", "chp8", "b8h3_t.rds")
)

b8h3_t <- add_criterion(b8h3_t, criterion = c("loo", "waic"))
```

How much does the Student's *t* distribution affect our model? Let's look at the posterior distribution of the difference between African and Non-African countries. In the Student's *t* model, the difference actually increased on average. This is because switching the distribution affects all points, not just the high leverage points that were flagged originally. Therefore, it's not unexpected that the distribution might shift.

```{r e8h3-4}
n_diff <- spread_draws(b8h3, b_b_cidAfrican, b_b_cidNotAfrican) %>% 
  mutate(diff = b_b_cidAfrican - b_b_cidNotAfrican)

t_diff <- spread_draws(b8h3_t, b_b_cidAfrican, b_b_cidNotAfrican) %>% 
  mutate(diff = b_b_cidAfrican - b_b_cidNotAfrican)

ggplot() +
  geom_density(data = n_diff, aes(x = diff, fill = "Normal"),
               color = NA, alpha = 0.6) +
  geom_density(data = t_diff, aes(x = diff, fill = "Student's *t*"),
               color = NA, alpha = 0.6) +
  scale_fill_manual(values = c("#009FB7", "#FED766")) +
  labs(x = "African &minus; Non-African", y = "Density", fill = NULL) +
  theme(legend.text = element_markdown())
```


:::{.question .code-question}
> **8H4.** The values in `data(nettle)` are data on language diversity in 74 nations.^[Data from @nettle1998] The meaning of each column is given below.

> (1) `country`: Name of the country
> (2) `num.lang`: Number of recognized languages spoken
> (3) `area`: Area in square kilometers
> (4) `k.pop`: Population, in thousands
> (5) `num.stations`: Number of weather stations that provided data for the next two columns
> (6) `mean.growing.season`: Average length of growing season, in months
> (7) `sd.growing.season`: Standard deviation of length of growing season, in months

> Use these data to evaluate the hypothesis that language diversity is partly a product of food security. The notion is that, in productive ecologies, people don't need large social networks to buffer them against risk of food shortfalls. The means cultural groups can be smaller and more self-sufficient, leading to more languages per capita. Use the number of languages per capita as the outcome:

```{r e8h4-intro, eval = FALSE}
d$lang.per.cap <- d$num.lang / d$k.pop
```

> Use the logarithm of this new variable as your regression outcome. (A count model would be better here, but you'll learn those later, in Chapter 11.) This problem is open ended, allowing you to decide how you address the hypotheses and the uncertain advice the modeling provides. If you think you need to use WAIC anyplace, please do. If you think you need certain priors, argue for them. If you think you need to plot predictions in a certain way, please do. Just try to honestly evaluate the main effects of both `mean.growing.season` and `sd.growing.season`, as well as their two-way interaction. Here are three parts to help.

> (a) Evaluate the hypothesis that language diversity, as measured by `log(lang.per.cap)`, is positively associated with average length of the growing season, `mean.growing.season`. Consider `log(area)` in your regression(s) as a covariate (not an interaction). Interpret your results.
:::

First, let's calculate some new variables we'll need for these models. We'll also go ahead and standardize everything to make setting the priors a little bit easier.

```{r e8h4-1}
data(nettle)

nettle <- nettle %>%
  as_tibble() %>%
  mutate(lang_per_cap = num.lang / k.pop,
         log_lang_per_cap = log(lang_per_cap),
         log_area = log(area),
         lang_per_cap_std = standardize(log_lang_per_cap),
         area_std = standardize(log_area),
         mean_growing_std = standardize(mean.growing.season),
         sd_growing_std = standardize(sd.growing.season))

nettle
```

We'll start by fitting two models, one with only `mean_growing_std` as a predictor, and one that also includes `area_std`. We can then compare the models using PSIS-LOO. 

```{r e8h4-2}
b8h4a_1 <- brm(lang_per_cap_std ~ mean_growing_std,
               data = nettle, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
               file = here("fits", "chp8", "b8h4a_1.rds"))

b8h4a_2 <- brm(lang_per_cap_std ~ mean_growing_std + area_std,
               data = nettle, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
               file = here("fits", "chp8", "b8h4a_2.rds"))

b8h4a_1 <- add_criterion(b8h4a_1, criterion = "loo")
b8h4a_2 <- add_criterion(b8h4a_2, criterion = "loo")

loo_compare(b8h4a_1, b8h4a_2)
```

We see that the model without area is slightly preferred, but PSIS-LOO isn't really able to distinguish between the models very well. For the purpose of this exercise, we'll continue with model `b8h4a_2`, which includes both predictors. Visualizing the model, we see that, as expected given the model comparisons, area doesn't appear to have much impact. However, there does appear to be a positive relationship between the the mean growing season and the number of languages.

```{r e8h4-3, cache = TRUE}
new_nettle <- crossing(area_std = seq(-4, 4, by = 2),
                       mean_growing_std = seq(-4, 4, by = 1),
                       sd_growing_std = seq(-4, 4, by = 1))

to_string <- as_labeller(c(`-4` = "Area = -4", `-2` = "Area = -2",
                           `0` = "Area = 0", 
                           `2` = "Area = 2", `4` = "Area = 4"))

new_nettle %>% 
  add_epred_draws(b8h4a_2, ndraws = 1000) %>% 
  mean_qi(.width = c(0.67, 0.89, 0.97)) %>% 
  ggplot(aes(x = mean_growing_std, y = .epred, ymin = .lower, ymax = .upper)) +
  facet_wrap(~area_std, nrow = 1, labeller = to_string) +
  geom_lineribbon(color = NA) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.2, length.out = 3)),
                    breaks = c("0.67", "0.89", "0.97")) +
  labs(x = "Mean Growing Season (standardized)",
       y = "Log Languages per Capita (standardized)",
       fill = "Interval")
```


:::question
> (b) Now evaluate the hypothesis that language diversity is negatively associated with the standard deviation of length of growing season, `sd.growing.season`. This hypothesis follows from uncertainty in harvest favoring social insurance through larger social networks and therefore fewer languages. Again, consider `log(area)` as a covariate (not an interaction). Interpret your results.
:::

For the second part, we replace `mean_growing_std` with `sd_growing_std`. Again, we'll fit two models and compare with PSIS-LOO.

```{r e8h4-4}
b8h4b_1 <- brm(lang_per_cap_std ~ sd_growing_std,
               data = nettle, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
               file = here("fits", "chp8", "b8h4b_1.rds"))

b8h4b_2 <- brm(lang_per_cap_std ~ sd_growing_std + area_std,
               data = nettle, family = gaussian,
               prior = c(prior(normal(0, 0.2), class = Intercept),
                         prior(normal(0, 0.5), class = b),
                         prior(exponential(1), class = sigma)),
               iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
               file = here("fits", "chp8", "b8h4b_2.rds"))

b8h4b_1 <- add_criterion(b8h4b_1, criterion = "loo")
b8h4b_2 <- add_criterion(b8h4b_2, criterion = "loo")

loo_compare(b8h4b_1, b8h4b_2)
```

The story here is much the same. PSIS-LOO can't really differentiate between the two models, indicating that area doesn't add too much information. This is again reflected in the visualization. We see that the expected distribution of regression lines is fairly similar for all levels of area. Additionally, we do see a negative relationship between the standard deviation of the growing season and the number of languages, as the question suggested.

```{r e8h4-5, cache = TRUE}
new_nettle %>% 
  add_epred_draws(b8h4b_2, ndraws = 1000) %>% 
  mean_qi(.width = c(0.67, 0.89, 0.97)) %>% 
  ggplot(aes(x = sd_growing_std, y = .epred, ymin = .lower, ymax = .upper)) +
  facet_wrap(~area_std, nrow = 1, labeller = to_string) +
  geom_lineribbon(color = NA) +
  scale_fill_manual(values = ramp_blue(seq(0.9, 0.2, length.out = 3)),
                    breaks = c("0.67", "0.89", "0.97")) +
  labs(x = "Standard Deviation of Growing Season (standardized)",
       y = "Log Languages per Capita (standardized)",
       fill = "Interval")
```

:::question
> (c) Finally, evaluate the hypothesis that `mean.growing.season` and `sd.growing.season` interact to synergistically reduce language diversity. The idea is that, in nations with longer average growing seasons, high variance makes storage and redistribution even more important than it would be otherwise. That way, people can cooperate to preserve and protect windfalls to be used during the droughts.
:::

In the third part of the question, we are asked to add the interaction term. We'll drop area since it does not appear to have an effect in either of the previous parts of this question. 

```{r e8h4-6}
b8h4_c <- brm(lang_per_cap_std ~ mean_growing_std * sd_growing_std,
              data = nettle, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "chp8", "b8h4_c.rds"))

summary(b8h4_c)
```

We see that the interaction is negative. What this means can be seen by visualizing the interaction from both directions. This is shown in the following image. In the top row, we plot the expected effect of mean growing season on languages. We see that there is a positive relationship between mean growing season and languages, except when there is high variance in the growing. Similarly, the bottom row shows the expected effect of standard deviation of the growing season on languages. When the mean growing season is short, there is no effect of the variance on languages. When the mean growing season is long, there is a negative relationship between variance and languages.

<details><summary>Code to reproduce</summary>

```{r e8h4-7, fig.show = "hide", cache = TRUE}
library(patchwork)

new_nettle <- crossing(mean_growing_std = seq(-2, 2, by = 0.5),
                       sd_growing_std = seq(-2, 4, by = 0.5))

int_preds <- new_nettle %>% 
  add_epred_draws(b8h4_c, ndraws = 1000) %>% 
  mean_qi(.width = 0.97)

facet_levels <- seq(-2, 2, by = 2)
sd_facets <- list_along(facet_levels)
for (i in seq_along(sd_facets)) {
  points <- nettle %>% 
    mutate(diff = sd_growing_std - facet_levels[i])
  
  p <- int_preds %>% 
    filter(sd_growing_std == facet_levels[i]) %>% 
    ggplot(aes(x = mean_growing_std, y = .epred, ymin = .lower,
               ymax = .upper)) +
    geom_lineribbon(fill = "#99D8E2", color = "black") +
    geom_point(data = points,
               aes(x = mean_growing_std, y = lang_per_cap_std,
                   alpha = -1 * abs(diff)), size = 0.5,
               inherit.aes = FALSE, show.legend = FALSE) +
    expand_limits(x = c(-2, 2), y = c(-2.5, 3.5)) +
    labs(x = "Mean growing season", y = "Languages",
         subtitle = glue("SD growing season = {facet_levels[i]}")) +
    theme(plot.subtitle = element_text(size = 10))
  
  if (i == 2) {
    p <- p +
      theme(plot.margin = margin(0, 20, 0, 20))
  } else {
    p <- p +
      theme(plot.margin = margin(0, 0, 0, 0))
  }
  
  sd_facets[[i]] <- p
}

mean_facets <- list_along(facet_levels)
for (i in seq_along(mean_facets)) {
  points <- nettle %>% 
    mutate(diff = mean_growing_std - facet_levels[i])
  
  p <- int_preds %>% 
    filter(mean_growing_std == facet_levels[i]) %>% 
    ggplot(aes(x = sd_growing_std, y = .epred, ymin = .lower,
               ymax = .upper)) +
    geom_lineribbon(fill = "#99D8E2", color = "black") +
    geom_point(data = points,
               aes(x = sd_growing_std, y = lang_per_cap_std,
                   alpha = -1 * abs(diff)), size = 0.5,
               inherit.aes = FALSE, show.legend = FALSE) +
    expand_limits(x = c(-2, 2), y = c(-2.5, 3.5)) +
    labs(x = "SD growing season", y = "Languages",
         subtitle = glue("Mean growing season = {facet_levels[i]}")) +
    theme(plot.subtitle = element_text(size = 10))
  
  if (i == 2) {
    p <- p +
      theme(plot.margin = margin(30, 20, 0, 20))
  } else {
    p <- p +
      theme(plot.margin = margin(30, 0, 0, 0))
  }
  
  mean_facets[[i]] <- p
}

sd_patch <- (sd_facets[[1]] | sd_facets[[2]] | sd_facets[[3]])
mean_patch <- (mean_facets[[1]] | mean_facets[[2]] | mean_facets[[3]])

sd_patch / mean_patch
```

</details>

```{r e8h4-7-show, echo = FALSE, out.width = "100%"}
knitr::include_graphics(knitr::fig_chunk("e8h4-7", "png"))
```


:::question
> **8H5.** Consider the `data(Wines2012)` data table. These data are expert ratings of 20 different French and American wines by 9 different French and American judges. Your goal is to model `score`, the subjective rating assigned by each judge to each wine. I recommend standardizing it. In this problem, consider only variation among judges and wines. Construct index variables of `judge` and `wine` and then use these index variables to construct a linear regression model. Justify your priors. You should end up with 9 judge parameters and 20 wine parameters. How do you interpret the variation among individual judges and individual wines? Do you notice any patterns, just by plotting the differences? Which judges gave the highest/lowest ratings? Which wines were rated worst/best on average?
:::

First, let's prepare the data. We'll standardize the score and create index variables for both `judge` and `wine`.

```{r b8h5-1}
data(Wines2012)

wine <- Wines2012 %>%
  as_tibble() %>%
  mutate(score_std = standardize(score),
         judge_ind = factor(as.integer(judge)),
         wine_ind = factor(as.integer(wine)),
         red = factor(flight, levels = c("white", "red")),
         wine_amer = factor(wine.amer),
         judge_amer = factor(judge.amer))
```

Next, we'll fit the model, remembering to remove the overall intercept (i.e., `~ 0`) so that the index variables are estimated correctly. Because the scores have been standardized, we can use the `normal(0, 0.5)` prior we've used previously and which is used throughout much of the text. We'll also extract the posterior draws so that we can explore the posterior for each judge and wine individually.

```{r b8h5-2}
b8h5 <- brm(bf(score_std ~ 0 + j + w,
               j ~ 0 + judge_ind,
               w ~ 0 + wine_ind,
               nl = TRUE),
            data = wine, family = gaussian,
            prior = c(prior(normal(0, 0.5), nlpar = j),
                      prior(normal(0, 0.5), nlpar = w),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp8", "b8h5.rds"))

draws <- as_draws_df(b8h5) %>%
  as_tibble() %>%
  select(-sigma, -lp__) %>%
  pivot_longer(-c(.chain, .iteration, .draw), names_to = c(NA, NA, "type", "num"), names_sep = "_",
               values_to = "value", ) %>%
  mutate(num = str_replace_all(num, "ind", ""),
         num = as.integer(num))
```

Looking at the distributions by judge, we can see that John Foy tends to give the highest scores, whereas Robert Hodgson and Jean-M Cardebat give the lowest scores on average.

```{r b8h5-3}
draws %>%
  filter(type == "judge") %>%
  mutate(num = factor(num)) %>%
  left_join(wine %>%
              distinct(judge, judge_ind),
            by = c("num" = "judge_ind")) %>%
  select(judge, value) %>%
  group_by(judge) %>%
  median_hdci(.width = c(0.67, 0.89, 0.97)) %>%
  ggplot(aes(y = fct_rev(judge), x = value, xmin = .lower, xmax = .upper)) +
  geom_interval() +
  scale_color_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                     limits = as.character(c(0.67, 0.89, 0.97))) +
  labs(y = NULL, x = "Parameter Value", color = "Interval")
```

For wines, B2, J2, D2, D1, and B1 received the highest scores on average, and wine I2 was clearly the lowest rated of the wines. However, overall, there is more variability among the judges than among the wines.

```{r b8h5-4}
draws %>%
  filter(type == "wine") %>%
  mutate(num = factor(num)) %>%
  left_join(wine %>%
              distinct(wine, wine_ind),
            by = c("num" = "wine_ind")) %>%
  select(wine, value) %>%
  group_by(wine) %>%
  median_hdci(.width = c(0.67, 0.89, 0.97)) %>%
  ggplot(aes(y = fct_rev(wine), x = value, xmin = .lower, xmax = .upper)) +
  geom_interval() +
  scale_color_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                     limits = as.character(c(0.67, 0.89, 0.97))) +
  labs(y = NULL, x = "Parameter Value", color = "Interval")
```

:::question
> **8H6.** Now consider three features of the wines and judges:

> 1. `flight`: Whether the wine is red or white.
> 2. `wine.amer`: Indicator variable for American wines.
> 3. `judge.amer`: Indicator variable for American judges.

> Use indicator or index variables to model the influence of these features on the scores. Omit the individual judge and wine index variables from Problem 1. Do not include interaction effects yet. Again justify your priors What do you conclude about the differences among the wines and judges? Try to relate the results to the inferences in the previous problem.
:::

We can estimate the {brms} model as defined below. In this definition, we're using indicator variables rather than index variables, as that will make the next question a little bit easier. We'll use our standard `normal(0, 0.2)` prior for the intercept, since that will have to be near zero given that the `score` has been standardized. For the coefficients, we have to ask what a reasonable difference in scores would be between New Jersey and French wines, New Jersey and French judges, and red and white wine. One standard deviation seems to be the edge of what might be reasonable, so we'll use `normal(0, 0.5)`, as that will leave sufficient probability in the tails. These priors constrain the space to plausible values, but we could probably use tighter priors on the slope parameters if we wanted to get more regularization.

```{r e8h6-1}
b8h6 <- brm(score_std ~ wine_amer + judge_amer + red,
            data = wine, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept),
                      prior(normal(0, 0.5), class = b),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp8", "b8h6.rds"))

fixef(b8h6)
```

As indicated by the last problem, there is basically no effect of red vs. white wine. There does appear to be a slight preference for French wines; however, there's a good deal of posterior density on both sides of 0. This confirms what we found in the last problem: that there is little variation coming from the wines. We do see a slightly stronger effect among judges, with American judges giving higher scores on average.

:::question
> **8H7.** Now consider two-way interactions among the three features. You should end up with three different interaction terms in your model. These will be easier to build, if you use indicator variables. Again justify your priors. Explain what each interaction means. Be sure to interpret the model's predictions on the outcome scale (`mu`, the expected score), not on the scale of individual parameters. You can use `link` to help with this, or just use your knowledge of the linear model instead. What do you conclude about the features and the scores? Can you relate the results of your model(s) to the individual judge and wine inferences from **8H5**?
:::

We'll start by fitting the model. Here I've used the same priors as the previous problem for the intercept and main effect parameters. For the interactions, I have specified a `normal(0, 0.25)` prior. The interaction priors are a little tighter because as we keep subsetting the sample, values can't keep getting larger and larger.

```{r e8h7-1}
b8h7 <- brm(score_std ~ wine_amer + judge_amer + red +
              wine_amer:judge_amer + wine_amer:red + judge_amer:red,
            data = wine, family = gaussian,
            prior = c(prior(normal(0, 0.2), class = Intercept),
                      prior(normal(0, 0.5), class = b),
                      prior(normal(0, 0.25), class = b,
                            coef = judge_amer1:redred),
                      prior(normal(0, 0.25), class = b,
                            coef = wine_amer1:judge_amer1),
                      prior(normal(0, 0.25), class = b,
                            coef = wine_amer1:redred),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "chp8", "b8h7.rds"))

fixef(b8h7)
```

To interpret what all these parameters mean, we can visualize the predicted score for each combination of judge, wine location, and wine type. In this figure we see that all combinations of of judge and wine for white wines. However, there do appear to be some slight differences in the red wines. In general, French judges really don't like the American reds, and American judges really do like the French reds.

```{r e8h7-2}
wine %>% 
  distinct(wine_amer, judge_amer, red) %>% 
  mutate(combo = glue("{ifelse(judge_amer == 0, 'French', 'American')} judge, ",
                      "{ifelse(wine_amer == 0, 'French', 'American')} wine")) %>% 
  add_epred_draws(b8h7) %>% 
  median_hdi(.width = c(0.67, 0.89, 0.97)) %>% 
  ggplot(aes(x = .epred, xmin = .lower, xmax = .upper, y = combo)) +
  facet_wrap(~red, nrow = 1, labeller = as_labeller(str_to_title)) +
  geom_interval() +
  scale_color_manual(values = ramp_blue(seq(0.9, 0.1, length.out = 3)),
                     breaks = c("0.67", "0.89", "0.97")) +
  labs(x = "Value", y = NULL, color = "Interval")
```


### Chapter 9

Something...

## Homework

:::question
> **1.** Revisit the marriage, age, and happiness collider bias example from Chapter 6. Run models `m6.9` and `m6.10` again (pages 178--179). Compare these two models using both PSIS and WAIC. Which model is expected to make better predictions, according to these criteria? On the basis of the causal model, how should you interpret the parameter estimates from the model preferred by PSIS and WAIC?
:::

As a reminder, here is the DAG for this example, where $H$ is happiness, $M$ is marriage, and $A$ is age.

```{r w4h1-1, message = FALSE, out.width = "40%", echo = FALSE}
library(dagitty)
library(ggdag)

hma_dag <- dagitty("dag{H -> M <- A}")
coordinates(hma_dag) <- list(x = c(H = 1, M = 2, A = 3),
                             y = c(H = 1, M = 1, A = 1))

ggplot(hma_dag, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = "black", size = 10) +
  geom_dag_edges(edge_color = "black", edge_width = 2,
                 arrow_directed = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  theme_void()
```

First, let's regenerate the data and estimate the models.

```{r w4h1-2, cache = TRUE}
d <- sim_happiness(seed = 1977, N_years = 1000)
dat <- d %>%
  filter(age > 17) %>%
  mutate(a = (age - 18) / (65 - 18),
         mid = factor(married + 1, labels = c("single", "married")))

b6.9 <- brm(happiness ~ 0 + mid + a, data = dat, family = gaussian,
            prior = c(prior(normal(0, 1), class = b, coef = midmarried),
                      prior(normal(0, 1), class = b, coef = midsingle),
                      prior(normal(0, 2), class = b, coef = a),
                      prior(exponential(1), class = sigma)),
            iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
            file = here("fits", "hw4", "w4h1-6.9"))

b6.10 <- brm(happiness ~ 1 + a, data = dat, family = gaussian,
             prior = c(prior(normal(0, 1), class = Intercept),
                       prior(normal(0, 2), class = b, coef = a),
                       prior(exponential(1), class = sigma)),
             iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
             file = here("fits", "hw4", "w4h1-6.10"))
```

Now we'll compare both PSIS and WAIC. The results from PSIS and WAIC are identical, with both showing a very strong preference for `b6.9` which includes marriage as a predictor. Thus, if we are interested only in predicting happiness, we should use model `b6.9`. However, due to the causal model, the coefficients can't be interpreted. The coefficient for age is confounded by the collider. Similarly, the coefficient for marriage is odd, because in the DAG, marriage does not cause happiness. If we ignore the direction of the arrow, then this would be partial causal effect of marriage on happiness, as age is explaining some of the variance in marriage.

```{r w4h1-3}
b6.9 <- add_criterion(b6.9, criterion = c("waic", "loo"))
b6.10 <- add_criterion(b6.10, criterion = c("waic", "loo"))

loo_compare(b6.9, b6.10, criterion = "waic")
loo_compare(b6.9, b6.10, criterion = "loo")
```


:::question
> **2.** Reconsider the urban fox analysis from last week's homework. On the basis of PSIS and WAIC scores, which combination of variables best predicts body weight ($W$, `weight`)? How would you interpret the estimates from the best scoring model?
:::

As a reminder, here is the DAG for the fox data.

```{r w4h2-1, out.width = "40%", echo = FALSE}
fox_dag <- dagitty("dag{ A -> F -> G -> W <- F }")
coordinates(fox_dag) <- list(x = c(A = 2, `F` = 1, W = 2, G = 3),
                             y = c(A = 3, `F` = 2, W = 1, G = 2))

ggplot(fox_dag, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_dag_text(color = "black", size = 10) +
  geom_dag_edges(edge_color = "black", edge_width = 2,
                 arrow_directed = grid::arrow(length = grid::unit(15, "pt"),
                                              type = "closed")) +
  theme_void()
```

In last week's homework, we estimated 2 models to predict weight. Let's restimate them here and add the PSIS and WAIC scores.

```{r w4h2-2}
data(foxes)

fox_dat <- foxes %>%
  as_tibble() %>%
  select(area, avgfood, weight, groupsize) %>%
  mutate(across(everything(), standardize))

food_1 <- brm(weight ~ 1 + avgfood, data = fox_dat, family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b,),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "hw4", "w4h2-1"))

food_2 <- brm(weight ~ 1 + avgfood + groupsize, data = fox_dat,
              family = gaussian,
              prior = c(prior(normal(0, 0.2), class = Intercept),
                        prior(normal(0, 0.5), class = b,),
                        prior(exponential(1), class = sigma)),
              iter = 4000, warmup = 2000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "hw4", "w4h2-2"))

food_1 <- add_criterion(food_1, criterion = c("waic", "loo"))
food_2 <- add_criterion(food_2, criterion = c("waic", "loo"))
```

Here again, the WAIC and PSIS scores are identical. Both models prefer model 2, with both food and group size included in the model. However, the difference isn't too much larger than the standard error, so the preference is not overly strong.

```{r w4h2-3}
loo_compare(food_1, food_2, criterion = "waic")

loo_compare(food_1, food_2, criterion = "loo")
```

In this model, the food coefficient represents the direct effect of food on the number of wolves, because conditioning on group size closes the indirect path from food to wolves. From the perspective of group size, food is a fork. So by closing this path, the group size coefficient represents the total (and also direct since this is only one path remaining) causal effect of group size on the number of wolves.


:::question
> **3.** Build a predictive model of the relationship shown on the cover of the book, the relationship between the timing of cherry blossoms and March temperature in the same year. The data are found in `data(cherry_blossoms)`. Consider at least two functions to predict `doy` with `temp`. Compare them with PSIS or WAIC.
>
> Suppose March temperatures reach 9 degrees by the year 2050. What does your best model predict for the predictive distribution of the day-in-year that the cherry trees will blossom?
:::

For this exercise, I'll fit 2 spline models and a linear model. For the spline models I'll use 30 knots, one model with loose priors and the other with tighter priors.

```{r w4h3-1}
library(splines)

data(cherry_blossoms)
cb_dat <- cherry_blossoms %>%
  drop_na(doy, temp)

linear <- brm(doy ~ 1 + temp, data = cb_dat, family = gaussian,
              prior = c(prior(normal(100, 10), class = Intercept),
                        prior(normal(0, 10), class = b),
                        prior(exponential(1), class = sigma)),
              iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,
              file = here("fits", "hw4", "w4h3-1"))

spline_1 <- brm(doy ~ 1 + s(temp, bs = "bs", k = 30), data = cb_dat,
                family = gaussian,
                prior = c(prior(normal(100, 10), class = Intercept),
                          prior(normal(0, 10), class = b),
                          prior(student_t(3, 0, 5.9), class = sds),
                          prior(exponential(1), class = sigma)),
                iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,
                file = here("fits", "hw4", "w4h3-2"))

spline_2 <- brm(doy ~ 1 + s(temp, bs = "bs", k = 30), data = cb_dat,
                family = gaussian,
                prior = c(prior(normal(100, 10), class = Intercept),
                          prior(normal(0, 2), class = b),
                          prior(student_t(3, 0, 5.9), class = sds),
                          prior(exponential(1), class = sigma)),
                iter = 2000, warmup = 1000, chains = 4, cores = 4, seed = 1234,
                file = here("fits", "hw4", "w4h3-3"))


linear <- add_criterion(linear, criterion = c("waic", "loo"))
spline_1 <- add_criterion(spline_1, criterion = c("waic", "loo"))
spline_2 <- add_criterion(spline_2, criterion = c("waic", "loo"))
```

When comparing the models, both PSIS and WAIC prefer the linear. Although the two spline models perform almost identically, the standard errors of the differences between these models and the linear model are only about one fifth of the difference.

```{r w4h3-2}
loo_compare(linear, spline_1, spline_2, criterion = "waic")

loo_compare(linear, spline_1, spline_2, criterion = "loo")
```

Using our linear model, we can predict the the date of the first bloom with a March temperature of 9. We see expected value is around day 96, with an 89% compatibility interval of 87 to 106. Since 2050 is not a leap year, this corresponds to a first bloom between March 28 and April 16.

```{r w4h3-3, cache = TRUE}
new_dat <- tibble(temp = 9)

preds <- new_dat %>% 
  add_predicted_draws(linear)

mean_hdi(preds$.prediction, .width = c(0.89))

ggplot(preds, aes(x = .prediction)) +
  stat_halfeye(.width = c(0.67, 0.89, 0.97)) +
  labs(x = "Day of Year", y = "Posterior Predictive Density")
```


:::question
> **4 - OPTIONAL CHALLENGE.** The data in `data(Dinosaurs)` are body mass estimates at different estimated ages for six different dinosaur species. See `?Dinosaurs` for more details. Choose one or more of these species (at least one, but as many as you like) and model its growth. To be precise: Make a predictive model of body mass using age as a predictor. Consider two or more model types for the function relating age to body mass and score each using PSIS and WAIC.
>
> Which model do you think is best, on predictive grounds? On scientific grounds? If your answers to these questions differ, why?
>
> This is a challenging exercise, because the data are so scarce. But it is also a realistic example, because people publish *Nature* papers with even less data. So do your best, and I look forward to seeing your growth curves.
:::
